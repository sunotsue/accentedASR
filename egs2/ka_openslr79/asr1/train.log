# python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/token_list/bpe_unigram100/bpe.model --token_type bpe --token_list data/token_list/bpe_unigram100/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/en_dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/en_dev/text,text,text --valid_shape_file exp/asr_stats_raw_bpe100_sp/valid/speech_shape --valid_shape_file exp/asr_stats_raw_bpe100_sp/valid/text_shape.bpe --resume true --init_param --ignore_init_mismatch false --fold_length 80000 --fold_length 150 --output_dir exp/asr_train_asr_conformer5_raw_bpe100_sp --config conf/tuning/train_asr_conformer5.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/en_train_sp/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/en_train_sp/text,text,text --train_shape_file exp/asr_stats_raw_bpe100_sp/train/speech_shape --train_shape_file exp/asr_stats_raw_bpe100_sp/train/text_shape.bpe --ngpu 1 --multiprocessing_distributed True 
# Started at Wed Apr 27 22:26:33 UTC 2022
#
/home/ubuntu/accentedASR/tools/anaconda/envs/espnet/bin/python3 /home/ubuntu/accentedASR/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel data/token_list/bpe_unigram100/bpe.model --token_type bpe --token_list data/token_list/bpe_unigram100/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/en_dev/wav.scp,speech,sound --valid_data_path_and_name_and_type dump/raw/en_dev/text,text,text --valid_shape_file exp/asr_stats_raw_bpe100_sp/valid/speech_shape --valid_shape_file exp/asr_stats_raw_bpe100_sp/valid/text_shape.bpe --resume true --init_param --ignore_init_mismatch false --fold_length 80000 --fold_length 150 --output_dir exp/asr_train_asr_conformer5_raw_bpe100_sp --config conf/tuning/train_asr_conformer5.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type dump/raw/en_train_sp/wav.scp,speech,sound --train_data_path_and_name_and_type dump/raw/en_train_sp/text,text,text --train_shape_file exp/asr_stats_raw_bpe100_sp/train/speech_shape --train_shape_file exp/asr_stats_raw_bpe100_sp/train/text_shape.bpe --ngpu 1 --multiprocessing_distributed True
[ip-10-0-0-95] 2022-04-27 22:26:40,059 (asr:411) INFO: Vocabulary size: 100
[ip-10-0-0-95] 2022-04-27 22:26:40,168 (conformer_encoder:137) WARNING: Using legacy_rel_pos and it will be deprecated in the future.
[ip-10-0-0-95] 2022-04-27 22:26:40,282 (conformer_encoder:237) WARNING: Using legacy_rel_selfattn and it will be deprecated in the future.
/home/ubuntu/accentedASR/espnet2/schedulers/noam_lr.py:40: UserWarning: NoamLR is deprecated. Use WarmupLR(warmup_steps=25000) with Optimizer(lr=0.0015811388300841895)
  warnings.warn(
[ip-10-0-0-95] 2022-04-27 22:26:45,974 (abs_task:1157) INFO: pytorch.version=1.10.1, cuda.available=True, cudnn.version=7605, cudnn.benchmark=False, cudnn.deterministic=False
[ip-10-0-0-95] 2022-04-27 22:26:45,982 (abs_task:1158) INFO: Model structure:
ESPnetASRModel(
  (frontend): DefaultFrontend(
    (stft): Stft(n_fft=512, win_length=400, hop_length=160, center=True, normalized=False, onesided=True)
    (frontend): Frontend()
    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (encoder): ConformerEncoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=4864, out_features=256, bias=True)
        (1): LegacyRelPositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): EncoderLayer(
        (self_attn): LegacyRelPositionMultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear_pos): Linear(in_features=256, out_features=256, bias=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (feed_forward_macaron): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): Swish()
        )
        (conv_module): ConvolutionModule(
          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (activation): Swish()
        )
        (norm_ff): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_mha): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_ff_macaron): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_conv): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm_final): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embed): Sequential(
      (0): Embedding(100, 256)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=256, out_features=100, bias=True)
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=256, out_features=256, bias=True)
          (linear_k): Linear(in_features=256, out_features=256, bias=True)
          (linear_v): Linear(in_features=256, out_features=256, bias=True)
          (linear_out): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=100, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 43.01 M
    Number of trainable parameters: 43.01 M (100.0%)
    Size: 172.06 MB
    Type: torch.float32
[ip-10-0-0-95] 2022-04-27 22:26:45,982 (abs_task:1161) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 4.0
    lr: 6.324555320336758e-08
    weight_decay: 0
)
[ip-10-0-0-95] 2022-04-27 22:26:45,982 (abs_task:1162) INFO: Scheduler: NoamLR(model_size=256, warmup_steps=25000)
[ip-10-0-0-95] 2022-04-27 22:26:45,982 (abs_task:1171) INFO: Saving the configuration in exp/asr_train_asr_conformer5_raw_bpe100_sp/config.yaml
[ip-10-0-0-95] 2022-04-27 22:26:46,610 (abs_task:1525) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/en_train_sp/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/en_train_sp/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f064bf7b460>)
[ip-10-0-0-95] 2022-04-27 22:26:46,611 (abs_task:1526) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=824, batch_bins=2500000, sort_in_batch=descending, sort_batch=descending)
[ip-10-0-0-95] 2022-04-27 22:26:46,611 (abs_task:1527) INFO: [train] mini-batch sizes summary: N-batch=824, mean=30.6, min=11, max=83
[ip-10-0-0-95] 2022-04-27 22:26:46,644 (abs_task:1525) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/en_dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/en_dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f064bf7b7f0>)
[ip-10-0-0-95] 2022-04-27 22:26:46,644 (abs_task:1526) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=29, batch_bins=2500000, sort_in_batch=descending, sort_batch=descending)
[ip-10-0-0-95] 2022-04-27 22:26:46,644 (abs_task:1527) INFO: [valid] mini-batch sizes summary: N-batch=29, mean=26.0, min=8, max=46
[ip-10-0-0-95] 2022-04-27 22:26:46,665 (abs_task:1525) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/en_dev/wav.scp", "type": "sound"}
  text: {"path": "dump/raw/en_dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f064bf7be20>)
[ip-10-0-0-95] 2022-04-27 22:26:46,665 (abs_task:1526) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=754, batch_size=1, key_file=exp/asr_stats_raw_bpe100_sp/valid/speech_shape, 
[ip-10-0-0-95] 2022-04-27 22:26:46,665 (abs_task:1527) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[ip-10-0-0-95] 2022-04-27 22:26:46,859 (trainer:280) INFO: 1/50epoch started
/home/ubuntu/accentedASR/espnet2/layers/stft.py:166: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  olens = (ilens - self.n_fft) // self.hop_length + 1
[ip-10-0-0-95] 2022-04-27 22:26:48,994 (trainer:618) WARNING: The grad norm is inf. Skipping updating the model.
[ip-10-0-0-95] 2022-04-27 22:26:49,286 (trainer:618) WARNING: The grad norm is inf. Skipping updating the model.
[ip-10-0-0-95] 2022-04-27 22:26:49,579 (trainer:618) WARNING: The grad norm is inf. Skipping updating the model.
[ip-10-0-0-95] 2022-04-27 22:26:49,870 (trainer:618) WARNING: The grad norm is inf. Skipping updating the model.
[ip-10-0-0-95] 2022-04-27 22:26:51,392 (trainer:618) WARNING: The grad norm is inf. Skipping updating the model.
[ip-10-0-0-95] 2022-04-27 22:26:58,637 (trainer:618) WARNING: The grad norm is inf. Skipping updating the model.
[ip-10-0-0-95] 2022-04-27 22:27:03,186 (trainer:676) INFO: 1epoch:train:1-41batch: iter_time=0.007, forward_time=0.146, loss_ctc=416.174, loss_att=161.663, acc=0.008, loss=238.016, backward_time=0.103, optim0_lr0=1.074e-06, train_time=0.396, optim_step_time=0.042
[ip-10-0-0-95] 2022-04-27 22:27:17,770 (trainer:676) INFO: 1epoch:train:42-82batch: iter_time=1.392e-04, forward_time=0.108, loss_ctc=223.792, loss_att=167.645, acc=0.010, loss=184.489, backward_time=0.103, optim0_lr0=3.605e-06, train_time=0.356, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:27:32,356 (trainer:676) INFO: 1epoch:train:83-123batch: iter_time=1.371e-04, forward_time=0.109, loss_ctc=164.741, loss_att=154.458, acc=0.024, loss=157.543, backward_time=0.102, optim0_lr0=6.198e-06, train_time=0.355, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:27:46,873 (trainer:676) INFO: 1epoch:train:124-164batch: iter_time=1.453e-04, forward_time=0.108, loss_ctc=166.598, loss_att=155.340, acc=0.066, loss=158.718, backward_time=0.101, optim0_lr0=8.791e-06, train_time=0.354, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:28:01,815 (trainer:676) INFO: 1epoch:train:165-205batch: iter_time=1.559e-04, forward_time=0.110, loss_ctc=156.603, loss_att=141.551, acc=0.113, loss=146.067, backward_time=0.106, optim0_lr0=1.138e-05, train_time=0.364, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:28:16,494 (trainer:676) INFO: 1epoch:train:206-246batch: iter_time=1.636e-04, forward_time=0.111, loss_ctc=153.306, loss_att=134.065, acc=0.127, loss=139.837, backward_time=0.104, optim0_lr0=1.398e-05, train_time=0.358, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:28:31,536 (trainer:676) INFO: 1epoch:train:247-287batch: iter_time=1.319e-04, forward_time=0.109, loss_ctc=160.605, loss_att=136.834, acc=0.138, loss=143.965, backward_time=0.110, optim0_lr0=1.657e-05, train_time=0.367, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:28:46,495 (trainer:676) INFO: 1epoch:train:288-328batch: iter_time=1.249e-04, forward_time=0.109, loss_ctc=159.528, loss_att=133.199, acc=0.143, loss=141.097, backward_time=0.107, optim0_lr0=1.916e-05, train_time=0.365, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:29:01,461 (trainer:676) INFO: 1epoch:train:329-369batch: iter_time=1.312e-04, forward_time=0.110, loss_ctc=161.697, loss_att=132.215, acc=0.162, loss=141.059, backward_time=0.106, optim0_lr0=2.176e-05, train_time=0.365, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:29:16,767 (trainer:676) INFO: 1epoch:train:370-410batch: iter_time=1.415e-04, forward_time=0.114, loss_ctc=147.067, loss_att=118.823, acc=0.169, loss=127.296, backward_time=0.108, optim0_lr0=2.435e-05, train_time=0.373, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:29:31,769 (trainer:676) INFO: 1epoch:train:411-451batch: iter_time=1.834e-04, forward_time=0.111, loss_ctc=151.459, loss_att=119.949, acc=0.176, loss=129.402, backward_time=0.107, optim0_lr0=2.694e-05, train_time=0.366, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:29:47,057 (trainer:676) INFO: 1epoch:train:452-492batch: iter_time=1.622e-04, forward_time=0.112, loss_ctc=152.165, loss_att=117.940, acc=0.184, loss=128.207, backward_time=0.108, optim0_lr0=2.954e-05, train_time=0.373, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:30:02,515 (trainer:676) INFO: 1epoch:train:493-533batch: iter_time=1.571e-04, forward_time=0.116, loss_ctc=150.917, loss_att=114.944, acc=0.188, loss=125.736, backward_time=0.110, optim0_lr0=3.213e-05, train_time=0.377, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:30:09,342 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:30:17,533 (trainer:676) INFO: 1epoch:train:534-574batch: iter_time=1.501e-04, forward_time=0.114, loss_ctc=151.069, loss_att=114.237, acc=0.194, loss=125.287, backward_time=0.105, optim0_lr0=3.472e-05, train_time=0.366, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:30:28,431 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:30:32,828 (trainer:676) INFO: 1epoch:train:575-615batch: iter_time=1.627e-04, forward_time=0.112, loss_ctc=146.603, loss_att=109.300, acc=0.196, loss=120.491, backward_time=0.111, optim0_lr0=3.731e-05, train_time=0.373, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:30:48,469 (trainer:676) INFO: 1epoch:train:616-656batch: iter_time=1.618e-04, forward_time=0.114, loss_ctc=152.730, loss_att=113.099, acc=0.198, loss=124.988, backward_time=0.113, optim0_lr0=3.991e-05, train_time=0.381, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:31:03,606 (trainer:676) INFO: 1epoch:train:657-697batch: iter_time=1.555e-04, forward_time=0.111, loss_ctc=146.181, loss_att=108.804, acc=0.202, loss=120.017, backward_time=0.107, optim0_lr0=4.250e-05, train_time=0.369, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:31:19,014 (trainer:676) INFO: 1epoch:train:698-738batch: iter_time=1.688e-04, forward_time=0.116, loss_ctc=138.532, loss_att=102.699, acc=0.207, loss=113.449, backward_time=0.109, optim0_lr0=4.509e-05, train_time=0.375, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:31:34,190 (trainer:676) INFO: 1epoch:train:739-779batch: iter_time=1.478e-04, forward_time=0.113, loss_ctc=146.760, loss_att=109.323, acc=0.208, loss=120.554, backward_time=0.108, optim0_lr0=4.769e-05, train_time=0.370, optim_step_time=0.041
[ip-10-0-0-95] 2022-04-27 22:31:49,517 (trainer:676) INFO: 1epoch:train:780-820batch: iter_time=1.263e-04, forward_time=0.111, loss_ctc=147.699, loss_att=110.772, acc=0.211, loss=121.850, backward_time=0.109, optim0_lr0=5.028e-05, train_time=0.373, optim_step_time=0.041
/home/ubuntu/accentedASR/espnet2/layers/stft.py:166: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  olens = (ilens - self.n_fft) // self.hop_length + 1
[ip-10-0-0-95] 2022-04-27 22:32:30,680 (trainer:334) INFO: 1epoch results: [train] iter_time=4.684e-04, forward_time=0.113, loss_ctc=169.684, loss_att=127.659, acc=0.147, loss=140.267, backward_time=0.107, optim0_lr0=2.578e-05, train_time=0.369, optim_step_time=0.041, time=5 minutes and 4.33 seconds, total_count=824, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=139.489, cer_ctc=0.890, loss_att=106.505, acc=0.217, cer=0.694, wer=1.000, loss=116.400, time=7.33 seconds, total_count=29, gpu_max_cached_mem_GB=7.422, [att_plot] time=32.14 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 22:32:33,200 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 22:32:33,200 (trainer:268) INFO: 2/50epoch started. Estimated time to finish: 4 hours, 42 minutes and 50.75 seconds
[ip-10-0-0-95] 2022-04-27 22:32:47,984 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:32:48,608 (trainer:676) INFO: 2epoch:train:1-41batch: iter_time=0.005, forward_time=0.109, loss_ctc=141.377, loss_att=106.877, acc=0.209, loss=117.227, backward_time=0.109, optim_step_time=0.041, optim0_lr0=5.313e-05, train_time=0.375
[ip-10-0-0-95] 2022-04-27 22:33:03,443 (trainer:676) INFO: 2epoch:train:42-82batch: iter_time=1.603e-04, forward_time=0.109, loss_ctc=135.502, loss_att=102.205, acc=0.217, loss=112.194, backward_time=0.104, optim_step_time=0.041, optim0_lr0=5.572e-05, train_time=0.362
[ip-10-0-0-95] 2022-04-27 22:33:18,709 (trainer:676) INFO: 2epoch:train:83-123batch: iter_time=1.623e-04, forward_time=0.110, loss_ctc=131.300, loss_att=99.497, acc=0.220, loss=109.038, backward_time=0.109, optim_step_time=0.041, optim0_lr0=5.831e-05, train_time=0.372
[ip-10-0-0-95] 2022-04-27 22:33:34,075 (trainer:676) INFO: 2epoch:train:124-164batch: iter_time=1.571e-04, forward_time=0.110, loss_ctc=141.233, loss_att=108.200, acc=0.217, loss=118.110, backward_time=0.112, optim_step_time=0.041, optim0_lr0=6.091e-05, train_time=0.374
[ip-10-0-0-95] 2022-04-27 22:33:49,118 (trainer:676) INFO: 2epoch:train:165-205batch: iter_time=1.649e-04, forward_time=0.110, loss_ctc=133.842, loss_att=103.308, acc=0.224, loss=112.468, backward_time=0.107, optim_step_time=0.041, optim0_lr0=6.350e-05, train_time=0.367
[ip-10-0-0-95] 2022-04-27 22:34:04,458 (trainer:676) INFO: 2epoch:train:206-246batch: iter_time=1.747e-04, forward_time=0.110, loss_ctc=131.420, loss_att=103.526, acc=0.218, loss=111.894, backward_time=0.111, optim_step_time=0.041, optim0_lr0=6.609e-05, train_time=0.374
[ip-10-0-0-95] 2022-04-27 22:34:19,499 (trainer:676) INFO: 2epoch:train:247-287batch: iter_time=1.668e-04, forward_time=0.109, loss_ctc=132.287, loss_att=105.547, acc=0.221, loss=113.569, backward_time=0.107, optim_step_time=0.041, optim0_lr0=6.868e-05, train_time=0.367
[ip-10-0-0-95] 2022-04-27 22:34:34,668 (trainer:676) INFO: 2epoch:train:288-328batch: iter_time=1.465e-04, forward_time=0.110, loss_ctc=132.132, loss_att=106.999, acc=0.223, loss=114.539, backward_time=0.109, optim_step_time=0.041, optim0_lr0=7.128e-05, train_time=0.370
[ip-10-0-0-95] 2022-04-27 22:34:50,067 (trainer:676) INFO: 2epoch:train:329-369batch: iter_time=1.648e-04, forward_time=0.111, loss_ctc=124.745, loss_att=102.201, acc=0.225, loss=108.964, backward_time=0.111, optim_step_time=0.041, optim0_lr0=7.387e-05, train_time=0.375
[ip-10-0-0-95] 2022-04-27 22:35:05,245 (trainer:676) INFO: 2epoch:train:370-410batch: iter_time=1.744e-04, forward_time=0.110, loss_ctc=123.202, loss_att=101.951, acc=0.230, loss=108.326, backward_time=0.108, optim_step_time=0.041, optim0_lr0=7.646e-05, train_time=0.370
[ip-10-0-0-95] 2022-04-27 22:35:09,056 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:35:20,649 (trainer:676) INFO: 2epoch:train:411-451batch: iter_time=1.799e-04, forward_time=0.111, loss_ctc=114.192, loss_att=95.327, acc=0.235, loss=100.987, backward_time=0.112, optim_step_time=0.041, optim0_lr0=7.906e-05, train_time=0.375
[ip-10-0-0-95] 2022-04-27 22:35:36,209 (trainer:676) INFO: 2epoch:train:452-492batch: iter_time=1.783e-04, forward_time=0.111, loss_ctc=122.463, loss_att=105.248, acc=0.226, loss=110.413, backward_time=0.113, optim_step_time=0.041, optim0_lr0=8.165e-05, train_time=0.379
[ip-10-0-0-95] 2022-04-27 22:35:51,614 (trainer:676) INFO: 2epoch:train:493-533batch: iter_time=1.810e-04, forward_time=0.111, loss_ctc=120.628, loss_att=104.533, acc=0.229, loss=109.361, backward_time=0.112, optim_step_time=0.041, optim0_lr0=8.424e-05, train_time=0.375
[ip-10-0-0-95] 2022-04-27 22:36:06,974 (trainer:676) INFO: 2epoch:train:534-574batch: iter_time=1.599e-04, forward_time=0.112, loss_ctc=118.813, loss_att=104.572, acc=0.231, loss=108.844, backward_time=0.110, optim_step_time=0.041, optim0_lr0=8.684e-05, train_time=0.374
[ip-10-0-0-95] 2022-04-27 22:36:22,336 (trainer:676) INFO: 2epoch:train:575-615batch: iter_time=1.870e-04, forward_time=0.111, loss_ctc=117.207, loss_att=105.419, acc=0.231, loss=108.955, backward_time=0.111, optim_step_time=0.041, optim0_lr0=8.943e-05, train_time=0.374
[ip-10-0-0-95] 2022-04-27 22:36:37,963 (trainer:676) INFO: 2epoch:train:616-656batch: iter_time=1.646e-04, forward_time=0.113, loss_ctc=112.714, loss_att=100.953, acc=0.232, loss=104.481, backward_time=0.115, optim_step_time=0.041, optim0_lr0=9.202e-05, train_time=0.381
[ip-10-0-0-95] 2022-04-27 22:36:53,733 (trainer:676) INFO: 2epoch:train:657-697batch: iter_time=1.823e-04, forward_time=0.112, loss_ctc=105.225, loss_att=94.243, acc=0.237, loss=97.538, backward_time=0.115, optim_step_time=0.041, optim0_lr0=9.462e-05, train_time=0.384
[ip-10-0-0-95] 2022-04-27 22:37:09,304 (trainer:676) INFO: 2epoch:train:698-738batch: iter_time=1.554e-04, forward_time=0.112, loss_ctc=106.782, loss_att=96.975, acc=0.242, loss=99.917, backward_time=0.113, optim_step_time=0.041, optim0_lr0=9.721e-05, train_time=0.379
[ip-10-0-0-95] 2022-04-27 22:37:24,930 (trainer:676) INFO: 2epoch:train:739-779batch: iter_time=1.252e-04, forward_time=0.112, loss_ctc=101.236, loss_att=92.409, acc=0.245, loss=95.058, backward_time=0.113, optim_step_time=0.041, optim0_lr0=9.980e-05, train_time=0.381
[ip-10-0-0-95] 2022-04-27 22:37:40,717 (trainer:676) INFO: 2epoch:train:780-820batch: iter_time=1.284e-04, forward_time=0.112, loss_ctc=111.711, loss_att=105.427, acc=0.234, loss=107.312, backward_time=0.113, optim_step_time=0.041, optim0_lr0=1.024e-04, train_time=0.385
[ip-10-0-0-95] 2022-04-27 22:38:20,833 (trainer:334) INFO: 2epoch results: [train] iter_time=4.215e-04, forward_time=0.111, loss_ctc=122.619, loss_att=102.092, acc=0.227, loss=108.250, backward_time=0.111, optim_step_time=0.041, optim0_lr0=7.789e-05, train_time=0.375, time=5 minutes and 9.04 seconds, total_count=1648, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=89.168, cer_ctc=0.505, loss_att=100.393, acc=0.244, cer=0.707, wer=1.000, loss=97.026, time=7.48 seconds, total_count=58, gpu_max_cached_mem_GB=7.422, [att_plot] time=31.12 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 22:38:26,322 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 22:38:26,322 (trainer:268) INFO: 3/50epoch started. Estimated time to finish: 4 hours, 39 minutes and 47.13 seconds
[ip-10-0-0-95] 2022-04-27 22:38:41,950 (trainer:676) INFO: 3epoch:train:1-41batch: iter_time=0.005, forward_time=0.111, loss_ctc=108.411, loss_att=103.228, acc=0.239, loss=104.783, backward_time=0.112, optim_step_time=0.041, optim0_lr0=1.052e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 22:38:57,033 (trainer:676) INFO: 3epoch:train:42-82batch: iter_time=1.378e-04, forward_time=0.110, loss_ctc=108.758, loss_att=106.101, acc=0.237, loss=106.898, backward_time=0.108, optim_step_time=0.041, optim0_lr0=1.078e-04, train_time=0.368
[ip-10-0-0-95] 2022-04-27 22:39:12,171 (trainer:676) INFO: 3epoch:train:83-123batch: iter_time=1.344e-04, forward_time=0.110, loss_ctc=104.349, loss_att=102.417, acc=0.243, loss=102.997, backward_time=0.109, optim_step_time=0.041, optim0_lr0=1.104e-04, train_time=0.369
[ip-10-0-0-95] 2022-04-27 22:39:18,256 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:39:27,454 (trainer:676) INFO: 3epoch:train:124-164batch: iter_time=1.372e-04, forward_time=0.111, loss_ctc=99.649, loss_att=96.162, acc=0.247, loss=97.208, backward_time=0.108, optim_step_time=0.041, optim0_lr0=1.130e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-27 22:39:42,528 (trainer:676) INFO: 3epoch:train:165-205batch: iter_time=1.306e-04, forward_time=0.111, loss_ctc=99.639, loss_att=97.356, acc=0.253, loss=98.041, backward_time=0.108, optim_step_time=0.041, optim0_lr0=1.156e-04, train_time=0.367
[ip-10-0-0-95] 2022-04-27 22:39:58,014 (trainer:676) INFO: 3epoch:train:206-246batch: iter_time=1.269e-04, forward_time=0.111, loss_ctc=101.351, loss_att=100.562, acc=0.250, loss=100.799, backward_time=0.113, optim_step_time=0.041, optim0_lr0=1.182e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 22:40:13,358 (trainer:676) INFO: 3epoch:train:247-287batch: iter_time=1.285e-04, forward_time=0.111, loss_ctc=99.206, loss_att=99.023, acc=0.253, loss=99.078, backward_time=0.110, optim_step_time=0.041, optim0_lr0=1.208e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 22:40:29,015 (trainer:676) INFO: 3epoch:train:288-328batch: iter_time=1.316e-04, forward_time=0.112, loss_ctc=91.874, loss_att=91.790, acc=0.258, loss=91.815, backward_time=0.113, optim_step_time=0.041, optim0_lr0=1.234e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 22:40:44,494 (trainer:676) INFO: 3epoch:train:329-369batch: iter_time=1.266e-04, forward_time=0.112, loss_ctc=93.573, loss_att=94.344, acc=0.260, loss=94.113, backward_time=0.113, optim_step_time=0.041, optim0_lr0=1.260e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 22:40:59,969 (trainer:676) INFO: 3epoch:train:370-410batch: iter_time=1.316e-04, forward_time=0.111, loss_ctc=96.397, loss_att=96.954, acc=0.262, loss=96.787, backward_time=0.111, optim_step_time=0.041, optim0_lr0=1.286e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 22:41:15,629 (trainer:676) INFO: 3epoch:train:411-451batch: iter_time=1.394e-04, forward_time=0.112, loss_ctc=95.534, loss_att=97.090, acc=0.266, loss=96.623, backward_time=0.113, optim_step_time=0.041, optim0_lr0=1.312e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 22:41:30,767 (trainer:676) INFO: 3epoch:train:452-492batch: iter_time=1.578e-04, forward_time=0.112, loss_ctc=98.079, loss_att=102.621, acc=0.262, loss=101.258, backward_time=0.106, optim_step_time=0.041, optim0_lr0=1.338e-04, train_time=0.369
[ip-10-0-0-95] 2022-04-27 22:41:46,349 (trainer:676) INFO: 3epoch:train:493-533batch: iter_time=1.300e-04, forward_time=0.112, loss_ctc=90.765, loss_att=92.672, acc=0.269, loss=92.100, backward_time=0.112, optim_step_time=0.041, optim0_lr0=1.364e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 22:42:02,115 (trainer:676) INFO: 3epoch:train:534-574batch: iter_time=1.434e-04, forward_time=0.113, loss_ctc=86.876, loss_att=89.463, acc=0.280, loss=88.687, backward_time=0.115, optim_step_time=0.041, optim0_lr0=1.390e-04, train_time=0.384
[ip-10-0-0-95] 2022-04-27 22:42:17,837 (trainer:676) INFO: 3epoch:train:575-615batch: iter_time=1.353e-04, forward_time=0.111, loss_ctc=89.993, loss_att=90.806, acc=0.281, loss=90.562, backward_time=0.113, optim_step_time=0.041, optim0_lr0=1.415e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 22:42:33,385 (trainer:676) INFO: 3epoch:train:616-656batch: iter_time=1.435e-04, forward_time=0.113, loss_ctc=89.254, loss_att=93.467, acc=0.288, loss=92.203, backward_time=0.112, optim_step_time=0.041, optim0_lr0=1.441e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 22:42:47,569 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:42:48,594 (trainer:676) INFO: 3epoch:train:657-697batch: iter_time=1.398e-04, forward_time=0.111, loss_ctc=85.250, loss_att=87.887, acc=0.282, loss=87.096, backward_time=0.107, optim_step_time=0.041, optim0_lr0=1.467e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 22:43:04,403 (trainer:676) INFO: 3epoch:train:698-738batch: iter_time=1.248e-04, forward_time=0.113, loss_ctc=91.986, loss_att=100.838, acc=0.280, loss=98.183, backward_time=0.116, optim_step_time=0.041, optim0_lr0=1.493e-04, train_time=0.385
[ip-10-0-0-95] 2022-04-27 22:43:19,764 (trainer:676) INFO: 3epoch:train:739-779batch: iter_time=1.461e-04, forward_time=0.112, loss_ctc=88.775, loss_att=92.915, acc=0.288, loss=91.673, backward_time=0.109, optim_step_time=0.041, optim0_lr0=1.519e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 22:43:35,168 (trainer:676) INFO: 3epoch:train:780-820batch: iter_time=1.605e-04, forward_time=0.110, loss_ctc=89.602, loss_att=94.335, acc=0.293, loss=92.915, backward_time=0.110, optim_step_time=0.041, optim0_lr0=1.545e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 22:44:15,073 (trainer:334) INFO: 3epoch results: [train] iter_time=3.830e-04, forward_time=0.111, loss_ctc=95.202, loss_att=96.259, acc=0.265, loss=95.942, backward_time=0.111, optim_step_time=0.041, optim0_lr0=1.300e-04, train_time=0.376, time=5 minutes and 10.45 seconds, total_count=2472, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=65.317, cer_ctc=0.355, loss_att=91.920, acc=0.309, cer=0.710, wer=1.000, loss=83.939, time=7.47 seconds, total_count=87, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.83 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 22:44:20,601 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 22:44:20,602 (trainer:268) INFO: 4/50epoch started. Estimated time to finish: 4 hours, 35 minutes and 8.64 seconds
[ip-10-0-0-95] 2022-04-27 22:44:36,179 (trainer:676) INFO: 4epoch:train:1-41batch: iter_time=0.005, forward_time=0.111, loss_ctc=86.416, loss_att=91.614, acc=0.306, loss=90.055, backward_time=0.110, optim_step_time=0.041, optim0_lr0=1.574e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 22:44:51,212 (trainer:676) INFO: 4epoch:train:42-82batch: iter_time=1.690e-04, forward_time=0.114, loss_ctc=78.475, loss_att=81.291, acc=0.314, loss=80.446, backward_time=0.104, optim_step_time=0.041, optim0_lr0=1.599e-04, train_time=0.366
[ip-10-0-0-95] 2022-04-27 22:45:06,826 (trainer:676) INFO: 4epoch:train:83-123batch: iter_time=1.418e-04, forward_time=0.115, loss_ctc=86.332, loss_att=93.855, acc=0.305, loss=91.598, backward_time=0.114, optim_step_time=0.041, optim0_lr0=1.625e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 22:45:22,321 (trainer:676) INFO: 4epoch:train:124-164batch: iter_time=1.573e-04, forward_time=0.113, loss_ctc=88.556, loss_att=96.542, acc=0.300, loss=94.146, backward_time=0.110, optim_step_time=0.041, optim0_lr0=1.651e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 22:45:37,657 (trainer:676) INFO: 4epoch:train:165-205batch: iter_time=1.561e-04, forward_time=0.113, loss_ctc=82.615, loss_att=88.414, acc=0.316, loss=86.674, backward_time=0.107, optim_step_time=0.041, optim0_lr0=1.677e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 22:45:53,282 (trainer:676) INFO: 4epoch:train:206-246batch: iter_time=1.607e-04, forward_time=0.115, loss_ctc=81.364, loss_att=85.944, acc=0.322, loss=84.570, backward_time=0.111, optim_step_time=0.041, optim0_lr0=1.703e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 22:46:08,822 (trainer:676) INFO: 4epoch:train:247-287batch: iter_time=1.830e-04, forward_time=0.114, loss_ctc=80.040, loss_att=86.706, acc=0.329, loss=84.707, backward_time=0.112, optim_step_time=0.041, optim0_lr0=1.729e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 22:46:24,388 (trainer:676) INFO: 4epoch:train:288-328batch: iter_time=1.858e-04, forward_time=0.111, loss_ctc=81.007, loss_att=86.060, acc=0.341, loss=84.544, backward_time=0.112, optim_step_time=0.041, optim0_lr0=1.755e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 22:46:36,262 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:46:39,831 (trainer:676) INFO: 4epoch:train:329-369batch: iter_time=1.742e-04, forward_time=0.111, loss_ctc=80.870, loss_att=86.169, acc=0.340, loss=84.579, backward_time=0.111, optim_step_time=0.041, optim0_lr0=1.781e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 22:46:54,940 (trainer:676) INFO: 4epoch:train:370-410batch: iter_time=1.772e-04, forward_time=0.111, loss_ctc=79.859, loss_att=85.585, acc=0.345, loss=83.867, backward_time=0.107, optim_step_time=0.041, optim0_lr0=1.807e-04, train_time=0.368
[ip-10-0-0-95] 2022-04-27 22:47:10,410 (trainer:676) INFO: 4epoch:train:411-451batch: iter_time=1.765e-04, forward_time=0.112, loss_ctc=82.064, loss_att=88.503, acc=0.348, loss=86.571, backward_time=0.112, optim_step_time=0.041, optim0_lr0=1.833e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 22:47:26,081 (trainer:676) INFO: 4epoch:train:452-492batch: iter_time=1.670e-04, forward_time=0.112, loss_ctc=84.606, loss_att=93.423, acc=0.342, loss=90.778, backward_time=0.115, optim_step_time=0.041, optim0_lr0=1.859e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 22:47:41,528 (trainer:676) INFO: 4epoch:train:493-533batch: iter_time=1.797e-04, forward_time=0.112, loss_ctc=79.134, loss_att=84.125, acc=0.363, loss=82.628, backward_time=0.109, optim_step_time=0.041, optim0_lr0=1.885e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 22:47:57,105 (trainer:676) INFO: 4epoch:train:534-574batch: iter_time=1.635e-04, forward_time=0.112, loss_ctc=81.870, loss_att=86.610, acc=0.366, loss=85.188, backward_time=0.113, optim_step_time=0.041, optim0_lr0=1.911e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 22:48:12,300 (trainer:676) INFO: 4epoch:train:575-615batch: iter_time=1.716e-04, forward_time=0.111, loss_ctc=79.150, loss_att=82.666, acc=0.375, loss=81.611, backward_time=0.106, optim_step_time=0.041, optim0_lr0=1.937e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 22:48:27,856 (trainer:676) INFO: 4epoch:train:616-656batch: iter_time=1.751e-04, forward_time=0.111, loss_ctc=80.005, loss_att=84.629, acc=0.379, loss=83.242, backward_time=0.112, optim_step_time=0.041, optim0_lr0=1.963e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 22:48:43,446 (trainer:676) INFO: 4epoch:train:657-697batch: iter_time=1.712e-04, forward_time=0.111, loss_ctc=76.828, loss_att=79.544, acc=0.393, loss=78.729, backward_time=0.112, optim_step_time=0.041, optim0_lr0=1.988e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 22:48:58,964 (trainer:676) INFO: 4epoch:train:698-738batch: iter_time=1.430e-04, forward_time=0.112, loss_ctc=75.843, loss_att=76.742, acc=0.397, loss=76.472, backward_time=0.112, optim_step_time=0.041, optim0_lr0=2.014e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 22:49:14,253 (trainer:676) INFO: 4epoch:train:739-779batch: iter_time=1.251e-04, forward_time=0.112, loss_ctc=76.516, loss_att=77.478, acc=0.401, loss=77.190, backward_time=0.109, optim_step_time=0.041, optim0_lr0=2.040e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 22:49:20,130 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:49:29,746 (trainer:676) INFO: 4epoch:train:780-820batch: iter_time=1.227e-04, forward_time=0.111, loss_ctc=77.451, loss_att=78.488, acc=0.408, loss=78.177, backward_time=0.111, optim_step_time=0.041, optim0_lr0=2.066e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 22:50:09,248 (trainer:334) INFO: 4epoch results: [train] iter_time=4.084e-04, forward_time=0.112, loss_ctc=80.849, loss_att=85.535, acc=0.350, loss=84.129, backward_time=0.111, optim_step_time=0.041, optim0_lr0=1.821e-04, train_time=0.377, time=5 minutes and 10.65 seconds, total_count=3296, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=54.553, cer_ctc=0.284, loss_att=71.931, acc=0.465, cer=0.545, wer=1.000, loss=66.718, time=7.44 seconds, total_count=116, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.56 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 22:50:14,683 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 22:50:14,683 (trainer:268) INFO: 5/50epoch started. Estimated time to finish: 4 hours, 29 minutes and 49.99 seconds
[ip-10-0-0-95] 2022-04-27 22:50:29,922 (trainer:676) INFO: 5epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=76.939, loss_att=76.200, acc=0.431, loss=76.421, backward_time=0.104, optim_step_time=0.041, optim0_lr0=2.095e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 22:50:45,074 (trainer:676) INFO: 5epoch:train:42-82batch: iter_time=1.486e-04, forward_time=0.109, loss_ctc=76.846, loss_att=76.660, acc=0.432, loss=76.716, backward_time=0.108, optim_step_time=0.041, optim0_lr0=2.121e-04, train_time=0.369
[ip-10-0-0-95] 2022-04-27 22:51:00,130 (trainer:676) INFO: 5epoch:train:83-123batch: iter_time=1.598e-04, forward_time=0.109, loss_ctc=76.346, loss_att=73.162, acc=0.447, loss=74.117, backward_time=0.106, optim_step_time=0.041, optim0_lr0=2.147e-04, train_time=0.367
[ip-10-0-0-95] 2022-04-27 22:51:15,027 (trainer:676) INFO: 5epoch:train:124-164batch: iter_time=1.433e-04, forward_time=0.110, loss_ctc=70.356, loss_att=66.530, acc=0.459, loss=67.678, backward_time=0.105, optim_step_time=0.041, optim0_lr0=2.172e-04, train_time=0.363
[ip-10-0-0-95] 2022-04-27 22:51:30,269 (trainer:676) INFO: 5epoch:train:165-205batch: iter_time=1.287e-04, forward_time=0.111, loss_ctc=75.169, loss_att=73.322, acc=0.460, loss=73.876, backward_time=0.110, optim_step_time=0.041, optim0_lr0=2.198e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 22:51:45,598 (trainer:676) INFO: 5epoch:train:206-246batch: iter_time=1.293e-04, forward_time=0.111, loss_ctc=69.976, loss_att=64.783, acc=0.483, loss=66.341, backward_time=0.109, optim_step_time=0.041, optim0_lr0=2.224e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 22:52:01,029 (trainer:676) INFO: 5epoch:train:247-287batch: iter_time=1.361e-04, forward_time=0.111, loss_ctc=70.396, loss_att=64.042, acc=0.488, loss=65.948, backward_time=0.110, optim_step_time=0.041, optim0_lr0=2.250e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 22:52:16,707 (trainer:676) INFO: 5epoch:train:288-328batch: iter_time=1.378e-04, forward_time=0.111, loss_ctc=76.700, loss_att=70.202, acc=0.487, loss=72.151, backward_time=0.114, optim_step_time=0.042, optim0_lr0=2.276e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 22:52:31,897 (trainer:676) INFO: 5epoch:train:329-369batch: iter_time=1.266e-04, forward_time=0.111, loss_ctc=73.409, loss_att=66.314, acc=0.501, loss=68.443, backward_time=0.109, optim_step_time=0.041, optim0_lr0=2.302e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 22:52:47,092 (trainer:676) INFO: 5epoch:train:370-410batch: iter_time=1.326e-04, forward_time=0.111, loss_ctc=72.802, loss_att=63.389, acc=0.507, loss=66.213, backward_time=0.108, optim_step_time=0.041, optim0_lr0=2.328e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 22:53:02,490 (trainer:676) INFO: 5epoch:train:411-451batch: iter_time=1.281e-04, forward_time=0.110, loss_ctc=74.786, loss_att=64.104, acc=0.520, loss=67.308, backward_time=0.110, optim_step_time=0.041, optim0_lr0=2.354e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 22:53:17,987 (trainer:676) INFO: 5epoch:train:452-492batch: iter_time=1.377e-04, forward_time=0.112, loss_ctc=68.586, loss_att=59.890, acc=0.524, loss=62.499, backward_time=0.112, optim_step_time=0.041, optim0_lr0=2.380e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 22:53:21,523 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:53:33,636 (trainer:676) INFO: 5epoch:train:493-533batch: iter_time=1.232e-04, forward_time=0.112, loss_ctc=69.559, loss_att=60.661, acc=0.528, loss=63.330, backward_time=0.114, optim_step_time=0.041, optim0_lr0=2.406e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 22:53:48,861 (trainer:676) INFO: 5epoch:train:534-574batch: iter_time=1.612e-04, forward_time=0.111, loss_ctc=71.293, loss_att=60.676, acc=0.542, loss=63.861, backward_time=0.109, optim_step_time=0.041, optim0_lr0=2.432e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 22:54:04,658 (trainer:676) INFO: 5epoch:train:575-615batch: iter_time=1.398e-04, forward_time=0.116, loss_ctc=69.865, loss_att=58.259, acc=0.554, loss=61.741, backward_time=0.113, optim_step_time=0.041, optim0_lr0=2.458e-04, train_time=0.385
[ip-10-0-0-95] 2022-04-27 22:54:13,918 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:54:20,453 (trainer:676) INFO: 5epoch:train:616-656batch: iter_time=1.507e-04, forward_time=0.116, loss_ctc=69.041, loss_att=60.133, acc=0.540, loss=62.805, backward_time=0.115, optim_step_time=0.041, optim0_lr0=2.484e-04, train_time=0.385
[ip-10-0-0-95] 2022-04-27 22:54:36,245 (trainer:676) INFO: 5epoch:train:657-697batch: iter_time=1.309e-04, forward_time=0.115, loss_ctc=71.341, loss_att=59.035, acc=0.555, loss=62.727, backward_time=0.113, optim_step_time=0.041, optim0_lr0=2.510e-04, train_time=0.385
[ip-10-0-0-95] 2022-04-27 22:54:51,962 (trainer:676) INFO: 5epoch:train:698-738batch: iter_time=1.505e-04, forward_time=0.117, loss_ctc=68.694, loss_att=55.486, acc=0.569, loss=59.449, backward_time=0.112, optim_step_time=0.041, optim0_lr0=2.536e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 22:55:07,617 (trainer:676) INFO: 5epoch:train:739-779batch: iter_time=1.575e-04, forward_time=0.116, loss_ctc=69.445, loss_att=56.284, acc=0.568, loss=60.232, backward_time=0.111, optim_step_time=0.041, optim0_lr0=2.561e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 22:55:23,547 (trainer:676) INFO: 5epoch:train:780-820batch: iter_time=1.483e-04, forward_time=0.114, loss_ctc=69.610, loss_att=56.438, acc=0.582, loss=60.390, backward_time=0.118, optim_step_time=0.041, optim0_lr0=2.587e-04, train_time=0.388
[ip-10-0-0-95] 2022-04-27 22:56:03,147 (trainer:334) INFO: 5epoch results: [train] iter_time=4.012e-04, forward_time=0.112, loss_ctc=72.051, loss_att=64.259, acc=0.509, loss=66.597, backward_time=0.111, optim_step_time=0.041, optim0_lr0=2.342e-04, train_time=0.376, time=5 minutes and 10.51 seconds, total_count=4120, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=46.609, cer_ctc=0.240, loss_att=44.576, acc=0.668, cer=0.337, wer=0.999, loss=45.186, time=7.48 seconds, total_count=145, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.47 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 22:56:08,742 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 22:56:08,743 (trainer:268) INFO: 6/50epoch started. Estimated time to finish: 4 hours, 24 minutes and 16.96 seconds
[ip-10-0-0-95] 2022-04-27 22:56:24,202 (trainer:676) INFO: 6epoch:train:1-41batch: iter_time=0.005, forward_time=0.111, loss_ctc=68.273, loss_att=54.476, acc=0.595, loss=58.615, backward_time=0.109, optim_step_time=0.041, optim0_lr0=2.616e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 22:56:39,341 (trainer:676) INFO: 6epoch:train:42-82batch: iter_time=1.379e-04, forward_time=0.110, loss_ctc=67.205, loss_att=52.738, acc=0.591, loss=57.078, backward_time=0.107, optim_step_time=0.041, optim0_lr0=2.642e-04, train_time=0.369
[ip-10-0-0-95] 2022-04-27 22:56:54,519 (trainer:676) INFO: 6epoch:train:83-123batch: iter_time=1.317e-04, forward_time=0.110, loss_ctc=68.723, loss_att=53.490, acc=0.594, loss=58.060, backward_time=0.107, optim_step_time=0.041, optim0_lr0=2.668e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 22:56:56,808 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:57:09,602 (trainer:676) INFO: 6epoch:train:124-164batch: iter_time=1.284e-04, forward_time=0.110, loss_ctc=67.184, loss_att=51.912, acc=0.598, loss=56.494, backward_time=0.108, optim_step_time=0.041, optim0_lr0=2.694e-04, train_time=0.368
[ip-10-0-0-95] 2022-04-27 22:57:25,007 (trainer:676) INFO: 6epoch:train:165-205batch: iter_time=1.299e-04, forward_time=0.111, loss_ctc=68.921, loss_att=52.926, acc=0.600, loss=57.725, backward_time=0.112, optim_step_time=0.041, optim0_lr0=2.720e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 22:57:40,089 (trainer:676) INFO: 6epoch:train:206-246batch: iter_time=1.273e-04, forward_time=0.111, loss_ctc=65.686, loss_att=50.539, acc=0.603, loss=55.083, backward_time=0.106, optim_step_time=0.041, optim0_lr0=2.745e-04, train_time=0.368
[ip-10-0-0-95] 2022-04-27 22:57:55,246 (trainer:676) INFO: 6epoch:train:247-287batch: iter_time=1.309e-04, forward_time=0.110, loss_ctc=66.625, loss_att=49.818, acc=0.616, loss=54.860, backward_time=0.107, optim_step_time=0.041, optim0_lr0=2.771e-04, train_time=0.369
[ip-10-0-0-95] 2022-04-27 22:58:10,735 (trainer:676) INFO: 6epoch:train:288-328batch: iter_time=1.659e-04, forward_time=0.110, loss_ctc=65.092, loss_att=49.370, acc=0.617, loss=54.087, backward_time=0.112, optim_step_time=0.041, optim0_lr0=2.797e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 22:58:26,182 (trainer:676) INFO: 6epoch:train:329-369batch: iter_time=1.540e-04, forward_time=0.111, loss_ctc=67.612, loss_att=50.796, acc=0.620, loss=55.841, backward_time=0.112, optim_step_time=0.041, optim0_lr0=2.823e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 22:58:41,673 (trainer:676) INFO: 6epoch:train:370-410batch: iter_time=1.454e-04, forward_time=0.112, loss_ctc=59.543, loss_att=44.786, acc=0.621, loss=49.213, backward_time=0.113, optim_step_time=0.041, optim0_lr0=2.849e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 22:58:57,370 (trainer:676) INFO: 6epoch:train:411-451batch: iter_time=1.401e-04, forward_time=0.112, loss_ctc=63.599, loss_att=47.103, acc=0.626, loss=52.052, backward_time=0.114, optim_step_time=0.041, optim0_lr0=2.875e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 22:59:12,966 (trainer:676) INFO: 6epoch:train:452-492batch: iter_time=1.306e-04, forward_time=0.111, loss_ctc=66.035, loss_att=49.241, acc=0.639, loss=54.279, backward_time=0.114, optim_step_time=0.041, optim0_lr0=2.901e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 22:59:28,161 (trainer:676) INFO: 6epoch:train:493-533batch: iter_time=1.244e-04, forward_time=0.111, loss_ctc=64.652, loss_att=47.033, acc=0.630, loss=52.319, backward_time=0.107, optim_step_time=0.041, optim0_lr0=2.927e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 22:59:30,547 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 22:59:43,729 (trainer:676) INFO: 6epoch:train:534-574batch: iter_time=1.287e-04, forward_time=0.112, loss_ctc=61.866, loss_att=44.522, acc=0.634, loss=49.726, backward_time=0.112, optim_step_time=0.041, optim0_lr0=2.953e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 22:59:59,219 (trainer:676) INFO: 6epoch:train:575-615batch: iter_time=1.239e-04, forward_time=0.112, loss_ctc=66.318, loss_att=48.760, acc=0.651, loss=54.027, backward_time=0.114, optim_step_time=0.041, optim0_lr0=2.979e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:00:14,851 (trainer:676) INFO: 6epoch:train:616-656batch: iter_time=1.265e-04, forward_time=0.111, loss_ctc=63.984, loss_att=45.721, acc=0.642, loss=51.200, backward_time=0.112, optim_step_time=0.041, optim0_lr0=3.005e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:00:30,510 (trainer:676) INFO: 6epoch:train:657-697batch: iter_time=1.224e-04, forward_time=0.111, loss_ctc=63.263, loss_att=44.800, acc=0.651, loss=50.339, backward_time=0.113, optim_step_time=0.041, optim0_lr0=3.031e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 23:00:46,036 (trainer:676) INFO: 6epoch:train:698-738batch: iter_time=1.279e-04, forward_time=0.111, loss_ctc=63.906, loss_att=44.229, acc=0.652, loss=50.132, backward_time=0.112, optim_step_time=0.041, optim0_lr0=3.057e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:01:01,216 (trainer:676) INFO: 6epoch:train:739-779batch: iter_time=1.275e-04, forward_time=0.112, loss_ctc=62.618, loss_att=45.317, acc=0.658, loss=50.507, backward_time=0.109, optim_step_time=0.041, optim0_lr0=3.083e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 23:01:17,180 (trainer:676) INFO: 6epoch:train:780-820batch: iter_time=1.216e-04, forward_time=0.112, loss_ctc=66.169, loss_att=46.061, acc=0.662, loss=52.093, backward_time=0.118, optim_step_time=0.041, optim0_lr0=3.109e-04, train_time=0.389
[ip-10-0-0-95] 2022-04-27 23:01:56,872 (trainer:334) INFO: 6epoch results: [train] iter_time=3.704e-04, forward_time=0.111, loss_ctc=65.291, loss_att=48.596, acc=0.625, loss=53.605, backward_time=0.111, optim_step_time=0.041, optim0_lr0=2.863e-04, train_time=0.376, time=5 minutes and 9.98 seconds, total_count=4944, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=43.606, cer_ctc=0.227, loss_att=35.895, acc=0.736, cer=0.255, wer=0.997, loss=38.209, time=7.46 seconds, total_count=174, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.69 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 23:02:02,368 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 23:02:02,369 (trainer:268) INFO: 7/50epoch started. Estimated time to finish: 4 hours, 18 minutes and 33.74 seconds
[ip-10-0-0-95] 2022-04-27 23:02:14,502 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:02:17,620 (trainer:676) INFO: 7epoch:train:1-41batch: iter_time=0.005, forward_time=0.111, loss_ctc=60.015, loss_att=41.203, acc=0.663, loss=46.847, backward_time=0.106, optim_step_time=0.041, optim0_lr0=3.137e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-27 23:02:32,881 (trainer:676) INFO: 7epoch:train:42-82batch: iter_time=1.695e-04, forward_time=0.110, loss_ctc=61.456, loss_att=42.344, acc=0.676, loss=48.078, backward_time=0.110, optim_step_time=0.041, optim0_lr0=3.163e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-27 23:02:47,974 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:02:48,266 (trainer:676) INFO: 7epoch:train:83-123batch: iter_time=1.377e-04, forward_time=0.111, loss_ctc=57.442, loss_att=38.441, acc=0.672, loss=44.141, backward_time=0.111, optim_step_time=0.041, optim0_lr0=3.189e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:03:03,529 (trainer:676) INFO: 7epoch:train:124-164batch: iter_time=1.513e-04, forward_time=0.110, loss_ctc=60.662, loss_att=40.864, acc=0.668, loss=46.803, backward_time=0.109, optim_step_time=0.041, optim0_lr0=3.215e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-27 23:03:19,136 (trainer:676) INFO: 7epoch:train:165-205batch: iter_time=1.316e-04, forward_time=0.110, loss_ctc=60.737, loss_att=41.319, acc=0.684, loss=47.145, backward_time=0.113, optim_step_time=0.041, optim0_lr0=3.241e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 23:03:34,566 (trainer:676) INFO: 7epoch:train:206-246batch: iter_time=1.363e-04, forward_time=0.110, loss_ctc=60.125, loss_att=41.137, acc=0.678, loss=46.833, backward_time=0.111, optim_step_time=0.041, optim0_lr0=3.267e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:03:50,075 (trainer:676) INFO: 7epoch:train:247-287batch: iter_time=1.291e-04, forward_time=0.114, loss_ctc=62.860, loss_att=42.628, acc=0.689, loss=48.697, backward_time=0.111, optim_step_time=0.041, optim0_lr0=3.293e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:04:05,698 (trainer:676) INFO: 7epoch:train:288-328batch: iter_time=1.374e-04, forward_time=0.115, loss_ctc=58.009, loss_att=38.379, acc=0.697, loss=44.268, backward_time=0.112, optim_step_time=0.041, optim0_lr0=3.318e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:04:21,424 (trainer:676) INFO: 7epoch:train:329-369batch: iter_time=1.412e-04, forward_time=0.115, loss_ctc=61.357, loss_att=41.191, acc=0.687, loss=47.241, backward_time=0.113, optim_step_time=0.041, optim0_lr0=3.344e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 23:04:36,664 (trainer:676) INFO: 7epoch:train:370-410batch: iter_time=1.571e-04, forward_time=0.110, loss_ctc=61.101, loss_att=41.320, acc=0.695, loss=47.254, backward_time=0.110, optim_step_time=0.041, optim0_lr0=3.370e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 23:04:52,362 (trainer:676) INFO: 7epoch:train:411-451batch: iter_time=1.294e-04, forward_time=0.112, loss_ctc=60.116, loss_att=39.937, acc=0.695, loss=45.990, backward_time=0.114, optim_step_time=0.041, optim0_lr0=3.396e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 23:05:07,573 (trainer:676) INFO: 7epoch:train:452-492batch: iter_time=1.313e-04, forward_time=0.111, loss_ctc=62.698, loss_att=41.984, acc=0.696, loss=48.198, backward_time=0.108, optim_step_time=0.041, optim0_lr0=3.422e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 23:05:23,185 (trainer:676) INFO: 7epoch:train:493-533batch: iter_time=1.270e-04, forward_time=0.113, loss_ctc=57.989, loss_att=37.901, acc=0.711, loss=43.928, backward_time=0.114, optim_step_time=0.041, optim0_lr0=3.448e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 23:05:38,348 (trainer:676) INFO: 7epoch:train:534-574batch: iter_time=1.342e-04, forward_time=0.111, loss_ctc=62.242, loss_att=39.970, acc=0.700, loss=46.651, backward_time=0.107, optim_step_time=0.041, optim0_lr0=3.474e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 23:05:53,595 (trainer:676) INFO: 7epoch:train:575-615batch: iter_time=1.256e-04, forward_time=0.112, loss_ctc=57.508, loss_att=36.077, acc=0.708, loss=42.506, backward_time=0.108, optim_step_time=0.041, optim0_lr0=3.500e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-27 23:06:09,255 (trainer:676) INFO: 7epoch:train:616-656batch: iter_time=1.296e-04, forward_time=0.111, loss_ctc=56.585, loss_att=37.669, acc=0.699, loss=43.343, backward_time=0.113, optim_step_time=0.041, optim0_lr0=3.526e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 23:06:24,643 (trainer:676) INFO: 7epoch:train:657-697batch: iter_time=1.274e-04, forward_time=0.111, loss_ctc=59.520, loss_att=38.174, acc=0.707, loss=44.578, backward_time=0.109, optim_step_time=0.041, optim0_lr0=3.552e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:06:40,177 (trainer:676) INFO: 7epoch:train:698-738batch: iter_time=1.325e-04, forward_time=0.112, loss_ctc=59.923, loss_att=39.133, acc=0.706, loss=45.370, backward_time=0.112, optim_step_time=0.041, optim0_lr0=3.578e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:06:55,866 (trainer:676) INFO: 7epoch:train:739-779batch: iter_time=1.416e-04, forward_time=0.112, loss_ctc=57.214, loss_att=36.494, acc=0.706, loss=42.710, backward_time=0.114, optim_step_time=0.041, optim0_lr0=3.604e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 23:07:11,268 (trainer:676) INFO: 7epoch:train:780-820batch: iter_time=1.230e-04, forward_time=0.112, loss_ctc=59.642, loss_att=38.004, acc=0.720, loss=44.496, backward_time=0.111, optim_step_time=0.041, optim0_lr0=3.630e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:07:50,843 (trainer:334) INFO: 7epoch results: [train] iter_time=3.955e-04, forward_time=0.112, loss_ctc=59.814, loss_att=39.673, acc=0.693, loss=45.716, backward_time=0.111, optim_step_time=0.041, optim0_lr0=3.385e-04, train_time=0.376, time=5 minutes and 10.33 seconds, total_count=5768, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=39.573, cer_ctc=0.205, loss_att=28.465, acc=0.793, cer=0.208, wer=0.989, loss=31.797, time=7.45 seconds, total_count=203, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.69 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 23:07:56,371 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 23:07:56,371 (trainer:268) INFO: 8/50epoch started. Estimated time to finish: 4 hours, 12 minutes and 49.86 seconds
[ip-10-0-0-95] 2022-04-27 23:08:11,385 (trainer:676) INFO: 8epoch:train:1-41batch: iter_time=0.006, forward_time=0.109, loss_ctc=56.984, loss_att=35.306, acc=0.726, loss=41.809, backward_time=0.104, optim_step_time=0.041, optim0_lr0=3.658e-04, train_time=0.366
[ip-10-0-0-95] 2022-04-27 23:08:26,554 (trainer:676) INFO: 8epoch:train:42-82batch: iter_time=1.607e-04, forward_time=0.110, loss_ctc=58.188, loss_att=36.634, acc=0.731, loss=43.100, backward_time=0.109, optim_step_time=0.041, optim0_lr0=3.684e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 23:08:37,509 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:08:41,780 (trainer:676) INFO: 8epoch:train:83-123batch: iter_time=1.696e-04, forward_time=0.110, loss_ctc=57.570, loss_att=36.422, acc=0.720, loss=42.767, backward_time=0.110, optim_step_time=0.041, optim0_lr0=3.710e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 23:08:56,900 (trainer:676) INFO: 8epoch:train:124-164batch: iter_time=1.725e-04, forward_time=0.110, loss_ctc=54.734, loss_att=34.059, acc=0.727, loss=40.262, backward_time=0.108, optim_step_time=0.041, optim0_lr0=3.736e-04, train_time=0.368
[ip-10-0-0-95] 2022-04-27 23:09:12,112 (trainer:676) INFO: 8epoch:train:165-205batch: iter_time=1.492e-04, forward_time=0.109, loss_ctc=57.060, loss_att=35.560, acc=0.730, loss=42.010, backward_time=0.108, optim_step_time=0.041, optim0_lr0=3.762e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 23:09:27,489 (trainer:676) INFO: 8epoch:train:206-246batch: iter_time=1.746e-04, forward_time=0.111, loss_ctc=54.396, loss_att=33.234, acc=0.724, loss=39.583, backward_time=0.111, optim_step_time=0.042, optim0_lr0=3.788e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:09:42,887 (trainer:676) INFO: 8epoch:train:247-287batch: iter_time=1.438e-04, forward_time=0.112, loss_ctc=57.328, loss_att=35.453, acc=0.739, loss=42.016, backward_time=0.111, optim_step_time=0.042, optim0_lr0=3.814e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:09:58,318 (trainer:676) INFO: 8epoch:train:288-328batch: iter_time=1.649e-04, forward_time=0.111, loss_ctc=54.213, loss_att=32.798, acc=0.730, loss=39.222, backward_time=0.111, optim_step_time=0.041, optim0_lr0=3.840e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:10:13,471 (trainer:676) INFO: 8epoch:train:329-369batch: iter_time=1.705e-04, forward_time=0.110, loss_ctc=54.490, loss_att=33.368, acc=0.737, loss=39.705, backward_time=0.107, optim_step_time=0.041, optim0_lr0=3.866e-04, train_time=0.369
[ip-10-0-0-95] 2022-04-27 23:10:28,943 (trainer:676) INFO: 8epoch:train:370-410batch: iter_time=1.542e-04, forward_time=0.110, loss_ctc=54.192, loss_att=33.292, acc=0.739, loss=39.562, backward_time=0.112, optim_step_time=0.041, optim0_lr0=3.891e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:10:44,813 (trainer:676) INFO: 8epoch:train:411-451batch: iter_time=1.545e-04, forward_time=0.113, loss_ctc=53.938, loss_att=33.623, acc=0.738, loss=39.717, backward_time=0.120, optim_step_time=0.041, optim0_lr0=3.917e-04, train_time=0.387
[ip-10-0-0-95] 2022-04-27 23:10:49,482 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:11:00,452 (trainer:676) INFO: 8epoch:train:452-492batch: iter_time=1.595e-04, forward_time=0.116, loss_ctc=51.101, loss_att=31.646, acc=0.729, loss=37.483, backward_time=0.112, optim_step_time=0.041, optim0_lr0=3.943e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:11:16,022 (trainer:676) INFO: 8epoch:train:493-533batch: iter_time=1.614e-04, forward_time=0.115, loss_ctc=55.325, loss_att=33.689, acc=0.737, loss=40.180, backward_time=0.109, optim_step_time=0.041, optim0_lr0=3.969e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:11:31,414 (trainer:676) INFO: 8epoch:train:534-574batch: iter_time=1.627e-04, forward_time=0.114, loss_ctc=53.187, loss_att=31.527, acc=0.739, loss=38.025, backward_time=0.108, optim_step_time=0.041, optim0_lr0=3.995e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:11:46,968 (trainer:676) INFO: 8epoch:train:575-615batch: iter_time=1.475e-04, forward_time=0.112, loss_ctc=56.788, loss_att=34.712, acc=0.746, loss=41.335, backward_time=0.114, optim_step_time=0.041, optim0_lr0=4.021e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:12:02,309 (trainer:676) INFO: 8epoch:train:616-656batch: iter_time=1.721e-04, forward_time=0.110, loss_ctc=54.222, loss_att=32.355, acc=0.747, loss=38.915, backward_time=0.109, optim_step_time=0.041, optim0_lr0=4.047e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:12:17,932 (trainer:676) INFO: 8epoch:train:657-697batch: iter_time=1.268e-04, forward_time=0.111, loss_ctc=54.332, loss_att=32.685, acc=0.744, loss=39.179, backward_time=0.114, optim_step_time=0.041, optim0_lr0=4.073e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:12:33,223 (trainer:676) INFO: 8epoch:train:698-738batch: iter_time=1.210e-04, forward_time=0.110, loss_ctc=53.179, loss_att=31.919, acc=0.758, loss=38.297, backward_time=0.110, optim_step_time=0.041, optim0_lr0=4.099e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:12:48,578 (trainer:676) INFO: 8epoch:train:739-779batch: iter_time=1.240e-04, forward_time=0.111, loss_ctc=55.457, loss_att=33.687, acc=0.758, loss=40.218, backward_time=0.110, optim_step_time=0.041, optim0_lr0=4.125e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:13:04,086 (trainer:676) INFO: 8epoch:train:780-820batch: iter_time=1.255e-04, forward_time=0.112, loss_ctc=55.518, loss_att=33.225, acc=0.746, loss=39.913, backward_time=0.113, optim_step_time=0.041, optim0_lr0=4.151e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:13:43,526 (trainer:334) INFO: 8epoch results: [train] iter_time=4.247e-04, forward_time=0.111, loss_ctc=55.067, loss_att=33.721, acc=0.737, loss=40.125, backward_time=0.110, optim_step_time=0.041, optim0_lr0=3.906e-04, train_time=0.375, time=5 minutes and 9.29 seconds, total_count=6592, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=35.584, cer_ctc=0.182, loss_att=23.993, acc=0.827, cer=0.177, wer=0.969, loss=27.470, time=7.36 seconds, total_count=232, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.51 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 23:13:49,236 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 23:13:49,236 (trainer:268) INFO: 9/50epoch started. Estimated time to finish: 4 hours, 6 minutes and 57.48 seconds
[ip-10-0-0-95] 2022-04-27 23:14:04,761 (trainer:676) INFO: 9epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=51.508, loss_att=30.375, acc=0.769, loss=36.715, backward_time=0.112, optim_step_time=0.041, optim0_lr0=4.179e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:14:19,680 (trainer:676) INFO: 9epoch:train:42-82batch: iter_time=1.819e-04, forward_time=0.108, loss_ctc=52.689, loss_att=30.366, acc=0.759, loss=37.063, backward_time=0.106, optim_step_time=0.041, optim0_lr0=4.205e-04, train_time=0.364
[ip-10-0-0-95] 2022-04-27 23:14:35,038 (trainer:676) INFO: 9epoch:train:83-123batch: iter_time=1.452e-04, forward_time=0.114, loss_ctc=52.398, loss_att=30.974, acc=0.766, loss=37.401, backward_time=0.110, optim_step_time=0.041, optim0_lr0=4.231e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:14:50,516 (trainer:676) INFO: 9epoch:train:124-164batch: iter_time=1.321e-04, forward_time=0.111, loss_ctc=52.003, loss_att=30.844, acc=0.759, loss=37.192, backward_time=0.113, optim_step_time=0.041, optim0_lr0=4.257e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:15:05,834 (trainer:676) INFO: 9epoch:train:165-205batch: iter_time=1.289e-04, forward_time=0.110, loss_ctc=50.676, loss_att=29.627, acc=0.756, loss=35.941, backward_time=0.111, optim_step_time=0.041, optim0_lr0=4.283e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:15:21,029 (trainer:676) INFO: 9epoch:train:206-246batch: iter_time=1.492e-04, forward_time=0.111, loss_ctc=52.739, loss_att=30.937, acc=0.763, loss=37.478, backward_time=0.108, optim_step_time=0.041, optim0_lr0=4.309e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 23:15:23,179 (trainer:618) WARNING: The grad norm is inf. Skipping updating the model.
[ip-10-0-0-95] 2022-04-27 23:15:36,340 (trainer:676) INFO: 9epoch:train:247-287batch: iter_time=1.275e-04, forward_time=0.111, loss_ctc=50.288, loss_att=30.237, acc=0.754, loss=36.253, backward_time=0.110, optim_step_time=0.041, optim0_lr0=4.334e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:15:51,487 (trainer:676) INFO: 9epoch:train:288-328batch: iter_time=1.371e-04, forward_time=0.110, loss_ctc=53.868, loss_att=31.700, acc=0.762, loss=38.350, backward_time=0.108, optim_step_time=0.041, optim0_lr0=4.360e-04, train_time=0.369
[ip-10-0-0-95] 2022-04-27 23:16:02,585 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:16:06,444 (trainer:676) INFO: 9epoch:train:329-369batch: iter_time=1.373e-04, forward_time=0.110, loss_ctc=52.008, loss_att=30.358, acc=0.756, loss=36.853, backward_time=0.106, optim_step_time=0.041, optim0_lr0=4.386e-04, train_time=0.365
[ip-10-0-0-95] 2022-04-27 23:16:21,740 (trainer:676) INFO: 9epoch:train:370-410batch: iter_time=1.610e-04, forward_time=0.110, loss_ctc=53.847, loss_att=31.131, acc=0.763, loss=37.945, backward_time=0.109, optim_step_time=0.041, optim0_lr0=4.412e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:16:37,103 (trainer:676) INFO: 9epoch:train:411-451batch: iter_time=1.464e-04, forward_time=0.109, loss_ctc=53.080, loss_att=30.759, acc=0.776, loss=37.455, backward_time=0.110, optim_step_time=0.041, optim0_lr0=4.438e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:16:52,657 (trainer:676) INFO: 9epoch:train:452-492batch: iter_time=1.324e-04, forward_time=0.112, loss_ctc=51.227, loss_att=29.445, acc=0.770, loss=35.979, backward_time=0.113, optim_step_time=0.041, optim0_lr0=4.464e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:17:08,069 (trainer:676) INFO: 9epoch:train:493-533batch: iter_time=1.569e-04, forward_time=0.112, loss_ctc=48.636, loss_att=27.963, acc=0.767, loss=34.165, backward_time=0.111, optim_step_time=0.041, optim0_lr0=4.490e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:17:23,504 (trainer:676) INFO: 9epoch:train:534-574batch: iter_time=1.727e-04, forward_time=0.111, loss_ctc=50.271, loss_att=28.326, acc=0.767, loss=34.910, backward_time=0.109, optim_step_time=0.041, optim0_lr0=4.516e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:17:38,818 (trainer:676) INFO: 9epoch:train:575-615batch: iter_time=1.553e-04, forward_time=0.111, loss_ctc=52.087, loss_att=29.823, acc=0.771, loss=36.502, backward_time=0.109, optim_step_time=0.041, optim0_lr0=4.542e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:17:44,500 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:17:54,162 (trainer:676) INFO: 9epoch:train:616-656batch: iter_time=1.468e-04, forward_time=0.111, loss_ctc=47.912, loss_att=27.410, acc=0.774, loss=33.561, backward_time=0.111, optim_step_time=0.041, optim0_lr0=4.568e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:18:09,810 (trainer:676) INFO: 9epoch:train:657-697batch: iter_time=1.471e-04, forward_time=0.112, loss_ctc=49.935, loss_att=28.599, acc=0.774, loss=35.000, backward_time=0.113, optim_step_time=0.041, optim0_lr0=4.594e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:18:11,957 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-27 23:18:25,246 (trainer:676) INFO: 9epoch:train:698-738batch: iter_time=1.339e-04, forward_time=0.112, loss_ctc=49.814, loss_att=28.421, acc=0.779, loss=34.839, backward_time=0.110, optim_step_time=0.041, optim0_lr0=4.619e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:18:40,839 (trainer:676) INFO: 9epoch:train:739-779batch: iter_time=1.416e-04, forward_time=0.112, loss_ctc=49.577, loss_att=28.362, acc=0.777, loss=34.726, backward_time=0.111, optim_step_time=0.041, optim0_lr0=4.645e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 23:18:56,387 (trainer:676) INFO: 9epoch:train:780-820batch: iter_time=1.333e-04, forward_time=0.111, loss_ctc=52.051, loss_att=29.281, acc=0.771, loss=36.112, backward_time=0.112, optim_step_time=0.041, optim0_lr0=4.671e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:19:35,966 (trainer:334) INFO: 9epoch results: [train] iter_time=3.957e-04, forward_time=0.111, loss_ctc=51.288, loss_att=29.712, acc=0.767, loss=36.185, backward_time=0.110, optim_step_time=0.041, optim0_lr0=4.426e-04, train_time=0.374, time=5 minutes and 8.7 seconds, total_count=7416, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=32.988, cer_ctc=0.174, loss_att=21.572, acc=0.844, cer=0.154, wer=0.971, loss=24.997, time=7.46 seconds, total_count=261, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.57 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 23:19:41,520 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 23:19:41,521 (trainer:268) INFO: 10/50epoch started. Estimated time to finish: 4 hours, 1 minute and 2.35 seconds
[ip-10-0-0-95] 2022-04-27 23:19:57,111 (trainer:676) INFO: 10epoch:train:1-41batch: iter_time=0.005, forward_time=0.112, loss_ctc=46.871, loss_att=25.923, acc=0.787, loss=32.208, backward_time=0.111, optim_step_time=0.041, optim0_lr0=4.699e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 23:20:12,110 (trainer:676) INFO: 10epoch:train:42-82batch: iter_time=1.255e-04, forward_time=0.108, loss_ctc=50.249, loss_att=28.101, acc=0.784, loss=34.746, backward_time=0.107, optim_step_time=0.041, optim0_lr0=4.725e-04, train_time=0.366
[ip-10-0-0-95] 2022-04-27 23:20:27,221 (trainer:676) INFO: 10epoch:train:83-123batch: iter_time=1.454e-04, forward_time=0.110, loss_ctc=47.611, loss_att=27.089, acc=0.790, loss=33.245, backward_time=0.108, optim_step_time=0.041, optim0_lr0=4.751e-04, train_time=0.368
[ip-10-0-0-95] 2022-04-27 23:20:42,438 (trainer:676) INFO: 10epoch:train:124-164batch: iter_time=1.672e-04, forward_time=0.110, loss_ctc=50.777, loss_att=29.010, acc=0.792, loss=35.540, backward_time=0.109, optim_step_time=0.041, optim0_lr0=4.777e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 23:20:57,793 (trainer:676) INFO: 10epoch:train:165-205batch: iter_time=1.602e-04, forward_time=0.110, loss_ctc=47.755, loss_att=26.754, acc=0.793, loss=33.055, backward_time=0.112, optim_step_time=0.041, optim0_lr0=4.803e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:21:12,794 (trainer:676) INFO: 10epoch:train:206-246batch: iter_time=1.273e-04, forward_time=0.110, loss_ctc=48.933, loss_att=27.854, acc=0.789, loss=34.177, backward_time=0.107, optim_step_time=0.041, optim0_lr0=4.829e-04, train_time=0.366
[ip-10-0-0-95] 2022-04-27 23:21:28,128 (trainer:676) INFO: 10epoch:train:247-287batch: iter_time=1.281e-04, forward_time=0.110, loss_ctc=48.307, loss_att=26.942, acc=0.794, loss=33.352, backward_time=0.110, optim_step_time=0.041, optim0_lr0=4.855e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:21:30,857 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:21:43,440 (trainer:676) INFO: 10epoch:train:288-328batch: iter_time=1.341e-04, forward_time=0.110, loss_ctc=47.822, loss_att=26.892, acc=0.785, loss=33.171, backward_time=0.110, optim_step_time=0.041, optim0_lr0=4.881e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:21:58,842 (trainer:676) INFO: 10epoch:train:329-369batch: iter_time=1.273e-04, forward_time=0.109, loss_ctc=48.392, loss_att=26.838, acc=0.788, loss=33.304, backward_time=0.111, optim_step_time=0.041, optim0_lr0=4.907e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:22:13,930 (trainer:676) INFO: 10epoch:train:370-410batch: iter_time=1.493e-04, forward_time=0.111, loss_ctc=47.040, loss_att=26.315, acc=0.789, loss=32.532, backward_time=0.108, optim_step_time=0.041, optim0_lr0=4.933e-04, train_time=0.368
[ip-10-0-0-95] 2022-04-27 23:22:29,302 (trainer:676) INFO: 10epoch:train:411-451batch: iter_time=1.600e-04, forward_time=0.111, loss_ctc=46.392, loss_att=25.339, acc=0.794, loss=31.655, backward_time=0.112, optim_step_time=0.041, optim0_lr0=4.958e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:22:44,974 (trainer:676) INFO: 10epoch:train:452-492batch: iter_time=1.198e-04, forward_time=0.111, loss_ctc=48.880, loss_att=26.738, acc=0.792, loss=33.380, backward_time=0.115, optim_step_time=0.041, optim0_lr0=4.984e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 23:22:55,737 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:23:00,505 (trainer:676) INFO: 10epoch:train:493-533batch: iter_time=1.271e-04, forward_time=0.111, loss_ctc=44.522, loss_att=24.645, acc=0.779, loss=30.608, backward_time=0.113, optim_step_time=0.041, optim0_lr0=5.010e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:23:15,974 (trainer:676) INFO: 10epoch:train:534-574batch: iter_time=1.340e-04, forward_time=0.112, loss_ctc=48.036, loss_att=26.916, acc=0.797, loss=33.252, backward_time=0.111, optim_step_time=0.041, optim0_lr0=5.036e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:23:31,509 (trainer:676) INFO: 10epoch:train:575-615batch: iter_time=1.263e-04, forward_time=0.111, loss_ctc=48.623, loss_att=26.723, acc=0.795, loss=33.293, backward_time=0.112, optim_step_time=0.041, optim0_lr0=5.062e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:23:47,205 (trainer:676) INFO: 10epoch:train:616-656batch: iter_time=1.315e-04, forward_time=0.112, loss_ctc=45.253, loss_att=24.764, acc=0.797, loss=30.911, backward_time=0.116, optim_step_time=0.041, optim0_lr0=5.088e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 23:24:02,013 (trainer:676) INFO: 10epoch:train:657-697batch: iter_time=1.301e-04, forward_time=0.109, loss_ctc=46.002, loss_att=25.160, acc=0.794, loss=31.413, backward_time=0.101, optim_step_time=0.041, optim0_lr0=5.114e-04, train_time=0.361
[ip-10-0-0-95] 2022-04-27 23:24:17,565 (trainer:676) INFO: 10epoch:train:698-738batch: iter_time=1.311e-04, forward_time=0.111, loss_ctc=48.059, loss_att=26.315, acc=0.798, loss=32.838, backward_time=0.112, optim_step_time=0.041, optim0_lr0=5.140e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:24:32,888 (trainer:676) INFO: 10epoch:train:739-779batch: iter_time=1.350e-04, forward_time=0.112, loss_ctc=46.366, loss_att=25.429, acc=0.792, loss=31.710, backward_time=0.109, optim_step_time=0.041, optim0_lr0=5.166e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:24:48,398 (trainer:676) INFO: 10epoch:train:780-820batch: iter_time=1.337e-04, forward_time=0.111, loss_ctc=47.451, loss_att=26.170, acc=0.801, loss=32.554, backward_time=0.111, optim_step_time=0.041, optim0_lr0=5.192e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:25:27,232 (trainer:334) INFO: 10epoch results: [train] iter_time=3.916e-04, forward_time=0.110, loss_ctc=47.626, loss_att=26.469, acc=0.791, loss=32.816, backward_time=0.110, optim_step_time=0.041, optim0_lr0=4.947e-04, train_time=0.374, time=5 minutes and 8.44 seconds, total_count=8240, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=30.031, cer_ctc=0.153, loss_att=20.351, acc=0.854, cer=0.155, wer=0.960, loss=23.255, time=7.39 seconds, total_count=290, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.88 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 23:25:32,933 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 23:25:32,934 (trainer:268) INFO: 11/50epoch started. Estimated time to finish: 3 hours, 55 minutes and 4.3 seconds
[ip-10-0-0-95] 2022-04-27 23:25:48,298 (trainer:676) INFO: 11epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=45.102, loss_att=24.434, acc=0.813, loss=30.634, backward_time=0.108, optim_step_time=0.041, optim0_lr0=5.220e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:25:48,389 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:26:03,340 (trainer:676) INFO: 11epoch:train:42-82batch: iter_time=1.320e-04, forward_time=0.110, loss_ctc=44.106, loss_att=23.875, acc=0.803, loss=29.944, backward_time=0.108, optim_step_time=0.041, optim0_lr0=5.246e-04, train_time=0.367
[ip-10-0-0-95] 2022-04-27 23:26:18,610 (trainer:676) INFO: 11epoch:train:83-123batch: iter_time=1.287e-04, forward_time=0.110, loss_ctc=44.021, loss_att=23.508, acc=0.809, loss=29.662, backward_time=0.111, optim_step_time=0.041, optim0_lr0=5.272e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-27 23:26:33,588 (trainer:676) INFO: 11epoch:train:124-164batch: iter_time=1.393e-04, forward_time=0.109, loss_ctc=43.981, loss_att=23.750, acc=0.808, loss=29.819, backward_time=0.105, optim_step_time=0.041, optim0_lr0=5.298e-04, train_time=0.365
[ip-10-0-0-95] 2022-04-27 23:26:48,725 (trainer:676) INFO: 11epoch:train:165-205batch: iter_time=1.245e-04, forward_time=0.110, loss_ctc=46.235, loss_att=25.397, acc=0.805, loss=31.649, backward_time=0.110, optim_step_time=0.041, optim0_lr0=5.324e-04, train_time=0.369
[ip-10-0-0-95] 2022-04-27 23:27:02,532 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:27:03,912 (trainer:676) INFO: 11epoch:train:206-246batch: iter_time=1.373e-04, forward_time=0.110, loss_ctc=44.318, loss_att=23.877, acc=0.795, loss=30.009, backward_time=0.108, optim_step_time=0.041, optim0_lr0=5.350e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 23:27:19,324 (trainer:676) INFO: 11epoch:train:247-287batch: iter_time=1.672e-04, forward_time=0.110, loss_ctc=45.076, loss_att=24.445, acc=0.813, loss=30.634, backward_time=0.111, optim_step_time=0.041, optim0_lr0=5.376e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:27:34,313 (trainer:676) INFO: 11epoch:train:288-328batch: iter_time=1.699e-04, forward_time=0.109, loss_ctc=44.807, loss_att=24.433, acc=0.812, loss=30.545, backward_time=0.107, optim_step_time=0.041, optim0_lr0=5.402e-04, train_time=0.365
[ip-10-0-0-95] 2022-04-27 23:27:49,588 (trainer:676) INFO: 11epoch:train:329-369batch: iter_time=1.636e-04, forward_time=0.110, loss_ctc=45.486, loss_att=24.662, acc=0.813, loss=30.909, backward_time=0.109, optim_step_time=0.041, optim0_lr0=5.428e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-27 23:28:05,146 (trainer:676) INFO: 11epoch:train:370-410batch: iter_time=1.235e-04, forward_time=0.111, loss_ctc=44.802, loss_att=24.204, acc=0.809, loss=30.384, backward_time=0.113, optim_step_time=0.041, optim0_lr0=5.454e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:28:20,493 (trainer:676) INFO: 11epoch:train:411-451batch: iter_time=1.194e-04, forward_time=0.110, loss_ctc=45.422, loss_att=24.912, acc=0.806, loss=31.065, backward_time=0.109, optim_step_time=0.041, optim0_lr0=5.480e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:28:35,882 (trainer:676) INFO: 11epoch:train:452-492batch: iter_time=1.207e-04, forward_time=0.111, loss_ctc=45.076, loss_att=24.251, acc=0.816, loss=30.498, backward_time=0.111, optim_step_time=0.041, optim0_lr0=5.506e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:28:51,294 (trainer:676) INFO: 11epoch:train:493-533batch: iter_time=1.188e-04, forward_time=0.110, loss_ctc=45.624, loss_att=24.553, acc=0.809, loss=30.875, backward_time=0.111, optim_step_time=0.041, optim0_lr0=5.531e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:29:06,854 (trainer:676) INFO: 11epoch:train:534-574batch: iter_time=1.192e-04, forward_time=0.112, loss_ctc=44.365, loss_att=23.644, acc=0.810, loss=29.860, backward_time=0.113, optim_step_time=0.041, optim0_lr0=5.557e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:29:22,260 (trainer:676) INFO: 11epoch:train:575-615batch: iter_time=1.237e-04, forward_time=0.111, loss_ctc=43.222, loss_att=23.168, acc=0.812, loss=29.184, backward_time=0.110, optim_step_time=0.041, optim0_lr0=5.583e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:29:37,812 (trainer:676) INFO: 11epoch:train:616-656batch: iter_time=1.277e-04, forward_time=0.112, loss_ctc=43.780, loss_att=23.092, acc=0.809, loss=29.298, backward_time=0.112, optim_step_time=0.041, optim0_lr0=5.609e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:29:53,204 (trainer:676) INFO: 11epoch:train:657-697batch: iter_time=1.217e-04, forward_time=0.112, loss_ctc=43.797, loss_att=23.200, acc=0.812, loss=29.379, backward_time=0.110, optim_step_time=0.041, optim0_lr0=5.635e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:30:08,539 (trainer:676) INFO: 11epoch:train:698-738batch: iter_time=1.198e-04, forward_time=0.112, loss_ctc=44.184, loss_att=23.189, acc=0.815, loss=29.487, backward_time=0.110, optim_step_time=0.041, optim0_lr0=5.661e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:30:24,095 (trainer:676) INFO: 11epoch:train:739-779batch: iter_time=1.181e-04, forward_time=0.111, loss_ctc=45.093, loss_att=24.773, acc=0.816, loss=30.869, backward_time=0.114, optim_step_time=0.041, optim0_lr0=5.687e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:30:39,516 (trainer:676) INFO: 11epoch:train:780-820batch: iter_time=1.200e-04, forward_time=0.111, loss_ctc=44.756, loss_att=24.251, acc=0.804, loss=30.403, backward_time=0.112, optim_step_time=0.041, optim0_lr0=5.713e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:31:18,479 (trainer:334) INFO: 11epoch results: [train] iter_time=3.766e-04, forward_time=0.110, loss_ctc=44.643, loss_att=24.068, acc=0.809, loss=30.240, backward_time=0.110, optim_step_time=0.041, optim0_lr0=5.468e-04, train_time=0.374, time=5 minutes and 8.15 seconds, total_count=9064, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=28.710, cer_ctc=0.145, loss_att=17.540, acc=0.872, cer=0.128, wer=0.952, loss=20.891, time=7.43 seconds, total_count=319, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.97 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 23:31:23,908 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 23:31:23,925 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/1epoch.pth
[ip-10-0-0-95] 2022-04-27 23:31:23,925 (trainer:268) INFO: 12/50epoch started. Estimated time to finish: 3 hours, 49 minutes and 5.96 seconds
[ip-10-0-0-95] 2022-04-27 23:31:24,319 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:31:39,598 (trainer:676) INFO: 12epoch:train:1-41batch: iter_time=0.006, forward_time=0.112, loss_ctc=40.928, loss_att=21.584, acc=0.820, loss=27.387, backward_time=0.112, optim_step_time=0.041, optim0_lr0=5.741e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 23:31:54,953 (trainer:676) INFO: 12epoch:train:42-82batch: iter_time=1.734e-04, forward_time=0.111, loss_ctc=41.048, loss_att=21.301, acc=0.821, loss=27.225, backward_time=0.110, optim_step_time=0.041, optim0_lr0=5.767e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:32:10,675 (trainer:676) INFO: 12epoch:train:83-123batch: iter_time=1.536e-04, forward_time=0.112, loss_ctc=42.977, loss_att=22.746, acc=0.825, loss=28.815, backward_time=0.116, optim_step_time=0.041, optim0_lr0=5.793e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 23:32:26,166 (trainer:676) INFO: 12epoch:train:124-164batch: iter_time=1.359e-04, forward_time=0.112, loss_ctc=39.047, loss_att=20.432, acc=0.833, loss=26.016, backward_time=0.112, optim_step_time=0.041, optim0_lr0=5.819e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:32:28,523 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:32:41,745 (trainer:676) INFO: 12epoch:train:165-205batch: iter_time=1.338e-04, forward_time=0.112, loss_ctc=42.459, loss_att=22.849, acc=0.807, loss=28.732, backward_time=0.113, optim_step_time=0.041, optim0_lr0=5.845e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 23:32:57,309 (trainer:676) INFO: 12epoch:train:206-246batch: iter_time=1.334e-04, forward_time=0.111, loss_ctc=43.285, loss_att=23.464, acc=0.819, loss=29.410, backward_time=0.113, optim_step_time=0.041, optim0_lr0=5.871e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:33:12,980 (trainer:676) INFO: 12epoch:train:247-287batch: iter_time=1.505e-04, forward_time=0.112, loss_ctc=44.365, loss_att=23.546, acc=0.819, loss=29.792, backward_time=0.113, optim_step_time=0.041, optim0_lr0=5.897e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 23:33:28,417 (trainer:676) INFO: 12epoch:train:288-328batch: iter_time=1.233e-04, forward_time=0.110, loss_ctc=41.846, loss_att=22.089, acc=0.817, loss=28.016, backward_time=0.111, optim_step_time=0.041, optim0_lr0=5.923e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:33:43,862 (trainer:676) INFO: 12epoch:train:329-369batch: iter_time=1.497e-04, forward_time=0.111, loss_ctc=43.178, loss_att=22.740, acc=0.823, loss=28.871, backward_time=0.111, optim_step_time=0.041, optim0_lr0=5.949e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:33:59,584 (trainer:676) INFO: 12epoch:train:370-410batch: iter_time=1.423e-04, forward_time=0.113, loss_ctc=39.955, loss_att=21.084, acc=0.828, loss=26.745, backward_time=0.114, optim_step_time=0.041, optim0_lr0=5.975e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 23:34:15,206 (trainer:676) INFO: 12epoch:train:411-451batch: iter_time=1.293e-04, forward_time=0.111, loss_ctc=44.082, loss_att=23.266, acc=0.821, loss=29.511, backward_time=0.115, optim_step_time=0.041, optim0_lr0=6.001e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:34:30,256 (trainer:676) INFO: 12epoch:train:452-492batch: iter_time=1.432e-04, forward_time=0.111, loss_ctc=42.752, loss_att=22.590, acc=0.824, loss=28.639, backward_time=0.106, optim_step_time=0.041, optim0_lr0=6.027e-04, train_time=0.367
[ip-10-0-0-95] 2022-04-27 23:34:46,030 (trainer:676) INFO: 12epoch:train:493-533batch: iter_time=1.393e-04, forward_time=0.112, loss_ctc=43.680, loss_att=23.397, acc=0.823, loss=29.482, backward_time=0.115, optim_step_time=0.041, optim0_lr0=6.053e-04, train_time=0.384
[ip-10-0-0-95] 2022-04-27 23:35:01,507 (trainer:676) INFO: 12epoch:train:534-574batch: iter_time=1.714e-04, forward_time=0.111, loss_ctc=42.145, loss_att=22.198, acc=0.818, loss=28.182, backward_time=0.111, optim_step_time=0.041, optim0_lr0=6.079e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:35:17,113 (trainer:676) INFO: 12epoch:train:575-615batch: iter_time=1.682e-04, forward_time=0.111, loss_ctc=44.379, loss_att=23.288, acc=0.824, loss=29.616, backward_time=0.111, optim_step_time=0.041, optim0_lr0=6.104e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 23:35:32,707 (trainer:676) INFO: 12epoch:train:616-656batch: iter_time=1.498e-04, forward_time=0.112, loss_ctc=43.589, loss_att=22.664, acc=0.823, loss=28.942, backward_time=0.112, optim_step_time=0.041, optim0_lr0=6.130e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 23:35:47,964 (trainer:676) INFO: 12epoch:train:657-697batch: iter_time=1.727e-04, forward_time=0.111, loss_ctc=40.963, loss_att=21.390, acc=0.824, loss=27.262, backward_time=0.108, optim_step_time=0.041, optim0_lr0=6.156e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-27 23:36:03,178 (trainer:676) INFO: 12epoch:train:698-738batch: iter_time=1.614e-04, forward_time=0.111, loss_ctc=41.932, loss_att=21.998, acc=0.829, loss=27.979, backward_time=0.108, optim_step_time=0.041, optim0_lr0=6.182e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 23:36:18,548 (trainer:676) INFO: 12epoch:train:739-779batch: iter_time=1.609e-04, forward_time=0.112, loss_ctc=40.549, loss_att=21.296, acc=0.826, loss=27.072, backward_time=0.110, optim_step_time=0.041, optim0_lr0=6.208e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:36:33,964 (trainer:676) INFO: 12epoch:train:780-820batch: iter_time=1.474e-04, forward_time=0.110, loss_ctc=43.487, loss_att=22.874, acc=0.827, loss=29.058, backward_time=0.111, optim_step_time=0.041, optim0_lr0=6.234e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:37:12,963 (trainer:334) INFO: 12epoch results: [train] iter_time=4.286e-04, forward_time=0.111, loss_ctc=42.295, loss_att=22.317, acc=0.823, loss=28.310, backward_time=0.112, optim_step_time=0.041, optim0_lr0=5.989e-04, train_time=0.378, time=5 minutes and 11.6 seconds, total_count=9888, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=27.843, cer_ctc=0.144, loss_att=16.574, acc=0.879, cer=0.123, wer=0.942, loss=19.955, time=7.41 seconds, total_count=348, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.02 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 23:37:18,532 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 23:37:18,549 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/2epoch.pth
[ip-10-0-0-95] 2022-04-27 23:37:18,550 (trainer:268) INFO: 13/50epoch started. Estimated time to finish: 3 hours, 43 minutes and 20.35 seconds
[ip-10-0-0-95] 2022-04-27 23:37:33,910 (trainer:676) INFO: 13epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=39.179, loss_att=20.316, acc=0.842, loss=25.975, backward_time=0.107, optim_step_time=0.041, optim0_lr0=6.263e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:37:49,046 (trainer:676) INFO: 13epoch:train:42-82batch: iter_time=1.469e-04, forward_time=0.111, loss_ctc=40.024, loss_att=20.819, acc=0.837, loss=26.580, backward_time=0.107, optim_step_time=0.041, optim0_lr0=6.289e-04, train_time=0.369
[ip-10-0-0-95] 2022-04-27 23:38:04,784 (trainer:676) INFO: 13epoch:train:83-123batch: iter_time=1.442e-04, forward_time=0.111, loss_ctc=39.037, loss_att=20.182, acc=0.837, loss=25.839, backward_time=0.115, optim_step_time=0.041, optim0_lr0=6.314e-04, train_time=0.384
[ip-10-0-0-95] 2022-04-27 23:38:20,353 (trainer:676) INFO: 13epoch:train:124-164batch: iter_time=1.489e-04, forward_time=0.113, loss_ctc=40.639, loss_att=21.068, acc=0.833, loss=26.939, backward_time=0.112, optim_step_time=0.041, optim0_lr0=6.340e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:38:35,807 (trainer:676) INFO: 13epoch:train:165-205batch: iter_time=1.224e-04, forward_time=0.111, loss_ctc=42.233, loss_att=21.664, acc=0.832, loss=27.835, backward_time=0.111, optim_step_time=0.041, optim0_lr0=6.366e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:38:51,519 (trainer:676) INFO: 13epoch:train:206-246batch: iter_time=1.409e-04, forward_time=0.113, loss_ctc=39.281, loss_att=20.508, acc=0.841, loss=26.140, backward_time=0.117, optim_step_time=0.041, optim0_lr0=6.392e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 23:39:06,835 (trainer:676) INFO: 13epoch:train:247-287batch: iter_time=1.274e-04, forward_time=0.112, loss_ctc=40.615, loss_att=20.945, acc=0.836, loss=26.846, backward_time=0.108, optim_step_time=0.041, optim0_lr0=6.418e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:39:22,358 (trainer:676) INFO: 13epoch:train:288-328batch: iter_time=1.498e-04, forward_time=0.112, loss_ctc=41.706, loss_att=21.475, acc=0.834, loss=27.544, backward_time=0.112, optim_step_time=0.041, optim0_lr0=6.444e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:39:37,817 (trainer:676) INFO: 13epoch:train:329-369batch: iter_time=1.319e-04, forward_time=0.112, loss_ctc=39.842, loss_att=20.389, acc=0.840, loss=26.225, backward_time=0.112, optim_step_time=0.041, optim0_lr0=6.470e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:39:53,268 (trainer:676) INFO: 13epoch:train:370-410batch: iter_time=1.398e-04, forward_time=0.112, loss_ctc=40.693, loss_att=21.352, acc=0.830, loss=27.154, backward_time=0.111, optim_step_time=0.041, optim0_lr0=6.496e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:39:57,941 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:40:08,884 (trainer:676) INFO: 13epoch:train:411-451batch: iter_time=1.524e-04, forward_time=0.111, loss_ctc=41.684, loss_att=22.389, acc=0.815, loss=28.178, backward_time=0.112, optim_step_time=0.041, optim0_lr0=6.522e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:40:24,123 (trainer:676) INFO: 13epoch:train:452-492batch: iter_time=1.260e-04, forward_time=0.111, loss_ctc=39.325, loss_att=20.015, acc=0.840, loss=25.808, backward_time=0.109, optim_step_time=0.041, optim0_lr0=6.548e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 23:40:39,483 (trainer:676) INFO: 13epoch:train:493-533batch: iter_time=1.269e-04, forward_time=0.110, loss_ctc=39.653, loss_att=19.997, acc=0.828, loss=25.893, backward_time=0.110, optim_step_time=0.041, optim0_lr0=6.574e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:40:54,922 (trainer:676) INFO: 13epoch:train:534-574batch: iter_time=1.195e-04, forward_time=0.111, loss_ctc=39.093, loss_att=20.107, acc=0.834, loss=25.803, backward_time=0.113, optim_step_time=0.041, optim0_lr0=6.600e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:41:10,557 (trainer:676) INFO: 13epoch:train:575-615batch: iter_time=1.192e-04, forward_time=0.111, loss_ctc=42.511, loss_att=21.747, acc=0.835, loss=27.976, backward_time=0.114, optim_step_time=0.041, optim0_lr0=6.626e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:41:26,044 (trainer:676) INFO: 13epoch:train:616-656batch: iter_time=1.208e-04, forward_time=0.111, loss_ctc=39.726, loss_att=20.037, acc=0.838, loss=25.944, backward_time=0.112, optim_step_time=0.041, optim0_lr0=6.652e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:41:38,794 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:41:41,752 (trainer:676) INFO: 13epoch:train:657-697batch: iter_time=1.230e-04, forward_time=0.112, loss_ctc=38.173, loss_att=19.131, acc=0.832, loss=24.844, backward_time=0.115, optim_step_time=0.041, optim0_lr0=6.677e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 23:41:57,004 (trainer:676) INFO: 13epoch:train:698-738batch: iter_time=1.188e-04, forward_time=0.110, loss_ctc=39.676, loss_att=20.270, acc=0.834, loss=26.092, backward_time=0.108, optim_step_time=0.041, optim0_lr0=6.703e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-27 23:42:12,202 (trainer:676) INFO: 13epoch:train:739-779batch: iter_time=1.253e-04, forward_time=0.111, loss_ctc=39.697, loss_att=20.504, acc=0.838, loss=26.262, backward_time=0.108, optim_step_time=0.041, optim0_lr0=6.729e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 23:42:27,947 (trainer:676) INFO: 13epoch:train:780-820batch: iter_time=1.352e-04, forward_time=0.115, loss_ctc=42.067, loss_att=21.873, acc=0.832, loss=27.931, backward_time=0.113, optim_step_time=0.041, optim0_lr0=6.755e-04, train_time=0.384
[ip-10-0-0-95] 2022-04-27 23:43:06,982 (trainer:334) INFO: 13epoch results: [train] iter_time=3.733e-04, forward_time=0.112, loss_ctc=40.230, loss_att=20.733, acc=0.834, loss=26.582, backward_time=0.111, optim_step_time=0.041, optim0_lr0=6.510e-04, train_time=0.377, time=5 minutes and 10.95 seconds, total_count=10712, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=27.203, cer_ctc=0.138, loss_att=15.753, acc=0.884, cer=0.117, wer=0.948, loss=19.188, time=7.38 seconds, total_count=377, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.1 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 23:43:12,428 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 23:43:12,445 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/3epoch.pth
[ip-10-0-0-95] 2022-04-27 23:43:12,446 (trainer:268) INFO: 14/50epoch started. Estimated time to finish: 3 hours, 37 minutes and 31.29 seconds
[ip-10-0-0-95] 2022-04-27 23:43:27,915 (trainer:676) INFO: 14epoch:train:1-41batch: iter_time=0.005, forward_time=0.111, loss_ctc=39.219, loss_att=19.923, acc=0.849, loss=25.712, backward_time=0.111, optim_step_time=0.041, optim0_lr0=6.784e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:43:37,552 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:43:43,416 (trainer:676) INFO: 14epoch:train:42-82batch: iter_time=1.478e-04, forward_time=0.112, loss_ctc=36.443, loss_att=18.900, acc=0.834, loss=24.163, backward_time=0.113, optim_step_time=0.041, optim0_lr0=6.810e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:43:58,899 (trainer:676) INFO: 14epoch:train:83-123batch: iter_time=1.667e-04, forward_time=0.115, loss_ctc=39.359, loss_att=20.039, acc=0.838, loss=25.835, backward_time=0.110, optim_step_time=0.041, optim0_lr0=6.836e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:44:14,260 (trainer:676) INFO: 14epoch:train:124-164batch: iter_time=1.737e-04, forward_time=0.112, loss_ctc=39.497, loss_att=20.209, acc=0.847, loss=25.996, backward_time=0.109, optim_step_time=0.041, optim0_lr0=6.862e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:44:29,898 (trainer:676) INFO: 14epoch:train:165-205batch: iter_time=1.724e-04, forward_time=0.112, loss_ctc=36.264, loss_att=18.536, acc=0.844, loss=23.855, backward_time=0.114, optim_step_time=0.041, optim0_lr0=6.887e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:44:45,358 (trainer:676) INFO: 14epoch:train:206-246batch: iter_time=1.529e-04, forward_time=0.112, loss_ctc=37.981, loss_att=19.829, acc=0.853, loss=25.274, backward_time=0.112, optim_step_time=0.041, optim0_lr0=6.913e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:45:00,805 (trainer:676) INFO: 14epoch:train:247-287batch: iter_time=1.631e-04, forward_time=0.111, loss_ctc=39.207, loss_att=20.114, acc=0.849, loss=25.842, backward_time=0.111, optim_step_time=0.041, optim0_lr0=6.939e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:45:16,518 (trainer:676) INFO: 14epoch:train:288-328batch: iter_time=1.546e-04, forward_time=0.111, loss_ctc=38.140, loss_att=19.290, acc=0.847, loss=24.945, backward_time=0.114, optim_step_time=0.041, optim0_lr0=6.965e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 23:45:32,169 (trainer:676) INFO: 14epoch:train:329-369batch: iter_time=1.621e-04, forward_time=0.111, loss_ctc=39.673, loss_att=19.826, acc=0.847, loss=25.780, backward_time=0.114, optim_step_time=0.041, optim0_lr0=6.991e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:45:35,237 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:45:47,377 (trainer:676) INFO: 14epoch:train:370-410batch: iter_time=1.755e-04, forward_time=0.111, loss_ctc=38.251, loss_att=19.156, acc=0.840, loss=24.885, backward_time=0.108, optim_step_time=0.041, optim0_lr0=7.017e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 23:46:02,557 (trainer:676) INFO: 14epoch:train:411-451batch: iter_time=1.659e-04, forward_time=0.111, loss_ctc=38.098, loss_att=18.780, acc=0.833, loss=24.575, backward_time=0.106, optim_step_time=0.041, optim0_lr0=7.043e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 23:46:17,966 (trainer:676) INFO: 14epoch:train:452-492batch: iter_time=1.642e-04, forward_time=0.111, loss_ctc=39.191, loss_att=19.918, acc=0.847, loss=25.700, backward_time=0.110, optim_step_time=0.041, optim0_lr0=7.069e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:46:33,469 (trainer:676) INFO: 14epoch:train:493-533batch: iter_time=1.396e-04, forward_time=0.112, loss_ctc=38.594, loss_att=19.806, acc=0.849, loss=25.442, backward_time=0.112, optim_step_time=0.041, optim0_lr0=7.095e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:46:48,711 (trainer:676) INFO: 14epoch:train:534-574batch: iter_time=1.252e-04, forward_time=0.111, loss_ctc=40.036, loss_att=20.222, acc=0.845, loss=26.166, backward_time=0.107, optim_step_time=0.041, optim0_lr0=7.121e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-27 23:47:04,104 (trainer:676) INFO: 14epoch:train:575-615batch: iter_time=1.242e-04, forward_time=0.112, loss_ctc=36.732, loss_att=18.855, acc=0.855, loss=24.218, backward_time=0.111, optim_step_time=0.041, optim0_lr0=7.147e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:47:19,667 (trainer:676) INFO: 14epoch:train:616-656batch: iter_time=1.257e-04, forward_time=0.111, loss_ctc=37.858, loss_att=19.013, acc=0.845, loss=24.667, backward_time=0.110, optim_step_time=0.041, optim0_lr0=7.173e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:47:28,008 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-27 23:47:35,315 (trainer:676) INFO: 14epoch:train:657-697batch: iter_time=1.226e-04, forward_time=0.112, loss_ctc=38.358, loss_att=19.217, acc=0.845, loss=24.959, backward_time=0.114, optim_step_time=0.041, optim0_lr0=7.198e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:47:50,669 (trainer:676) INFO: 14epoch:train:698-738batch: iter_time=1.254e-04, forward_time=0.111, loss_ctc=37.068, loss_att=18.348, acc=0.838, loss=23.964, backward_time=0.109, optim_step_time=0.041, optim0_lr0=7.224e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:48:06,364 (trainer:676) INFO: 14epoch:train:739-779batch: iter_time=1.266e-04, forward_time=0.111, loss_ctc=38.386, loss_att=18.908, acc=0.847, loss=24.751, backward_time=0.115, optim_step_time=0.041, optim0_lr0=7.250e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 23:48:21,816 (trainer:676) INFO: 14epoch:train:780-820batch: iter_time=1.379e-04, forward_time=0.111, loss_ctc=38.098, loss_att=19.125, acc=0.844, loss=24.817, backward_time=0.112, optim_step_time=0.041, optim0_lr0=7.276e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:49:00,982 (trainer:334) INFO: 14epoch results: [train] iter_time=3.982e-04, forward_time=0.111, loss_ctc=38.298, loss_att=19.382, acc=0.845, loss=25.057, backward_time=0.111, optim_step_time=0.041, optim0_lr0=7.031e-04, train_time=0.377, time=5 minutes and 10.97 seconds, total_count=11536, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=26.811, cer_ctc=0.128, loss_att=15.322, acc=0.892, cer=0.112, wer=0.935, loss=18.769, time=7.4 seconds, total_count=406, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.17 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 23:49:06,452 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 23:49:06,469 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/4epoch.pth
[ip-10-0-0-95] 2022-04-27 23:49:06,470 (trainer:268) INFO: 15/50epoch started. Estimated time to finish: 3 hours, 31 minutes and 41.86 seconds
[ip-10-0-0-95] 2022-04-27 23:49:21,776 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:49:22,067 (trainer:676) INFO: 15epoch:train:1-41batch: iter_time=0.006, forward_time=0.111, loss_ctc=35.138, loss_att=17.124, acc=0.851, loss=22.528, backward_time=0.111, optim_step_time=0.041, optim0_lr0=7.304e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 23:49:37,515 (trainer:676) INFO: 15epoch:train:42-82batch: iter_time=1.418e-04, forward_time=0.111, loss_ctc=34.461, loss_att=17.141, acc=0.855, loss=22.337, backward_time=0.111, optim_step_time=0.041, optim0_lr0=7.330e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:49:53,245 (trainer:676) INFO: 15epoch:train:83-123batch: iter_time=1.655e-04, forward_time=0.115, loss_ctc=38.094, loss_att=18.908, acc=0.849, loss=24.664, backward_time=0.114, optim_step_time=0.041, optim0_lr0=7.356e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-27 23:50:08,819 (trainer:676) INFO: 15epoch:train:124-164batch: iter_time=1.683e-04, forward_time=0.114, loss_ctc=35.401, loss_att=17.569, acc=0.858, loss=22.918, backward_time=0.111, optim_step_time=0.041, optim0_lr0=7.382e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 23:50:24,338 (trainer:676) INFO: 15epoch:train:165-205batch: iter_time=1.474e-04, forward_time=0.112, loss_ctc=36.136, loss_att=17.913, acc=0.855, loss=23.380, backward_time=0.112, optim_step_time=0.041, optim0_lr0=7.408e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:50:39,692 (trainer:676) INFO: 15epoch:train:206-246batch: iter_time=1.365e-04, forward_time=0.111, loss_ctc=37.492, loss_att=18.769, acc=0.853, loss=24.386, backward_time=0.109, optim_step_time=0.041, optim0_lr0=7.434e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:50:55,003 (trainer:676) INFO: 15epoch:train:247-287batch: iter_time=1.366e-04, forward_time=0.110, loss_ctc=37.319, loss_att=18.264, acc=0.856, loss=23.981, backward_time=0.110, optim_step_time=0.041, optim0_lr0=7.460e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:51:11,013 (trainer:676) INFO: 15epoch:train:288-328batch: iter_time=1.358e-04, forward_time=0.112, loss_ctc=37.069, loss_att=18.580, acc=0.851, loss=24.127, backward_time=0.119, optim_step_time=0.041, optim0_lr0=7.486e-04, train_time=0.390
[ip-10-0-0-95] 2022-04-27 23:51:26,415 (trainer:676) INFO: 15epoch:train:329-369batch: iter_time=1.381e-04, forward_time=0.111, loss_ctc=37.549, loss_att=19.026, acc=0.854, loss=24.583, backward_time=0.110, optim_step_time=0.041, optim0_lr0=7.512e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:51:41,802 (trainer:676) INFO: 15epoch:train:370-410batch: iter_time=1.395e-04, forward_time=0.111, loss_ctc=36.500, loss_att=18.287, acc=0.848, loss=23.751, backward_time=0.112, optim_step_time=0.041, optim0_lr0=7.538e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:51:57,206 (trainer:676) INFO: 15epoch:train:411-451batch: iter_time=1.298e-04, forward_time=0.112, loss_ctc=35.771, loss_att=17.926, acc=0.857, loss=23.280, backward_time=0.112, optim_step_time=0.041, optim0_lr0=7.564e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:52:12,019 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:52:13,042 (trainer:676) INFO: 15epoch:train:452-492batch: iter_time=1.378e-04, forward_time=0.114, loss_ctc=36.445, loss_att=18.995, acc=0.843, loss=24.230, backward_time=0.114, optim_step_time=0.041, optim0_lr0=7.589e-04, train_time=0.386
[ip-10-0-0-95] 2022-04-27 23:52:28,707 (trainer:676) INFO: 15epoch:train:493-533batch: iter_time=1.490e-04, forward_time=0.115, loss_ctc=37.340, loss_att=18.568, acc=0.852, loss=24.200, backward_time=0.112, optim_step_time=0.041, optim0_lr0=7.615e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 23:52:44,311 (trainer:676) INFO: 15epoch:train:534-574batch: iter_time=1.644e-04, forward_time=0.112, loss_ctc=36.899, loss_att=18.415, acc=0.855, loss=23.960, backward_time=0.114, optim_step_time=0.041, optim0_lr0=7.641e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 23:52:59,735 (trainer:676) INFO: 15epoch:train:575-615batch: iter_time=1.645e-04, forward_time=0.111, loss_ctc=39.036, loss_att=19.611, acc=0.851, loss=25.438, backward_time=0.111, optim_step_time=0.041, optim0_lr0=7.667e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:53:15,032 (trainer:676) INFO: 15epoch:train:616-656batch: iter_time=1.552e-04, forward_time=0.111, loss_ctc=34.908, loss_att=17.332, acc=0.853, loss=22.605, backward_time=0.110, optim_step_time=0.041, optim0_lr0=7.693e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:53:30,293 (trainer:676) INFO: 15epoch:train:657-697batch: iter_time=1.585e-04, forward_time=0.110, loss_ctc=37.765, loss_att=18.698, acc=0.854, loss=24.418, backward_time=0.108, optim_step_time=0.041, optim0_lr0=7.719e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-27 23:53:45,589 (trainer:676) INFO: 15epoch:train:698-738batch: iter_time=1.614e-04, forward_time=0.110, loss_ctc=37.867, loss_att=19.059, acc=0.852, loss=24.702, backward_time=0.110, optim_step_time=0.041, optim0_lr0=7.745e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:54:01,021 (trainer:676) INFO: 15epoch:train:739-779batch: iter_time=1.671e-04, forward_time=0.110, loss_ctc=36.690, loss_att=18.427, acc=0.856, loss=23.905, backward_time=0.110, optim_step_time=0.041, optim0_lr0=7.771e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:54:16,621 (trainer:676) INFO: 15epoch:train:780-820batch: iter_time=1.690e-04, forward_time=0.113, loss_ctc=36.950, loss_att=18.859, acc=0.859, loss=24.286, backward_time=0.115, optim_step_time=0.041, optim0_lr0=7.797e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-27 23:54:55,456 (trainer:334) INFO: 15epoch results: [train] iter_time=4.380e-04, forward_time=0.112, loss_ctc=36.705, loss_att=18.344, acc=0.853, loss=23.852, backward_time=0.112, optim_step_time=0.041, optim0_lr0=7.552e-04, train_time=0.378, time=5 minutes and 11.72 seconds, total_count=12360, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.243, cer_ctc=0.121, loss_att=14.510, acc=0.896, cer=0.106, wer=0.934, loss=17.730, time=7.4 seconds, total_count=435, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.86 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-27 23:55:01,079 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-27 23:55:01,097 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/5epoch.pth
[ip-10-0-0-95] 2022-04-27 23:55:01,097 (trainer:268) INFO: 16/50epoch started. Estimated time to finish: 3 hours, 25 minutes and 53.22 seconds
[ip-10-0-0-95] 2022-04-27 23:55:16,543 (trainer:676) INFO: 16epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=34.262, loss_att=16.911, acc=0.858, loss=22.116, backward_time=0.108, optim_step_time=0.041, optim0_lr0=7.825e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:55:31,986 (trainer:676) INFO: 16epoch:train:42-82batch: iter_time=1.394e-04, forward_time=0.110, loss_ctc=34.054, loss_att=16.624, acc=0.863, loss=21.853, backward_time=0.111, optim_step_time=0.041, optim0_lr0=7.851e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-27 23:55:47,543 (trainer:676) INFO: 16epoch:train:83-123batch: iter_time=1.274e-04, forward_time=0.111, loss_ctc=36.235, loss_att=17.930, acc=0.860, loss=23.421, backward_time=0.113, optim_step_time=0.041, optim0_lr0=7.877e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:56:03,172 (trainer:676) INFO: 16epoch:train:124-164batch: iter_time=1.338e-04, forward_time=0.113, loss_ctc=34.577, loss_att=17.054, acc=0.863, loss=22.311, backward_time=0.112, optim_step_time=0.041, optim0_lr0=7.903e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-27 23:56:18,949 (trainer:676) INFO: 16epoch:train:165-205batch: iter_time=1.255e-04, forward_time=0.111, loss_ctc=37.214, loss_att=18.537, acc=0.861, loss=24.140, backward_time=0.116, optim_step_time=0.041, optim0_lr0=7.929e-04, train_time=0.384
[ip-10-0-0-95] 2022-04-27 23:56:34,342 (trainer:676) INFO: 16epoch:train:206-246batch: iter_time=1.271e-04, forward_time=0.111, loss_ctc=34.861, loss_att=17.472, acc=0.865, loss=22.689, backward_time=0.112, optim_step_time=0.041, optim0_lr0=7.955e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:56:49,631 (trainer:676) INFO: 16epoch:train:247-287batch: iter_time=1.286e-04, forward_time=0.111, loss_ctc=35.685, loss_att=17.436, acc=0.862, loss=22.911, backward_time=0.109, optim_step_time=0.041, optim0_lr0=7.981e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-27 23:57:05,157 (trainer:676) INFO: 16epoch:train:288-328batch: iter_time=1.242e-04, forward_time=0.112, loss_ctc=35.867, loss_att=18.060, acc=0.864, loss=23.402, backward_time=0.112, optim_step_time=0.041, optim0_lr0=8.007e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-27 23:57:20,619 (trainer:676) INFO: 16epoch:train:329-369batch: iter_time=1.347e-04, forward_time=0.112, loss_ctc=35.009, loss_att=17.445, acc=0.862, loss=22.714, backward_time=0.111, optim_step_time=0.041, optim0_lr0=8.033e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:57:35,964 (trainer:676) INFO: 16epoch:train:370-410batch: iter_time=1.624e-04, forward_time=0.110, loss_ctc=36.676, loss_att=17.934, acc=0.861, loss=23.556, backward_time=0.108, optim_step_time=0.041, optim0_lr0=8.059e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-27 23:57:51,513 (trainer:676) INFO: 16epoch:train:411-451batch: iter_time=1.463e-04, forward_time=0.112, loss_ctc=35.020, loss_att=17.105, acc=0.862, loss=22.479, backward_time=0.113, optim_step_time=0.041, optim0_lr0=8.085e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-27 23:58:02,115 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:58:06,891 (trainer:676) INFO: 16epoch:train:452-492batch: iter_time=1.372e-04, forward_time=0.112, loss_ctc=35.234, loss_att=17.569, acc=0.852, loss=22.868, backward_time=0.111, optim_step_time=0.041, optim0_lr0=8.111e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:58:22,570 (trainer:676) INFO: 16epoch:train:493-533batch: iter_time=1.384e-04, forward_time=0.110, loss_ctc=34.437, loss_att=16.942, acc=0.861, loss=22.191, backward_time=0.114, optim_step_time=0.041, optim0_lr0=8.137e-04, train_time=0.382
[ip-10-0-0-95] 2022-04-27 23:58:37,764 (trainer:676) INFO: 16epoch:train:534-574batch: iter_time=1.365e-04, forward_time=0.111, loss_ctc=33.302, loss_att=16.510, acc=0.860, loss=21.547, backward_time=0.109, optim_step_time=0.041, optim0_lr0=8.162e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 23:58:53,139 (trainer:676) INFO: 16epoch:train:575-615batch: iter_time=1.462e-04, forward_time=0.111, loss_ctc=36.231, loss_att=17.747, acc=0.861, loss=23.293, backward_time=0.111, optim_step_time=0.041, optim0_lr0=8.188e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-27 23:59:08,337 (trainer:676) INFO: 16epoch:train:616-656batch: iter_time=1.381e-04, forward_time=0.111, loss_ctc=35.152, loss_att=17.016, acc=0.854, loss=22.457, backward_time=0.109, optim_step_time=0.041, optim0_lr0=8.214e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-27 23:59:23,791 (trainer:676) INFO: 16epoch:train:657-697batch: iter_time=1.377e-04, forward_time=0.112, loss_ctc=35.284, loss_att=17.310, acc=0.861, loss=22.702, backward_time=0.113, optim_step_time=0.041, optim0_lr0=8.240e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-27 23:59:39,072 (trainer:676) INFO: 16epoch:train:698-738batch: iter_time=1.429e-04, forward_time=0.111, loss_ctc=33.707, loss_att=16.965, acc=0.860, loss=21.988, backward_time=0.110, optim_step_time=0.041, optim0_lr0=8.266e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-27 23:59:53,180 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-27 23:59:54,575 (trainer:676) INFO: 16epoch:train:739-779batch: iter_time=1.340e-04, forward_time=0.111, loss_ctc=34.573, loss_att=16.949, acc=0.852, loss=22.236, backward_time=0.112, optim_step_time=0.041, optim0_lr0=8.292e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:00:10,004 (trainer:676) INFO: 16epoch:train:780-820batch: iter_time=1.375e-04, forward_time=0.111, loss_ctc=36.034, loss_att=17.566, acc=0.859, loss=23.106, backward_time=0.111, optim_step_time=0.041, optim0_lr0=8.318e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:00:48,875 (trainer:334) INFO: 16epoch results: [train] iter_time=3.828e-04, forward_time=0.111, loss_ctc=35.159, loss_att=17.343, acc=0.860, loss=22.688, backward_time=0.111, optim_step_time=0.041, optim0_lr0=8.073e-04, train_time=0.376, time=5 minutes and 10.47 seconds, total_count=13184, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.184, cer_ctc=0.119, loss_att=14.260, acc=0.898, cer=0.103, wer=0.915, loss=17.537, time=7.41 seconds, total_count=464, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.9 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 00:00:54,353 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 00:00:54,370 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/6epoch.pth
[ip-10-0-0-95] 2022-04-28 00:00:54,370 (trainer:268) INFO: 17/50epoch started. Estimated time to finish: 3 hours, 20 minutes and 0.96 seconds
[ip-10-0-0-95] 2022-04-28 00:01:09,866 (trainer:676) INFO: 17epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=35.709, loss_att=17.416, acc=0.866, loss=22.904, backward_time=0.110, optim_step_time=0.041, optim0_lr0=8.347e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:01:25,194 (trainer:676) INFO: 17epoch:train:42-82batch: iter_time=1.386e-04, forward_time=0.109, loss_ctc=32.228, loss_att=15.490, acc=0.868, loss=20.511, backward_time=0.111, optim_step_time=0.040, optim0_lr0=8.372e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:01:40,655 (trainer:676) INFO: 17epoch:train:83-123batch: iter_time=1.184e-04, forward_time=0.110, loss_ctc=34.765, loss_att=16.748, acc=0.873, loss=22.153, backward_time=0.112, optim_step_time=0.041, optim0_lr0=8.398e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:01:55,834 (trainer:676) INFO: 17epoch:train:124-164batch: iter_time=1.593e-04, forward_time=0.110, loss_ctc=34.038, loss_att=16.003, acc=0.863, loss=21.413, backward_time=0.107, optim_step_time=0.040, optim0_lr0=8.424e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-28 00:02:11,048 (trainer:676) INFO: 17epoch:train:165-205batch: iter_time=1.700e-04, forward_time=0.111, loss_ctc=35.190, loss_att=16.752, acc=0.866, loss=22.284, backward_time=0.109, optim_step_time=0.040, optim0_lr0=8.450e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-28 00:02:26,486 (trainer:676) INFO: 17epoch:train:206-246batch: iter_time=1.583e-04, forward_time=0.113, loss_ctc=35.090, loss_att=17.043, acc=0.867, loss=22.457, backward_time=0.111, optim_step_time=0.040, optim0_lr0=8.476e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:02:36,976 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:02:41,950 (trainer:676) INFO: 17epoch:train:247-287batch: iter_time=1.327e-04, forward_time=0.114, loss_ctc=33.725, loss_att=16.732, acc=0.857, loss=21.830, backward_time=0.110, optim_step_time=0.040, optim0_lr0=8.502e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:02:57,922 (trainer:676) INFO: 17epoch:train:288-328batch: iter_time=1.430e-04, forward_time=0.112, loss_ctc=34.568, loss_att=17.018, acc=0.865, loss=22.283, backward_time=0.118, optim_step_time=0.040, optim0_lr0=8.528e-04, train_time=0.389
[ip-10-0-0-95] 2022-04-28 00:03:13,243 (trainer:676) INFO: 17epoch:train:329-369batch: iter_time=1.426e-04, forward_time=0.111, loss_ctc=32.784, loss_att=15.575, acc=0.868, loss=20.738, backward_time=0.110, optim_step_time=0.040, optim0_lr0=8.554e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:03:28,807 (trainer:676) INFO: 17epoch:train:370-410batch: iter_time=1.421e-04, forward_time=0.112, loss_ctc=34.277, loss_att=16.544, acc=0.869, loss=21.864, backward_time=0.113, optim_step_time=0.040, optim0_lr0=8.580e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:03:44,517 (trainer:676) INFO: 17epoch:train:411-451batch: iter_time=1.494e-04, forward_time=0.112, loss_ctc=33.471, loss_att=16.210, acc=0.872, loss=21.388, backward_time=0.116, optim_step_time=0.040, optim0_lr0=8.606e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-28 00:04:00,102 (trainer:676) INFO: 17epoch:train:452-492batch: iter_time=1.510e-04, forward_time=0.111, loss_ctc=35.788, loss_att=17.610, acc=0.868, loss=23.064, backward_time=0.114, optim_step_time=0.040, optim0_lr0=8.632e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:04:05,783 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 00:04:15,582 (trainer:676) INFO: 17epoch:train:493-533batch: iter_time=1.455e-04, forward_time=0.112, loss_ctc=34.043, loss_att=16.389, acc=0.867, loss=21.685, backward_time=0.110, optim_step_time=0.041, optim0_lr0=8.657e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:04:31,116 (trainer:676) INFO: 17epoch:train:534-574batch: iter_time=1.390e-04, forward_time=0.112, loss_ctc=33.999, loss_att=16.195, acc=0.865, loss=21.536, backward_time=0.112, optim_step_time=0.041, optim0_lr0=8.683e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:04:46,454 (trainer:676) INFO: 17epoch:train:575-615batch: iter_time=1.482e-04, forward_time=0.112, loss_ctc=35.324, loss_att=17.498, acc=0.867, loss=22.846, backward_time=0.110, optim_step_time=0.041, optim0_lr0=8.709e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:05:01,838 (trainer:676) INFO: 17epoch:train:616-656batch: iter_time=1.582e-04, forward_time=0.112, loss_ctc=32.967, loss_att=15.633, acc=0.865, loss=20.834, backward_time=0.111, optim_step_time=0.041, optim0_lr0=8.735e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:05:17,457 (trainer:676) INFO: 17epoch:train:657-697batch: iter_time=1.565e-04, forward_time=0.111, loss_ctc=34.753, loss_att=16.782, acc=0.865, loss=22.174, backward_time=0.113, optim_step_time=0.041, optim0_lr0=8.761e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:05:30,235 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:05:32,773 (trainer:676) INFO: 17epoch:train:698-738batch: iter_time=1.455e-04, forward_time=0.112, loss_ctc=34.117, loss_att=16.276, acc=0.862, loss=21.628, backward_time=0.109, optim_step_time=0.041, optim0_lr0=8.787e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:05:48,285 (trainer:676) INFO: 17epoch:train:739-779batch: iter_time=1.402e-04, forward_time=0.111, loss_ctc=35.505, loss_att=17.110, acc=0.871, loss=22.628, backward_time=0.113, optim_step_time=0.041, optim0_lr0=8.813e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:06:04,178 (trainer:676) INFO: 17epoch:train:780-820batch: iter_time=1.487e-04, forward_time=0.112, loss_ctc=33.887, loss_att=16.526, acc=0.867, loss=21.734, backward_time=0.118, optim_step_time=0.040, optim0_lr0=8.839e-04, train_time=0.387
[ip-10-0-0-95] 2022-04-28 00:06:42,918 (trainer:334) INFO: 17epoch results: [train] iter_time=3.985e-04, forward_time=0.111, loss_ctc=34.274, loss_att=16.554, acc=0.866, loss=21.870, backward_time=0.112, optim_step_time=0.041, optim0_lr0=8.594e-04, train_time=0.377, time=5 minutes and 11.34 seconds, total_count=14008, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=24.937, cer_ctc=0.116, loss_att=13.890, acc=0.899, cer=0.104, wer=0.923, loss=17.204, time=7.41 seconds, total_count=493, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.8 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 00:06:48,434 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 00:06:48,451 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/7epoch.pth
[ip-10-0-0-95] 2022-04-28 00:06:48,451 (trainer:268) INFO: 18/50epoch started. Estimated time to finish: 3 hours, 14 minutes and 10.15 seconds
[ip-10-0-0-95] 2022-04-28 00:07:03,828 (trainer:676) INFO: 18epoch:train:1-41batch: iter_time=0.006, forward_time=0.111, loss_ctc=31.997, loss_att=15.149, acc=0.878, loss=20.204, backward_time=0.110, optim_step_time=0.041, optim0_lr0=8.867e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:07:19,322 (trainer:676) INFO: 18epoch:train:42-82batch: iter_time=1.466e-04, forward_time=0.110, loss_ctc=32.126, loss_att=15.531, acc=0.878, loss=20.510, backward_time=0.114, optim_step_time=0.041, optim0_lr0=8.893e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:07:34,795 (trainer:676) INFO: 18epoch:train:83-123batch: iter_time=1.360e-04, forward_time=0.110, loss_ctc=32.352, loss_att=15.432, acc=0.871, loss=20.508, backward_time=0.110, optim_step_time=0.041, optim0_lr0=8.919e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:07:50,339 (trainer:676) INFO: 18epoch:train:124-164batch: iter_time=1.309e-04, forward_time=0.112, loss_ctc=32.696, loss_att=15.573, acc=0.879, loss=20.710, backward_time=0.113, optim_step_time=0.041, optim0_lr0=8.945e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:08:05,985 (trainer:676) INFO: 18epoch:train:165-205batch: iter_time=1.465e-04, forward_time=0.112, loss_ctc=33.440, loss_att=16.347, acc=0.876, loss=21.475, backward_time=0.115, optim_step_time=0.041, optim0_lr0=8.971e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:08:21,407 (trainer:676) INFO: 18epoch:train:206-246batch: iter_time=1.278e-04, forward_time=0.111, loss_ctc=35.097, loss_att=16.868, acc=0.869, loss=22.337, backward_time=0.111, optim_step_time=0.041, optim0_lr0=8.997e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:08:36,852 (trainer:676) INFO: 18epoch:train:247-287batch: iter_time=1.208e-04, forward_time=0.111, loss_ctc=32.577, loss_att=15.541, acc=0.875, loss=20.652, backward_time=0.111, optim_step_time=0.041, optim0_lr0=9.023e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:08:52,172 (trainer:676) INFO: 18epoch:train:288-328batch: iter_time=1.240e-04, forward_time=0.111, loss_ctc=32.222, loss_att=15.451, acc=0.864, loss=20.482, backward_time=0.111, optim_step_time=0.041, optim0_lr0=9.049e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:09:07,370 (trainer:676) INFO: 18epoch:train:329-369batch: iter_time=1.404e-04, forward_time=0.111, loss_ctc=35.277, loss_att=17.365, acc=0.867, loss=22.738, backward_time=0.108, optim_step_time=0.041, optim0_lr0=9.074e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-28 00:09:19,163 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:09:22,861 (trainer:676) INFO: 18epoch:train:370-410batch: iter_time=1.463e-04, forward_time=0.111, loss_ctc=32.294, loss_att=16.186, acc=0.861, loss=21.019, backward_time=0.112, optim_step_time=0.041, optim0_lr0=9.100e-04, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:09:38,498 (trainer:676) INFO: 18epoch:train:411-451batch: iter_time=1.372e-04, forward_time=0.112, loss_ctc=32.949, loss_att=15.486, acc=0.871, loss=20.725, backward_time=0.115, optim_step_time=0.041, optim0_lr0=9.126e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:09:53,559 (trainer:676) INFO: 18epoch:train:452-492batch: iter_time=1.573e-04, forward_time=0.110, loss_ctc=32.898, loss_att=15.499, acc=0.867, loss=20.719, backward_time=0.106, optim_step_time=0.041, optim0_lr0=9.152e-04, train_time=0.367
[ip-10-0-0-95] 2022-04-28 00:10:09,155 (trainer:676) INFO: 18epoch:train:493-533batch: iter_time=1.408e-04, forward_time=0.111, loss_ctc=34.794, loss_att=16.607, acc=0.868, loss=22.063, backward_time=0.114, optim_step_time=0.041, optim0_lr0=9.178e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:10:24,497 (trainer:676) INFO: 18epoch:train:534-574batch: iter_time=1.461e-04, forward_time=0.111, loss_ctc=33.603, loss_att=15.976, acc=0.876, loss=21.264, backward_time=0.110, optim_step_time=0.041, optim0_lr0=9.204e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:10:39,900 (trainer:676) INFO: 18epoch:train:575-615batch: iter_time=1.465e-04, forward_time=0.111, loss_ctc=33.988, loss_att=16.201, acc=0.872, loss=21.537, backward_time=0.111, optim_step_time=0.041, optim0_lr0=9.230e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:10:54,955 (trainer:676) INFO: 18epoch:train:616-656batch: iter_time=1.574e-04, forward_time=0.110, loss_ctc=33.628, loss_att=15.653, acc=0.872, loss=21.045, backward_time=0.105, optim_step_time=0.041, optim0_lr0=9.256e-04, train_time=0.367
[ip-10-0-0-95] 2022-04-28 00:11:10,853 (trainer:676) INFO: 18epoch:train:657-697batch: iter_time=1.389e-04, forward_time=0.112, loss_ctc=33.176, loss_att=15.895, acc=0.876, loss=21.079, backward_time=0.119, optim_step_time=0.041, optim0_lr0=9.282e-04, train_time=0.387
[ip-10-0-0-95] 2022-04-28 00:11:13,571 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:11:26,460 (trainer:676) INFO: 18epoch:train:698-738batch: iter_time=1.230e-04, forward_time=0.116, loss_ctc=30.585, loss_att=14.158, acc=0.875, loss=19.086, backward_time=0.113, optim_step_time=0.041, optim0_lr0=9.308e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:11:41,809 (trainer:676) INFO: 18epoch:train:739-779batch: iter_time=1.613e-04, forward_time=0.111, loss_ctc=34.118, loss_att=16.364, acc=0.871, loss=21.690, backward_time=0.109, optim_step_time=0.041, optim0_lr0=9.334e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:11:57,007 (trainer:676) INFO: 18epoch:train:780-820batch: iter_time=1.499e-04, forward_time=0.109, loss_ctc=33.175, loss_att=16.030, acc=0.870, loss=21.174, backward_time=0.108, optim_step_time=0.040, optim0_lr0=9.360e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-28 00:12:35,598 (trainer:334) INFO: 18epoch results: [train] iter_time=4.149e-04, forward_time=0.111, loss_ctc=33.133, loss_att=15.850, acc=0.872, loss=21.035, backward_time=0.111, optim_step_time=0.041, optim0_lr0=9.115e-04, train_time=0.376, time=5 minutes and 10.11 seconds, total_count=14832, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.932, cer_ctc=0.115, loss_att=14.101, acc=0.900, cer=0.101, wer=0.905, loss=17.650, time=7.38 seconds, total_count=522, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.66 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 00:12:41,247 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 00:12:41,264 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/8epoch.pth
[ip-10-0-0-95] 2022-04-28 00:12:41,264 (trainer:268) INFO: 19/50epoch started. Estimated time to finish: 3 hours, 8 minutes and 16.72 seconds
[ip-10-0-0-95] 2022-04-28 00:12:56,723 (trainer:676) INFO: 19epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=32.172, loss_att=15.109, acc=0.878, loss=20.228, backward_time=0.110, optim_step_time=0.040, optim0_lr0=9.388e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:13:12,022 (trainer:676) INFO: 19epoch:train:42-82batch: iter_time=1.388e-04, forward_time=0.112, loss_ctc=29.741, loss_att=13.968, acc=0.878, loss=18.700, backward_time=0.109, optim_step_time=0.040, optim0_lr0=9.414e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:13:27,202 (trainer:676) INFO: 19epoch:train:83-123batch: iter_time=1.447e-04, forward_time=0.110, loss_ctc=29.970, loss_att=13.666, acc=0.879, loss=18.557, backward_time=0.108, optim_step_time=0.040, optim0_lr0=9.440e-04, train_time=0.370
[ip-10-0-0-95] 2022-04-28 00:13:42,847 (trainer:676) INFO: 19epoch:train:124-164batch: iter_time=1.453e-04, forward_time=0.111, loss_ctc=32.397, loss_att=14.858, acc=0.880, loss=20.120, backward_time=0.114, optim_step_time=0.040, optim0_lr0=9.466e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:13:58,547 (trainer:676) INFO: 19epoch:train:165-205batch: iter_time=1.425e-04, forward_time=0.112, loss_ctc=31.308, loss_att=15.330, acc=0.879, loss=20.123, backward_time=0.114, optim_step_time=0.040, optim0_lr0=9.492e-04, train_time=0.383
[ip-10-0-0-95] 2022-04-28 00:14:14,114 (trainer:676) INFO: 19epoch:train:206-246batch: iter_time=1.339e-04, forward_time=0.111, loss_ctc=31.844, loss_att=15.458, acc=0.881, loss=20.374, backward_time=0.113, optim_step_time=0.040, optim0_lr0=9.518e-04, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:14:29,382 (trainer:676) INFO: 19epoch:train:247-287batch: iter_time=1.629e-04, forward_time=0.110, loss_ctc=30.973, loss_att=14.874, acc=0.880, loss=19.704, backward_time=0.109, optim_step_time=0.040, optim0_lr0=9.544e-04, train_time=0.372
[ip-10-0-0-95] 2022-04-28 00:14:44,860 (trainer:676) INFO: 19epoch:train:288-328batch: iter_time=1.691e-04, forward_time=0.112, loss_ctc=32.441, loss_att=15.327, acc=0.882, loss=20.461, backward_time=0.114, optim_step_time=0.040, optim0_lr0=9.570e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:15:00,614 (trainer:676) INFO: 19epoch:train:329-369batch: iter_time=1.643e-04, forward_time=0.112, loss_ctc=31.115, loss_att=14.816, acc=0.883, loss=19.706, backward_time=0.117, optim_step_time=0.040, optim0_lr0=9.596e-04, train_time=0.384
[ip-10-0-0-95] 2022-04-28 00:15:16,069 (trainer:676) INFO: 19epoch:train:370-410batch: iter_time=1.518e-04, forward_time=0.112, loss_ctc=32.419, loss_att=15.320, acc=0.881, loss=20.450, backward_time=0.112, optim_step_time=0.040, optim0_lr0=9.622e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:15:31,534 (trainer:676) INFO: 19epoch:train:411-451batch: iter_time=1.621e-04, forward_time=0.111, loss_ctc=32.171, loss_att=15.441, acc=0.871, loss=20.460, backward_time=0.112, optim_step_time=0.040, optim0_lr0=9.647e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:15:46,764 (trainer:676) INFO: 19epoch:train:452-492batch: iter_time=1.826e-04, forward_time=0.111, loss_ctc=33.105, loss_att=15.867, acc=0.876, loss=21.039, backward_time=0.110, optim_step_time=0.041, optim0_lr0=9.673e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-28 00:16:01,970 (trainer:676) INFO: 19epoch:train:493-533batch: iter_time=1.454e-04, forward_time=0.111, loss_ctc=33.437, loss_att=15.955, acc=0.876, loss=21.200, backward_time=0.109, optim_step_time=0.040, optim0_lr0=9.699e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-28 00:16:17,460 (trainer:676) INFO: 19epoch:train:534-574batch: iter_time=1.604e-04, forward_time=0.112, loss_ctc=31.879, loss_att=15.096, acc=0.878, loss=20.131, backward_time=0.113, optim_step_time=0.040, optim0_lr0=9.725e-04, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:16:30,065 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:16:33,046 (trainer:676) INFO: 19epoch:train:575-615batch: iter_time=1.531e-04, forward_time=0.111, loss_ctc=32.946, loss_att=15.693, acc=0.860, loss=20.869, backward_time=0.113, optim_step_time=0.040, optim0_lr0=9.751e-04, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:16:48,486 (trainer:676) INFO: 19epoch:train:616-656batch: iter_time=1.669e-04, forward_time=0.110, loss_ctc=32.811, loss_att=15.334, acc=0.873, loss=20.577, backward_time=0.112, optim_step_time=0.040, optim0_lr0=9.777e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:17:00,090 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:17:03,690 (trainer:676) INFO: 19epoch:train:657-697batch: iter_time=1.471e-04, forward_time=0.111, loss_ctc=32.232, loss_att=15.016, acc=0.871, loss=20.181, backward_time=0.107, optim_step_time=0.040, optim0_lr0=9.803e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-28 00:17:19,136 (trainer:676) INFO: 19epoch:train:698-738batch: iter_time=1.400e-04, forward_time=0.111, loss_ctc=32.405, loss_att=15.373, acc=0.881, loss=20.483, backward_time=0.112, optim_step_time=0.040, optim0_lr0=9.829e-04, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:17:34,508 (trainer:676) INFO: 19epoch:train:739-779batch: iter_time=1.465e-04, forward_time=0.111, loss_ctc=33.056, loss_att=15.366, acc=0.879, loss=20.673, backward_time=0.111, optim_step_time=0.041, optim0_lr0=9.855e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:17:49,820 (trainer:676) INFO: 19epoch:train:780-820batch: iter_time=1.672e-04, forward_time=0.110, loss_ctc=32.431, loss_att=15.108, acc=0.881, loss=20.305, backward_time=0.109, optim_step_time=0.040, optim0_lr0=9.881e-04, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:18:28,537 (trainer:334) INFO: 19epoch results: [train] iter_time=3.887e-04, forward_time=0.111, loss_ctc=32.055, loss_att=15.143, acc=0.877, loss=20.217, backward_time=0.111, optim_step_time=0.040, optim0_lr0=9.636e-04, train_time=0.376, time=5 minutes and 10.12 seconds, total_count=15656, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.155, cer_ctc=0.112, loss_att=13.650, acc=0.902, cer=0.095, wer=0.902, loss=17.101, time=7.38 seconds, total_count=551, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.77 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 00:18:34,153 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 00:18:34,170 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/9epoch.pth
[ip-10-0-0-95] 2022-04-28 00:18:34,170 (trainer:268) INFO: 20/50epoch started. Estimated time to finish: 3 hours, 2 minutes and 23.51 seconds
[ip-10-0-0-95] 2022-04-28 00:18:44,372 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:18:49,398 (trainer:676) INFO: 20epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=30.897, loss_att=14.384, acc=0.877, loss=19.338, backward_time=0.107, optim_step_time=0.040, optim0_lr0=9.909e-04, train_time=0.371
[ip-10-0-0-95] 2022-04-28 00:19:04,730 (trainer:676) INFO: 20epoch:train:42-82batch: iter_time=1.787e-04, forward_time=0.111, loss_ctc=29.349, loss_att=13.900, acc=0.887, loss=18.535, backward_time=0.112, optim_step_time=0.040, optim0_lr0=9.935e-04, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:19:20,351 (trainer:676) INFO: 20epoch:train:83-123batch: iter_time=1.166e-04, forward_time=0.111, loss_ctc=31.007, loss_att=14.459, acc=0.886, loss=19.424, backward_time=0.113, optim_step_time=0.041, optim0_lr0=9.961e-04, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:19:35,721 (trainer:676) INFO: 20epoch:train:124-164batch: iter_time=1.234e-04, forward_time=0.111, loss_ctc=32.571, loss_att=15.152, acc=0.886, loss=20.377, backward_time=0.110, optim_step_time=0.041, optim0_lr0=9.987e-04, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:19:51,517 (trainer:676) INFO: 20epoch:train:165-205batch: iter_time=1.261e-04, forward_time=0.113, loss_ctc=30.734, loss_att=14.537, acc=0.882, loss=19.396, backward_time=0.116, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.385
[ip-10-0-0-95] 2022-04-28 00:20:07,080 (trainer:676) INFO: 20epoch:train:206-246batch: iter_time=1.244e-04, forward_time=0.110, loss_ctc=32.163, loss_att=15.285, acc=0.883, loss=20.348, backward_time=0.114, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:20:22,577 (trainer:676) INFO: 20epoch:train:247-287batch: iter_time=1.200e-04, forward_time=0.111, loss_ctc=30.381, loss_att=13.682, acc=0.884, loss=18.691, backward_time=0.112, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:20:38,173 (trainer:676) INFO: 20epoch:train:288-328batch: iter_time=1.250e-04, forward_time=0.111, loss_ctc=31.148, loss_att=14.340, acc=0.880, loss=19.382, backward_time=0.113, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:20:53,450 (trainer:676) INFO: 20epoch:train:329-369batch: iter_time=1.249e-04, forward_time=0.111, loss_ctc=30.505, loss_att=14.081, acc=0.883, loss=19.008, backward_time=0.110, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 00:21:09,087 (trainer:676) INFO: 20epoch:train:370-410batch: iter_time=1.236e-04, forward_time=0.112, loss_ctc=31.942, loss_att=14.564, acc=0.880, loss=19.778, backward_time=0.114, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:21:24,735 (trainer:676) INFO: 20epoch:train:411-451batch: iter_time=1.380e-04, forward_time=0.112, loss_ctc=31.791, loss_att=14.602, acc=0.884, loss=19.759, backward_time=0.115, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:21:40,228 (trainer:676) INFO: 20epoch:train:452-492batch: iter_time=1.450e-04, forward_time=0.111, loss_ctc=31.923, loss_att=14.946, acc=0.876, loss=20.039, backward_time=0.112, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:21:55,563 (trainer:676) INFO: 20epoch:train:493-533batch: iter_time=1.435e-04, forward_time=0.113, loss_ctc=31.409, loss_att=14.477, acc=0.879, loss=19.556, backward_time=0.109, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:22:10,908 (trainer:676) INFO: 20epoch:train:534-574batch: iter_time=1.554e-04, forward_time=0.115, loss_ctc=30.033, loss_att=14.013, acc=0.883, loss=18.819, backward_time=0.108, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:22:26,656 (trainer:676) INFO: 20epoch:train:575-615batch: iter_time=1.409e-04, forward_time=0.115, loss_ctc=32.494, loss_att=15.603, acc=0.882, loss=20.670, backward_time=0.116, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 00:22:42,107 (trainer:676) INFO: 20epoch:train:616-656batch: iter_time=1.367e-04, forward_time=0.114, loss_ctc=30.300, loss_att=14.361, acc=0.887, loss=19.143, backward_time=0.111, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:22:57,649 (trainer:676) INFO: 20epoch:train:657-697batch: iter_time=1.359e-04, forward_time=0.114, loss_ctc=29.755, loss_att=14.154, acc=0.889, loss=18.834, backward_time=0.113, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:23:12,945 (trainer:676) INFO: 20epoch:train:698-738batch: iter_time=1.439e-04, forward_time=0.114, loss_ctc=30.443, loss_att=14.348, acc=0.882, loss=19.176, backward_time=0.108, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:23:28,198 (trainer:676) INFO: 20epoch:train:739-779batch: iter_time=1.252e-04, forward_time=0.113, loss_ctc=32.617, loss_att=14.967, acc=0.879, loss=20.262, backward_time=0.107, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 00:23:39,881 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:23:43,520 (trainer:676) INFO: 20epoch:train:780-820batch: iter_time=1.335e-04, forward_time=0.115, loss_ctc=30.075, loss_att=13.411, acc=0.876, loss=18.410, backward_time=0.107, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:24:22,011 (trainer:334) INFO: 20epoch results: [train] iter_time=3.786e-04, forward_time=0.112, loss_ctc=31.068, loss_att=14.448, acc=0.882, loss=19.434, backward_time=0.111, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.377, time=5 minutes and 10.98 seconds, total_count=16480, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.363, cer_ctc=0.114, loss_att=13.822, acc=0.903, cer=0.097, wer=0.899, loss=17.284, time=7.38 seconds, total_count=580, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.48 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 00:24:27,406 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 00:24:27,424 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/10epoch.pth
[ip-10-0-0-95] 2022-04-28 00:24:27,424 (trainer:268) INFO: 21/50epoch started. Estimated time to finish: 2 hours, 56 minutes and 30.85 seconds
[ip-10-0-0-95] 2022-04-28 00:24:42,748 (trainer:676) INFO: 21epoch:train:1-41batch: iter_time=0.005, forward_time=0.109, loss_ctc=28.826, loss_att=13.205, acc=0.892, loss=17.891, backward_time=0.107, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:24:57,903 (trainer:676) INFO: 21epoch:train:42-82batch: iter_time=1.245e-04, forward_time=0.110, loss_ctc=30.228, loss_att=13.967, acc=0.890, loss=18.846, backward_time=0.110, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 00:25:13,171 (trainer:676) INFO: 21epoch:train:83-123batch: iter_time=1.411e-04, forward_time=0.110, loss_ctc=30.595, loss_att=14.202, acc=0.891, loss=19.120, backward_time=0.109, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 00:25:28,705 (trainer:676) INFO: 21epoch:train:124-164batch: iter_time=1.390e-04, forward_time=0.111, loss_ctc=29.797, loss_att=13.587, acc=0.893, loss=18.450, backward_time=0.113, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:25:44,057 (trainer:676) INFO: 21epoch:train:165-205batch: iter_time=1.538e-04, forward_time=0.111, loss_ctc=28.722, loss_att=13.067, acc=0.889, loss=17.763, backward_time=0.110, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:25:59,505 (trainer:676) INFO: 21epoch:train:206-246batch: iter_time=1.436e-04, forward_time=0.112, loss_ctc=31.412, loss_att=14.520, acc=0.888, loss=19.587, backward_time=0.112, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:26:14,901 (trainer:676) INFO: 21epoch:train:247-287batch: iter_time=1.552e-04, forward_time=0.111, loss_ctc=31.022, loss_att=14.502, acc=0.885, loss=19.458, backward_time=0.112, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:26:15,808 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:26:30,380 (trainer:676) INFO: 21epoch:train:288-328batch: iter_time=1.454e-04, forward_time=0.111, loss_ctc=31.088, loss_att=14.047, acc=0.882, loss=19.159, backward_time=0.111, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:26:45,967 (trainer:676) INFO: 21epoch:train:329-369batch: iter_time=1.417e-04, forward_time=0.113, loss_ctc=30.495, loss_att=14.190, acc=0.888, loss=19.081, backward_time=0.113, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:27:01,453 (trainer:676) INFO: 21epoch:train:370-410batch: iter_time=1.344e-04, forward_time=0.111, loss_ctc=29.439, loss_att=13.620, acc=0.886, loss=18.365, backward_time=0.113, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:27:16,853 (trainer:676) INFO: 21epoch:train:411-451batch: iter_time=1.223e-04, forward_time=0.112, loss_ctc=29.512, loss_att=13.594, acc=0.887, loss=18.369, backward_time=0.110, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:27:32,575 (trainer:676) INFO: 21epoch:train:452-492batch: iter_time=1.484e-04, forward_time=0.112, loss_ctc=30.511, loss_att=14.094, acc=0.889, loss=19.019, backward_time=0.117, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 00:27:48,375 (trainer:676) INFO: 21epoch:train:493-533batch: iter_time=1.308e-04, forward_time=0.112, loss_ctc=31.599, loss_att=14.274, acc=0.880, loss=19.471, backward_time=0.115, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.385
[ip-10-0-0-95] 2022-04-28 00:28:03,806 (trainer:676) INFO: 21epoch:train:534-574batch: iter_time=1.300e-04, forward_time=0.111, loss_ctc=30.094, loss_att=13.594, acc=0.887, loss=18.544, backward_time=0.111, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:28:06,806 (trainer:618) WARNING: The grad norm is inf. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 00:28:19,409 (trainer:676) INFO: 21epoch:train:575-615batch: iter_time=1.283e-04, forward_time=0.112, loss_ctc=30.770, loss_att=14.470, acc=0.887, loss=19.360, backward_time=0.115, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:28:35,101 (trainer:676) INFO: 21epoch:train:616-656batch: iter_time=1.365e-04, forward_time=0.112, loss_ctc=30.449, loss_att=13.958, acc=0.884, loss=18.905, backward_time=0.114, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 00:28:50,536 (trainer:676) INFO: 21epoch:train:657-697batch: iter_time=1.543e-04, forward_time=0.112, loss_ctc=28.081, loss_att=12.798, acc=0.888, loss=17.383, backward_time=0.110, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:29:05,693 (trainer:676) INFO: 21epoch:train:698-738batch: iter_time=1.393e-04, forward_time=0.111, loss_ctc=30.794, loss_att=14.293, acc=0.889, loss=19.243, backward_time=0.107, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 00:29:12,750 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:29:21,452 (trainer:676) INFO: 21epoch:train:739-779batch: iter_time=1.526e-04, forward_time=0.113, loss_ctc=30.032, loss_att=14.266, acc=0.877, loss=18.996, backward_time=0.116, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 00:29:37,125 (trainer:676) INFO: 21epoch:train:780-820batch: iter_time=1.659e-04, forward_time=0.111, loss_ctc=31.488, loss_att=14.141, acc=0.880, loss=19.345, backward_time=0.114, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 00:30:15,698 (trainer:334) INFO: 21epoch results: [train] iter_time=3.885e-04, forward_time=0.111, loss_ctc=30.221, loss_att=13.901, acc=0.887, loss=18.797, backward_time=0.112, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.377, time=5 minutes and 11.27 seconds, total_count=17304, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=24.403, cer_ctc=0.109, loss_att=13.262, acc=0.905, cer=0.095, wer=0.898, loss=16.604, time=7.46 seconds, total_count=609, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.54 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 00:30:21,126 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 00:30:21,144 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/11epoch.pth
[ip-10-0-0-95] 2022-04-28 00:30:21,144 (trainer:268) INFO: 22/50epoch started. Estimated time to finish: 2 hours, 50 minutes and 38.77 seconds
[ip-10-0-0-95] 2022-04-28 00:30:36,747 (trainer:676) INFO: 22epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=28.746, loss_att=13.072, acc=0.898, loss=17.774, backward_time=0.111, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:30:39,022 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:30:51,883 (trainer:676) INFO: 22epoch:train:42-82batch: iter_time=1.324e-04, forward_time=0.111, loss_ctc=27.852, loss_att=13.193, acc=0.887, loss=17.591, backward_time=0.109, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 00:31:07,449 (trainer:676) INFO: 22epoch:train:83-123batch: iter_time=1.590e-04, forward_time=0.111, loss_ctc=29.230, loss_att=13.532, acc=0.893, loss=18.241, backward_time=0.114, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:31:22,770 (trainer:676) INFO: 22epoch:train:124-164batch: iter_time=1.489e-04, forward_time=0.114, loss_ctc=29.024, loss_att=12.936, acc=0.892, loss=17.762, backward_time=0.107, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:31:38,540 (trainer:676) INFO: 22epoch:train:165-205batch: iter_time=1.575e-04, forward_time=0.114, loss_ctc=30.435, loss_att=13.968, acc=0.889, loss=18.908, backward_time=0.116, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 00:31:52,724 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:31:54,097 (trainer:676) INFO: 22epoch:train:206-246batch: iter_time=1.372e-04, forward_time=0.115, loss_ctc=29.562, loss_att=13.742, acc=0.886, loss=18.488, backward_time=0.111, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:32:09,626 (trainer:676) INFO: 22epoch:train:247-287batch: iter_time=1.365e-04, forward_time=0.113, loss_ctc=29.704, loss_att=13.742, acc=0.887, loss=18.531, backward_time=0.113, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:32:25,520 (trainer:676) INFO: 22epoch:train:288-328batch: iter_time=1.591e-04, forward_time=0.117, loss_ctc=29.049, loss_att=13.445, acc=0.896, loss=18.126, backward_time=0.116, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.387
[ip-10-0-0-95] 2022-04-28 00:32:41,168 (trainer:676) INFO: 22epoch:train:329-369batch: iter_time=1.447e-04, forward_time=0.116, loss_ctc=30.045, loss_att=13.172, acc=0.879, loss=18.234, backward_time=0.110, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:32:56,787 (trainer:676) INFO: 22epoch:train:370-410batch: iter_time=1.502e-04, forward_time=0.115, loss_ctc=29.634, loss_att=13.399, acc=0.896, loss=18.270, backward_time=0.111, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:33:12,201 (trainer:676) INFO: 22epoch:train:411-451batch: iter_time=1.511e-04, forward_time=0.113, loss_ctc=30.722, loss_att=14.021, acc=0.892, loss=19.032, backward_time=0.110, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:33:27,445 (trainer:676) INFO: 22epoch:train:452-492batch: iter_time=1.337e-04, forward_time=0.111, loss_ctc=29.555, loss_att=13.509, acc=0.891, loss=18.323, backward_time=0.109, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 00:33:43,121 (trainer:676) INFO: 22epoch:train:493-533batch: iter_time=1.571e-04, forward_time=0.112, loss_ctc=29.653, loss_att=13.619, acc=0.891, loss=18.429, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 00:33:58,884 (trainer:676) INFO: 22epoch:train:534-574batch: iter_time=1.487e-04, forward_time=0.112, loss_ctc=28.983, loss_att=13.161, acc=0.894, loss=17.908, backward_time=0.116, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 00:34:14,257 (trainer:676) INFO: 22epoch:train:575-615batch: iter_time=1.461e-04, forward_time=0.111, loss_ctc=29.932, loss_att=13.359, acc=0.885, loss=18.331, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:34:29,870 (trainer:676) INFO: 22epoch:train:616-656batch: iter_time=1.633e-04, forward_time=0.112, loss_ctc=28.437, loss_att=12.715, acc=0.896, loss=17.432, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:34:45,171 (trainer:676) INFO: 22epoch:train:657-697batch: iter_time=1.652e-04, forward_time=0.111, loss_ctc=28.977, loss_att=13.432, acc=0.888, loss=18.095, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:35:00,366 (trainer:676) INFO: 22epoch:train:698-738batch: iter_time=1.336e-04, forward_time=0.110, loss_ctc=29.655, loss_att=13.301, acc=0.884, loss=18.207, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 00:35:15,921 (trainer:676) INFO: 22epoch:train:739-779batch: iter_time=1.420e-04, forward_time=0.110, loss_ctc=30.247, loss_att=13.788, acc=0.891, loss=18.725, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:35:31,289 (trainer:676) INFO: 22epoch:train:780-820batch: iter_time=1.256e-04, forward_time=0.110, loss_ctc=30.109, loss_att=13.428, acc=0.888, loss=18.432, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:36:10,058 (trainer:334) INFO: 22epoch results: [train] iter_time=4.010e-04, forward_time=0.112, loss_ctc=29.460, loss_att=13.409, acc=0.890, loss=18.225, backward_time=0.112, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.378, time=5 minutes and 11.74 seconds, total_count=18128, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=23.256, cer_ctc=0.103, loss_att=12.632, acc=0.911, cer=0.088, wer=0.897, loss=15.819, time=7.39 seconds, total_count=638, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.78 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 00:36:15,683 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 00:36:15,700 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/12epoch.pth
[ip-10-0-0-95] 2022-04-28 00:36:15,701 (trainer:268) INFO: 23/50epoch started. Estimated time to finish: 2 hours, 44 minutes and 47.62 seconds
[ip-10-0-0-95] 2022-04-28 00:36:31,005 (trainer:676) INFO: 23epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=26.987, loss_att=11.798, acc=0.901, loss=16.355, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:36:46,453 (trainer:676) INFO: 23epoch:train:42-82batch: iter_time=1.219e-04, forward_time=0.110, loss_ctc=27.209, loss_att=12.241, acc=0.900, loss=16.732, backward_time=0.114, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:36:52,687 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:37:02,156 (trainer:676) INFO: 23epoch:train:83-123batch: iter_time=1.209e-04, forward_time=0.112, loss_ctc=28.600, loss_att=12.934, acc=0.886, loss=17.634, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 00:37:17,536 (trainer:676) INFO: 23epoch:train:124-164batch: iter_time=1.271e-04, forward_time=0.112, loss_ctc=28.757, loss_att=12.961, acc=0.897, loss=17.700, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:37:33,101 (trainer:676) INFO: 23epoch:train:165-205batch: iter_time=1.241e-04, forward_time=0.112, loss_ctc=28.289, loss_att=12.728, acc=0.899, loss=17.396, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:37:48,413 (trainer:676) INFO: 23epoch:train:206-246batch: iter_time=1.332e-04, forward_time=0.111, loss_ctc=28.387, loss_att=12.639, acc=0.896, loss=17.363, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:38:04,097 (trainer:676) INFO: 23epoch:train:247-287batch: iter_time=1.160e-04, forward_time=0.112, loss_ctc=28.292, loss_att=12.400, acc=0.896, loss=17.168, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 00:38:19,442 (trainer:676) INFO: 23epoch:train:288-328batch: iter_time=1.221e-04, forward_time=0.111, loss_ctc=27.352, loss_att=12.352, acc=0.898, loss=16.852, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:38:34,981 (trainer:676) INFO: 23epoch:train:329-369batch: iter_time=1.223e-04, forward_time=0.112, loss_ctc=28.447, loss_att=12.926, acc=0.899, loss=17.583, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:38:50,345 (trainer:676) INFO: 23epoch:train:370-410batch: iter_time=1.190e-04, forward_time=0.112, loss_ctc=28.821, loss_att=13.215, acc=0.895, loss=17.896, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:39:05,889 (trainer:676) INFO: 23epoch:train:411-451batch: iter_time=1.195e-04, forward_time=0.111, loss_ctc=28.521, loss_att=12.662, acc=0.899, loss=17.420, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:39:21,222 (trainer:676) INFO: 23epoch:train:452-492batch: iter_time=1.364e-04, forward_time=0.110, loss_ctc=28.052, loss_att=12.384, acc=0.898, loss=17.084, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:39:36,495 (trainer:676) INFO: 23epoch:train:493-533batch: iter_time=1.291e-04, forward_time=0.111, loss_ctc=30.585, loss_att=13.542, acc=0.891, loss=18.655, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 00:39:52,144 (trainer:676) INFO: 23epoch:train:534-574batch: iter_time=1.204e-04, forward_time=0.112, loss_ctc=28.311, loss_att=12.812, acc=0.894, loss=17.462, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:40:07,471 (trainer:676) INFO: 23epoch:train:575-615batch: iter_time=1.215e-04, forward_time=0.109, loss_ctc=28.424, loss_att=12.755, acc=0.893, loss=17.456, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:40:22,527 (trainer:676) INFO: 23epoch:train:616-656batch: iter_time=1.258e-04, forward_time=0.110, loss_ctc=29.475, loss_att=13.051, acc=0.891, loss=17.978, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 00:40:24,130 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:40:38,170 (trainer:676) INFO: 23epoch:train:657-697batch: iter_time=1.292e-04, forward_time=0.111, loss_ctc=30.006, loss_att=13.471, acc=0.887, loss=18.432, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:40:53,758 (trainer:676) INFO: 23epoch:train:698-738batch: iter_time=1.158e-04, forward_time=0.112, loss_ctc=27.885, loss_att=12.000, acc=0.897, loss=16.765, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:41:08,983 (trainer:676) INFO: 23epoch:train:739-779batch: iter_time=1.227e-04, forward_time=0.110, loss_ctc=29.402, loss_att=12.966, acc=0.896, loss=17.897, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 00:41:24,287 (trainer:676) INFO: 23epoch:train:780-820batch: iter_time=1.249e-04, forward_time=0.110, loss_ctc=29.746, loss_att=13.474, acc=0.896, loss=18.356, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:42:03,173 (trainer:334) INFO: 23epoch results: [train] iter_time=3.607e-04, forward_time=0.111, loss_ctc=28.582, loss_att=12.765, acc=0.895, loss=17.510, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376, time=5 minutes and 10.2 seconds, total_count=18952, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=23.498, cer_ctc=0.101, loss_att=12.463, acc=0.911, cer=0.089, wer=0.898, loss=15.773, time=7.39 seconds, total_count=667, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.88 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 00:42:08,676 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 00:42:08,693 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/13epoch.pth
[ip-10-0-0-95] 2022-04-28 00:42:08,693 (trainer:268) INFO: 24/50epoch started. Estimated time to finish: 2 hours, 38 minutes and 54.33 seconds
[ip-10-0-0-95] 2022-04-28 00:42:23,860 (trainer:676) INFO: 24epoch:train:1-41batch: iter_time=0.005, forward_time=0.111, loss_ctc=26.264, loss_att=11.544, acc=0.906, loss=15.960, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 00:42:39,022 (trainer:676) INFO: 24epoch:train:42-82batch: iter_time=1.492e-04, forward_time=0.110, loss_ctc=26.523, loss_att=11.428, acc=0.903, loss=15.956, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 00:42:54,252 (trainer:676) INFO: 24epoch:train:83-123batch: iter_time=1.347e-04, forward_time=0.110, loss_ctc=28.010, loss_att=12.115, acc=0.896, loss=16.883, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 00:43:10,228 (trainer:676) INFO: 24epoch:train:124-164batch: iter_time=1.410e-04, forward_time=0.112, loss_ctc=28.437, loss_att=12.627, acc=0.898, loss=17.370, backward_time=0.118, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.389
[ip-10-0-0-95] 2022-04-28 00:43:25,880 (trainer:676) INFO: 24epoch:train:165-205batch: iter_time=1.407e-04, forward_time=0.111, loss_ctc=28.929, loss_att=12.779, acc=0.898, loss=17.624, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:43:41,244 (trainer:676) INFO: 24epoch:train:206-246batch: iter_time=1.355e-04, forward_time=0.111, loss_ctc=28.156, loss_att=12.410, acc=0.900, loss=17.134, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:43:56,587 (trainer:676) INFO: 24epoch:train:247-287batch: iter_time=1.240e-04, forward_time=0.111, loss_ctc=27.953, loss_att=12.425, acc=0.898, loss=17.084, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:44:07,844 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:44:11,973 (trainer:676) INFO: 24epoch:train:288-328batch: iter_time=1.361e-04, forward_time=0.110, loss_ctc=29.728, loss_att=13.481, acc=0.881, loss=18.355, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:44:27,091 (trainer:676) INFO: 24epoch:train:329-369batch: iter_time=1.314e-04, forward_time=0.110, loss_ctc=29.176, loss_att=12.474, acc=0.896, loss=17.485, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 00:44:29,662 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 00:44:42,335 (trainer:676) INFO: 24epoch:train:370-410batch: iter_time=1.274e-04, forward_time=0.111, loss_ctc=26.420, loss_att=11.403, acc=0.900, loss=15.908, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 00:44:57,841 (trainer:676) INFO: 24epoch:train:411-451batch: iter_time=1.520e-04, forward_time=0.111, loss_ctc=28.903, loss_att=12.620, acc=0.899, loss=17.505, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:45:13,376 (trainer:676) INFO: 24epoch:train:452-492batch: iter_time=1.304e-04, forward_time=0.112, loss_ctc=28.551, loss_att=12.396, acc=0.899, loss=17.243, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:45:28,890 (trainer:676) INFO: 24epoch:train:493-533batch: iter_time=1.325e-04, forward_time=0.110, loss_ctc=27.155, loss_att=11.582, acc=0.902, loss=16.254, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:45:44,236 (trainer:676) INFO: 24epoch:train:534-574batch: iter_time=1.276e-04, forward_time=0.112, loss_ctc=28.692, loss_att=12.405, acc=0.899, loss=17.291, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:45:59,964 (trainer:676) INFO: 24epoch:train:575-615batch: iter_time=1.290e-04, forward_time=0.112, loss_ctc=27.602, loss_att=12.233, acc=0.900, loss=16.844, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 00:46:15,405 (trainer:676) INFO: 24epoch:train:616-656batch: iter_time=1.315e-04, forward_time=0.112, loss_ctc=27.542, loss_att=12.278, acc=0.904, loss=16.858, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:46:31,047 (trainer:676) INFO: 24epoch:train:657-697batch: iter_time=1.272e-04, forward_time=0.111, loss_ctc=26.992, loss_att=11.886, acc=0.901, loss=16.418, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:46:46,808 (trainer:676) INFO: 24epoch:train:698-738batch: iter_time=1.336e-04, forward_time=0.112, loss_ctc=25.437, loss_att=11.395, acc=0.904, loss=15.608, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 00:47:02,067 (trainer:676) INFO: 24epoch:train:739-779batch: iter_time=1.375e-04, forward_time=0.110, loss_ctc=28.705, loss_att=12.685, acc=0.898, loss=17.491, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 00:47:05,214 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:47:17,675 (trainer:676) INFO: 24epoch:train:780-820batch: iter_time=1.481e-04, forward_time=0.112, loss_ctc=27.727, loss_att=12.261, acc=0.898, loss=16.901, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:47:56,311 (trainer:334) INFO: 24epoch results: [train] iter_time=3.886e-04, forward_time=0.111, loss_ctc=27.883, loss_att=12.236, acc=0.899, loss=16.930, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376, time=5 minutes and 10.45 seconds, total_count=19776, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=26.001, cer_ctc=0.109, loss_att=13.580, acc=0.904, cer=0.095, wer=0.890, loss=17.306, time=7.41 seconds, total_count=696, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.76 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 00:48:01,954 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 00:48:01,971 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/14epoch.pth
[ip-10-0-0-95] 2022-04-28 00:48:01,972 (trainer:268) INFO: 25/50epoch started. Estimated time to finish: 2 hours, 33 minutes and 1.37 seconds
[ip-10-0-0-95] 2022-04-28 00:48:17,208 (trainer:676) INFO: 25epoch:train:1-41batch: iter_time=0.005, forward_time=0.109, loss_ctc=26.413, loss_att=11.477, acc=0.905, loss=15.958, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 00:48:32,188 (trainer:676) INFO: 25epoch:train:42-82batch: iter_time=1.634e-04, forward_time=0.110, loss_ctc=26.655, loss_att=11.700, acc=0.908, loss=16.187, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 00:48:47,291 (trainer:676) INFO: 25epoch:train:83-123batch: iter_time=1.506e-04, forward_time=0.110, loss_ctc=27.430, loss_att=12.056, acc=0.900, loss=16.668, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 00:49:02,521 (trainer:676) INFO: 25epoch:train:124-164batch: iter_time=1.373e-04, forward_time=0.110, loss_ctc=26.659, loss_att=11.662, acc=0.904, loss=16.161, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 00:49:18,122 (trainer:676) INFO: 25epoch:train:165-205batch: iter_time=1.505e-04, forward_time=0.111, loss_ctc=27.557, loss_att=12.040, acc=0.900, loss=16.695, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:49:33,630 (trainer:676) INFO: 25epoch:train:206-246batch: iter_time=1.408e-04, forward_time=0.112, loss_ctc=26.719, loss_att=11.789, acc=0.907, loss=16.268, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:49:49,198 (trainer:676) INFO: 25epoch:train:247-287batch: iter_time=1.263e-04, forward_time=0.111, loss_ctc=28.018, loss_att=12.211, acc=0.904, loss=16.953, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:50:04,677 (trainer:676) INFO: 25epoch:train:288-328batch: iter_time=1.345e-04, forward_time=0.111, loss_ctc=26.916, loss_att=11.829, acc=0.906, loss=16.355, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:50:20,338 (trainer:676) INFO: 25epoch:train:329-369batch: iter_time=1.479e-04, forward_time=0.111, loss_ctc=27.888, loss_att=11.919, acc=0.902, loss=16.710, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 00:50:35,823 (trainer:676) INFO: 25epoch:train:370-410batch: iter_time=1.467e-04, forward_time=0.112, loss_ctc=28.716, loss_att=12.590, acc=0.904, loss=17.428, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:50:51,247 (trainer:676) INFO: 25epoch:train:411-451batch: iter_time=1.314e-04, forward_time=0.111, loss_ctc=27.173, loss_att=11.434, acc=0.901, loss=16.156, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:50:56,257 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:51:06,694 (trainer:676) INFO: 25epoch:train:452-492batch: iter_time=1.207e-04, forward_time=0.112, loss_ctc=27.390, loss_att=12.532, acc=0.889, loss=16.989, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:51:21,624 (trainer:676) INFO: 25epoch:train:493-533batch: iter_time=1.242e-04, forward_time=0.109, loss_ctc=28.545, loss_att=12.292, acc=0.899, loss=17.168, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.364
[ip-10-0-0-95] 2022-04-28 00:51:37,022 (trainer:676) INFO: 25epoch:train:534-574batch: iter_time=1.331e-04, forward_time=0.111, loss_ctc=25.848, loss_att=11.232, acc=0.899, loss=15.617, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:51:52,278 (trainer:676) INFO: 25epoch:train:575-615batch: iter_time=1.249e-04, forward_time=0.110, loss_ctc=27.567, loss_att=11.949, acc=0.900, loss=16.635, backward_time=0.108, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 00:52:07,767 (trainer:676) INFO: 25epoch:train:616-656batch: iter_time=1.499e-04, forward_time=0.111, loss_ctc=27.780, loss_att=12.158, acc=0.900, loss=16.844, backward_time=0.112, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:52:23,443 (trainer:676) INFO: 25epoch:train:657-697batch: iter_time=1.437e-04, forward_time=0.112, loss_ctc=26.168, loss_att=11.197, acc=0.904, loss=15.688, backward_time=0.116, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 00:52:32,400 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:52:38,994 (trainer:676) INFO: 25epoch:train:698-738batch: iter_time=1.439e-04, forward_time=0.111, loss_ctc=26.804, loss_att=11.795, acc=0.897, loss=16.298, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 00:52:54,661 (trainer:676) INFO: 25epoch:train:739-779batch: iter_time=1.579e-04, forward_time=0.111, loss_ctc=26.777, loss_att=11.969, acc=0.901, loss=16.411, backward_time=0.116, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 00:53:10,314 (trainer:676) INFO: 25epoch:train:780-820batch: iter_time=1.473e-04, forward_time=0.115, loss_ctc=28.054, loss_att=12.183, acc=0.902, loss=16.944, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:53:49,238 (trainer:334) INFO: 25epoch results: [train] iter_time=3.828e-04, forward_time=0.111, loss_ctc=27.258, loss_att=11.901, acc=0.902, loss=16.508, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376, time=5 minutes and 9.93 seconds, total_count=20600, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=24.292, cer_ctc=0.103, loss_att=12.697, acc=0.911, cer=0.090, wer=0.882, loss=16.176, time=7.37 seconds, total_count=725, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.96 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 00:53:54,762 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 00:53:54,780 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/15epoch.pth
[ip-10-0-0-95] 2022-04-28 00:53:54,780 (trainer:268) INFO: 26/50epoch started. Estimated time to finish: 2 hours, 27 minutes and 7.92 seconds
[ip-10-0-0-95] 2022-04-28 00:54:10,607 (trainer:676) INFO: 26epoch:train:1-41batch: iter_time=0.005, forward_time=0.112, loss_ctc=25.940, loss_att=11.356, acc=0.912, loss=15.731, backward_time=0.116, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.386
[ip-10-0-0-95] 2022-04-28 00:54:25,683 (trainer:676) INFO: 26epoch:train:42-82batch: iter_time=1.490e-04, forward_time=0.108, loss_ctc=26.578, loss_att=11.363, acc=0.906, loss=15.928, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 00:54:34,962 (trainer:618) WARNING: The grad norm is inf. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 00:54:41,137 (trainer:676) INFO: 26epoch:train:83-123batch: iter_time=1.473e-04, forward_time=0.111, loss_ctc=26.300, loss_att=11.335, acc=0.909, loss=15.824, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:54:56,567 (trainer:676) INFO: 26epoch:train:124-164batch: iter_time=1.412e-04, forward_time=0.112, loss_ctc=26.168, loss_att=11.115, acc=0.908, loss=15.631, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 00:55:12,048 (trainer:676) INFO: 26epoch:train:165-205batch: iter_time=1.633e-04, forward_time=0.111, loss_ctc=27.501, loss_att=11.879, acc=0.901, loss=16.565, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 00:55:27,666 (trainer:676) INFO: 26epoch:train:206-246batch: iter_time=1.547e-04, forward_time=0.111, loss_ctc=25.938, loss_att=11.327, acc=0.907, loss=15.711, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:55:43,261 (trainer:676) INFO: 26epoch:train:247-287batch: iter_time=1.475e-04, forward_time=0.112, loss_ctc=25.686, loss_att=11.559, acc=0.909, loss=15.797, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 00:55:58,217 (trainer:676) INFO: 26epoch:train:288-328batch: iter_time=1.352e-04, forward_time=0.110, loss_ctc=27.118, loss_att=11.870, acc=0.903, loss=16.444, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.364
[ip-10-0-0-95] 2022-04-28 00:56:13,416 (trainer:676) INFO: 26epoch:train:329-369batch: iter_time=1.293e-04, forward_time=0.110, loss_ctc=28.315, loss_att=12.277, acc=0.902, loss=17.088, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 00:56:28,751 (trainer:676) INFO: 26epoch:train:370-410batch: iter_time=1.284e-04, forward_time=0.110, loss_ctc=26.383, loss_att=11.382, acc=0.907, loss=15.882, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 00:56:44,252 (trainer:676) INFO: 26epoch:train:411-451batch: iter_time=1.405e-04, forward_time=0.111, loss_ctc=26.214, loss_att=10.943, acc=0.908, loss=15.524, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:56:59,483 (trainer:676) INFO: 26epoch:train:452-492batch: iter_time=1.395e-04, forward_time=0.110, loss_ctc=27.552, loss_att=11.900, acc=0.905, loss=16.596, backward_time=0.109, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 00:57:14,868 (trainer:676) INFO: 26epoch:train:493-533batch: iter_time=1.384e-04, forward_time=0.110, loss_ctc=26.674, loss_att=11.440, acc=0.900, loss=16.010, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 00:57:19,951 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:57:30,365 (trainer:676) INFO: 26epoch:train:534-574batch: iter_time=1.222e-04, forward_time=0.111, loss_ctc=26.203, loss_att=11.183, acc=0.901, loss=15.689, backward_time=0.112, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 00:57:45,980 (trainer:676) INFO: 26epoch:train:575-615batch: iter_time=1.428e-04, forward_time=0.112, loss_ctc=25.998, loss_att=11.462, acc=0.913, loss=15.822, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:58:01,094 (trainer:676) INFO: 26epoch:train:616-656batch: iter_time=1.382e-04, forward_time=0.110, loss_ctc=26.924, loss_att=11.891, acc=0.908, loss=16.401, backward_time=0.108, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 00:58:16,746 (trainer:676) INFO: 26epoch:train:657-697batch: iter_time=1.470e-04, forward_time=0.112, loss_ctc=26.629, loss_att=11.350, acc=0.907, loss=15.934, backward_time=0.115, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:58:32,368 (trainer:676) INFO: 26epoch:train:698-738batch: iter_time=1.471e-04, forward_time=0.111, loss_ctc=25.321, loss_att=10.897, acc=0.909, loss=15.225, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 00:58:36,132 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 00:58:47,673 (trainer:676) INFO: 26epoch:train:739-779batch: iter_time=1.633e-04, forward_time=0.111, loss_ctc=25.842, loss_att=11.117, acc=0.893, loss=15.534, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:59:02,968 (trainer:676) INFO: 26epoch:train:780-820batch: iter_time=1.555e-04, forward_time=0.111, loss_ctc=26.781, loss_att=11.487, acc=0.903, loss=16.075, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 00:59:41,831 (trainer:334) INFO: 26epoch results: [train] iter_time=3.922e-04, forward_time=0.111, loss_ctc=26.491, loss_att=11.442, acc=0.905, loss=15.957, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376, time=5 minutes and 9.76 seconds, total_count=21424, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=24.121, cer_ctc=0.101, loss_att=13.076, acc=0.912, cer=0.086, wer=0.891, loss=16.390, time=7.38 seconds, total_count=754, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.91 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 00:59:47,508 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 00:59:47,525 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/16epoch.pth
[ip-10-0-0-95] 2022-04-28 00:59:47,526 (trainer:268) INFO: 27/50epoch started. Estimated time to finish: 2 hours, 21 minutes and 14.46 seconds
[ip-10-0-0-95] 2022-04-28 01:00:02,795 (trainer:676) INFO: 27epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=25.172, loss_att=10.588, acc=0.916, loss=14.963, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:00:13,884 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:00:17,894 (trainer:676) INFO: 27epoch:train:42-82batch: iter_time=1.766e-04, forward_time=0.110, loss_ctc=26.116, loss_att=10.732, acc=0.908, loss=15.347, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 01:00:33,176 (trainer:676) INFO: 27epoch:train:83-123batch: iter_time=1.410e-04, forward_time=0.112, loss_ctc=25.736, loss_att=10.917, acc=0.918, loss=15.363, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:00:48,649 (trainer:676) INFO: 27epoch:train:124-164batch: iter_time=1.228e-04, forward_time=0.112, loss_ctc=25.213, loss_att=10.522, acc=0.907, loss=14.929, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 01:01:04,278 (trainer:676) INFO: 27epoch:train:165-205batch: iter_time=1.241e-04, forward_time=0.111, loss_ctc=26.594, loss_att=11.323, acc=0.909, loss=15.905, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 01:01:19,620 (trainer:676) INFO: 27epoch:train:206-246batch: iter_time=1.600e-04, forward_time=0.111, loss_ctc=26.257, loss_att=11.116, acc=0.905, loss=15.658, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:01:35,217 (trainer:676) INFO: 27epoch:train:247-287batch: iter_time=1.689e-04, forward_time=0.111, loss_ctc=26.325, loss_att=10.795, acc=0.910, loss=15.454, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 01:01:51,029 (trainer:676) INFO: 27epoch:train:288-328batch: iter_time=1.542e-04, forward_time=0.112, loss_ctc=26.343, loss_att=11.194, acc=0.911, loss=15.739, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.385
[ip-10-0-0-95] 2022-04-28 01:02:06,642 (trainer:676) INFO: 27epoch:train:329-369batch: iter_time=1.580e-04, forward_time=0.111, loss_ctc=25.709, loss_att=10.959, acc=0.909, loss=15.384, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 01:02:21,979 (trainer:676) INFO: 27epoch:train:370-410batch: iter_time=1.550e-04, forward_time=0.111, loss_ctc=25.534, loss_att=10.711, acc=0.906, loss=15.158, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:02:37,677 (trainer:676) INFO: 27epoch:train:411-451batch: iter_time=1.421e-04, forward_time=0.111, loss_ctc=25.629, loss_att=10.732, acc=0.910, loss=15.201, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 01:02:53,218 (trainer:676) INFO: 27epoch:train:452-492batch: iter_time=1.548e-04, forward_time=0.112, loss_ctc=24.238, loss_att=10.366, acc=0.911, loss=14.527, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:03:08,917 (trainer:676) INFO: 27epoch:train:493-533batch: iter_time=1.587e-04, forward_time=0.112, loss_ctc=27.140, loss_att=11.740, acc=0.905, loss=16.360, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 01:03:24,491 (trainer:676) INFO: 27epoch:train:534-574batch: iter_time=1.628e-04, forward_time=0.112, loss_ctc=26.996, loss_att=11.438, acc=0.908, loss=16.106, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 01:03:39,867 (trainer:676) INFO: 27epoch:train:575-615batch: iter_time=1.719e-04, forward_time=0.112, loss_ctc=25.090, loss_att=10.712, acc=0.913, loss=15.025, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:03:55,364 (trainer:676) INFO: 27epoch:train:616-656batch: iter_time=1.648e-04, forward_time=0.111, loss_ctc=27.045, loss_att=11.513, acc=0.906, loss=16.173, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:04:10,747 (trainer:676) INFO: 27epoch:train:657-697batch: iter_time=1.570e-04, forward_time=0.110, loss_ctc=25.737, loss_att=11.055, acc=0.913, loss=15.459, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:04:26,005 (trainer:676) INFO: 27epoch:train:698-738batch: iter_time=1.582e-04, forward_time=0.111, loss_ctc=25.890, loss_att=11.228, acc=0.914, loss=15.627, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:04:41,377 (trainer:676) INFO: 27epoch:train:739-779batch: iter_time=1.323e-04, forward_time=0.110, loss_ctc=26.035, loss_att=11.256, acc=0.908, loss=15.690, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:04:43,440 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:04:56,719 (trainer:676) INFO: 27epoch:train:780-820batch: iter_time=1.262e-04, forward_time=0.111, loss_ctc=26.367, loss_att=11.907, acc=0.894, loss=16.245, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:05:35,614 (trainer:334) INFO: 27epoch results: [train] iter_time=3.900e-04, forward_time=0.111, loss_ctc=25.957, loss_att=11.037, acc=0.909, loss=15.513, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377, time=5 minutes and 10.73 seconds, total_count=22248, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.820, cer_ctc=0.102, loss_att=13.126, acc=0.910, cer=0.087, wer=0.871, loss=16.934, time=7.3 seconds, total_count=783, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.06 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 01:05:41,178 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 01:05:41,195 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/17epoch.pth
[ip-10-0-0-95] 2022-04-28 01:05:41,196 (trainer:268) INFO: 28/50epoch started. Estimated time to finish: 2 hours, 15 minutes and 21.84 seconds
[ip-10-0-0-95] 2022-04-28 01:05:56,352 (trainer:676) INFO: 28epoch:train:1-41batch: iter_time=0.005, forward_time=0.109, loss_ctc=23.330, loss_att=9.838, acc=0.916, loss=13.886, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 01:06:11,431 (trainer:676) INFO: 28epoch:train:42-82batch: iter_time=1.321e-04, forward_time=0.108, loss_ctc=24.712, loss_att=10.469, acc=0.911, loss=14.742, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 01:06:26,162 (trainer:676) INFO: 28epoch:train:83-123batch: iter_time=1.208e-04, forward_time=0.108, loss_ctc=25.565, loss_att=10.757, acc=0.914, loss=15.199, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.359
[ip-10-0-0-95] 2022-04-28 01:06:41,156 (trainer:676) INFO: 28epoch:train:124-164batch: iter_time=1.321e-04, forward_time=0.108, loss_ctc=24.538, loss_att=10.450, acc=0.916, loss=14.677, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 01:06:56,134 (trainer:676) INFO: 28epoch:train:165-205batch: iter_time=1.286e-04, forward_time=0.108, loss_ctc=24.461, loss_att=10.175, acc=0.918, loss=14.461, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 01:07:11,420 (trainer:676) INFO: 28epoch:train:206-246batch: iter_time=1.401e-04, forward_time=0.109, loss_ctc=24.105, loss_att=10.215, acc=0.916, loss=14.382, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:07:26,530 (trainer:676) INFO: 28epoch:train:247-287batch: iter_time=1.477e-04, forward_time=0.108, loss_ctc=26.319, loss_att=11.400, acc=0.907, loss=15.876, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 01:07:41,281 (trainer:676) INFO: 28epoch:train:288-328batch: iter_time=1.374e-04, forward_time=0.107, loss_ctc=26.012, loss_att=10.990, acc=0.912, loss=15.497, backward_time=0.104, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.359
[ip-10-0-0-95] 2022-04-28 01:07:56,433 (trainer:676) INFO: 28epoch:train:329-369batch: iter_time=1.396e-04, forward_time=0.109, loss_ctc=24.648, loss_att=10.382, acc=0.912, loss=14.662, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 01:08:11,845 (trainer:676) INFO: 28epoch:train:370-410batch: iter_time=1.352e-04, forward_time=0.110, loss_ctc=26.557, loss_att=11.621, acc=0.911, loss=16.102, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:08:26,739 (trainer:676) INFO: 28epoch:train:411-451batch: iter_time=1.223e-04, forward_time=0.110, loss_ctc=24.711, loss_att=10.256, acc=0.915, loss=14.592, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.363
[ip-10-0-0-95] 2022-04-28 01:08:41,986 (trainer:676) INFO: 28epoch:train:452-492batch: iter_time=1.229e-04, forward_time=0.111, loss_ctc=24.365, loss_att=10.228, acc=0.913, loss=14.469, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:08:57,476 (trainer:676) INFO: 28epoch:train:493-533batch: iter_time=1.330e-04, forward_time=0.111, loss_ctc=25.797, loss_att=11.129, acc=0.914, loss=15.529, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:09:12,684 (trainer:676) INFO: 28epoch:train:534-574batch: iter_time=1.201e-04, forward_time=0.109, loss_ctc=26.428, loss_att=11.213, acc=0.909, loss=15.778, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 01:09:28,002 (trainer:676) INFO: 28epoch:train:575-615batch: iter_time=1.181e-04, forward_time=0.110, loss_ctc=25.340, loss_att=10.689, acc=0.909, loss=15.084, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:09:43,224 (trainer:676) INFO: 28epoch:train:616-656batch: iter_time=1.299e-04, forward_time=0.110, loss_ctc=27.055, loss_att=11.619, acc=0.906, loss=16.250, backward_time=0.109, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 01:09:50,020 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:09:58,565 (trainer:676) INFO: 28epoch:train:657-697batch: iter_time=1.321e-04, forward_time=0.112, loss_ctc=24.862, loss_att=10.710, acc=0.900, loss=14.955, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:10:14,016 (trainer:676) INFO: 28epoch:train:698-738batch: iter_time=1.334e-04, forward_time=0.111, loss_ctc=26.504, loss_att=10.970, acc=0.909, loss=15.631, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 01:10:19,049 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:10:29,413 (trainer:676) INFO: 28epoch:train:739-779batch: iter_time=1.309e-04, forward_time=0.112, loss_ctc=27.059, loss_att=11.306, acc=0.899, loss=16.032, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:10:45,201 (trainer:676) INFO: 28epoch:train:780-820batch: iter_time=1.318e-04, forward_time=0.112, loss_ctc=25.562, loss_att=10.619, acc=0.911, loss=15.102, backward_time=0.116, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.385
[ip-10-0-0-95] 2022-04-28 01:11:24,225 (trainer:334) INFO: 28epoch results: [train] iter_time=3.794e-04, forward_time=0.110, loss_ctc=25.392, loss_att=10.748, acc=0.911, loss=15.141, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371, time=5 minutes and 5.62 seconds, total_count=23072, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.128, cer_ctc=0.102, loss_att=12.875, acc=0.911, cer=0.088, wer=0.871, loss=16.551, time=7.47 seconds, total_count=812, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.94 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 01:11:29,906 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 01:11:29,923 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/18epoch.pth
[ip-10-0-0-95] 2022-04-28 01:11:29,923 (trainer:268) INFO: 29/50epoch started. Estimated time to finish: 2 hours, 9 minutes and 25.27 seconds
[ip-10-0-0-95] 2022-04-28 01:11:32,625 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:11:45,342 (trainer:676) INFO: 29epoch:train:1-41batch: iter_time=0.005, forward_time=0.111, loss_ctc=23.351, loss_att=9.629, acc=0.909, loss=13.745, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:12:00,449 (trainer:676) INFO: 29epoch:train:42-82batch: iter_time=1.733e-04, forward_time=0.110, loss_ctc=23.080, loss_att=9.573, acc=0.920, loss=13.625, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 01:12:15,699 (trainer:676) INFO: 29epoch:train:83-123batch: iter_time=1.398e-04, forward_time=0.109, loss_ctc=23.938, loss_att=10.290, acc=0.916, loss=14.385, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:12:30,832 (trainer:676) INFO: 29epoch:train:124-164batch: iter_time=1.420e-04, forward_time=0.110, loss_ctc=23.752, loss_att=9.705, acc=0.917, loss=13.919, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 01:12:36,402 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:12:46,349 (trainer:676) INFO: 29epoch:train:165-205batch: iter_time=1.469e-04, forward_time=0.111, loss_ctc=24.092, loss_att=9.798, acc=0.915, loss=14.086, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:13:01,758 (trainer:676) INFO: 29epoch:train:206-246batch: iter_time=1.408e-04, forward_time=0.110, loss_ctc=26.761, loss_att=11.202, acc=0.909, loss=15.870, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:13:17,248 (trainer:676) INFO: 29epoch:train:247-287batch: iter_time=1.215e-04, forward_time=0.112, loss_ctc=23.994, loss_att=9.802, acc=0.919, loss=14.059, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:13:32,507 (trainer:676) INFO: 29epoch:train:288-328batch: iter_time=1.160e-04, forward_time=0.111, loss_ctc=24.268, loss_att=10.288, acc=0.917, loss=14.482, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:13:48,238 (trainer:676) INFO: 29epoch:train:329-369batch: iter_time=1.171e-04, forward_time=0.113, loss_ctc=23.612, loss_att=9.843, acc=0.909, loss=13.973, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 01:14:03,792 (trainer:676) INFO: 29epoch:train:370-410batch: iter_time=1.280e-04, forward_time=0.112, loss_ctc=22.970, loss_att=9.789, acc=0.920, loss=13.743, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:14:19,289 (trainer:676) INFO: 29epoch:train:411-451batch: iter_time=1.358e-04, forward_time=0.111, loss_ctc=25.452, loss_att=10.950, acc=0.915, loss=15.301, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:14:34,731 (trainer:676) INFO: 29epoch:train:452-492batch: iter_time=1.238e-04, forward_time=0.110, loss_ctc=25.546, loss_att=10.743, acc=0.913, loss=15.184, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:14:49,881 (trainer:676) INFO: 29epoch:train:493-533batch: iter_time=1.397e-04, forward_time=0.110, loss_ctc=25.348, loss_att=10.368, acc=0.914, loss=14.862, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 01:15:05,482 (trainer:676) INFO: 29epoch:train:534-574batch: iter_time=1.336e-04, forward_time=0.111, loss_ctc=25.212, loss_att=10.655, acc=0.913, loss=15.022, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 01:15:20,888 (trainer:676) INFO: 29epoch:train:575-615batch: iter_time=1.322e-04, forward_time=0.111, loss_ctc=25.626, loss_att=10.662, acc=0.913, loss=15.151, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:15:36,689 (trainer:676) INFO: 29epoch:train:616-656batch: iter_time=1.465e-04, forward_time=0.112, loss_ctc=25.650, loss_att=10.523, acc=0.916, loss=15.061, backward_time=0.116, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.385
[ip-10-0-0-95] 2022-04-28 01:15:52,079 (trainer:676) INFO: 29epoch:train:657-697batch: iter_time=1.511e-04, forward_time=0.111, loss_ctc=26.413, loss_att=10.820, acc=0.917, loss=15.498, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:16:07,251 (trainer:676) INFO: 29epoch:train:698-738batch: iter_time=1.392e-04, forward_time=0.111, loss_ctc=25.593, loss_att=10.596, acc=0.911, loss=15.095, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.370
[ip-10-0-0-95] 2022-04-28 01:16:22,500 (trainer:676) INFO: 29epoch:train:739-779batch: iter_time=1.239e-04, forward_time=0.110, loss_ctc=24.650, loss_att=10.120, acc=0.912, loss=14.479, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:16:38,048 (trainer:676) INFO: 29epoch:train:780-820batch: iter_time=1.471e-04, forward_time=0.111, loss_ctc=25.521, loss_att=10.653, acc=0.912, loss=15.114, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:17:16,988 (trainer:334) INFO: 29epoch results: [train] iter_time=3.847e-04, forward_time=0.111, loss_ctc=24.735, loss_att=10.294, acc=0.914, loss=14.627, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375, time=5 minutes and 9.65 seconds, total_count=23896, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=24.699, cer_ctc=0.100, loss_att=12.736, acc=0.913, cer=0.088, wer=0.861, loss=16.325, time=7.41 seconds, total_count=841, gpu_max_cached_mem_GB=7.422, [att_plot] time=30 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 01:17:22,495 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 01:17:22,513 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/19epoch.pth
[ip-10-0-0-95] 2022-04-28 01:17:22,513 (trainer:268) INFO: 30/50epoch started. Estimated time to finish: 2 hours, 3 minutes and 32.03 seconds
[ip-10-0-0-95] 2022-04-28 01:17:37,411 (trainer:676) INFO: 30epoch:train:1-41batch: iter_time=0.005, forward_time=0.109, loss_ctc=22.833, loss_att=9.285, acc=0.919, loss=13.349, backward_time=0.102, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.363
[ip-10-0-0-95] 2022-04-28 01:17:52,572 (trainer:676) INFO: 30epoch:train:42-82batch: iter_time=1.238e-04, forward_time=0.109, loss_ctc=23.936, loss_att=9.816, acc=0.922, loss=14.052, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.370
[ip-10-0-0-95] 2022-04-28 01:18:07,740 (trainer:676) INFO: 30epoch:train:83-123batch: iter_time=1.240e-04, forward_time=0.109, loss_ctc=24.028, loss_att=9.962, acc=0.916, loss=14.182, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.370
[ip-10-0-0-95] 2022-04-28 01:18:23,077 (trainer:676) INFO: 30epoch:train:124-164batch: iter_time=1.489e-04, forward_time=0.110, loss_ctc=24.182, loss_att=10.039, acc=0.921, loss=14.282, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:18:38,494 (trainer:676) INFO: 30epoch:train:165-205batch: iter_time=1.520e-04, forward_time=0.110, loss_ctc=24.620, loss_att=10.321, acc=0.917, loss=14.611, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:18:53,920 (trainer:676) INFO: 30epoch:train:206-246batch: iter_time=1.453e-04, forward_time=0.111, loss_ctc=23.216, loss_att=9.519, acc=0.921, loss=13.628, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:19:09,533 (trainer:676) INFO: 30epoch:train:247-287batch: iter_time=1.407e-04, forward_time=0.111, loss_ctc=24.098, loss_att=10.055, acc=0.919, loss=14.268, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.381
[ip-10-0-0-95] 2022-04-28 01:19:24,654 (trainer:676) INFO: 30epoch:train:288-328batch: iter_time=1.272e-04, forward_time=0.110, loss_ctc=23.163, loss_att=9.581, acc=0.919, loss=13.656, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.369
[ip-10-0-0-95] 2022-04-28 01:19:40,224 (trainer:676) INFO: 30epoch:train:329-369batch: iter_time=1.332e-04, forward_time=0.112, loss_ctc=24.966, loss_att=10.625, acc=0.915, loss=14.927, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:19:55,480 (trainer:676) INFO: 30epoch:train:370-410batch: iter_time=1.484e-04, forward_time=0.110, loss_ctc=25.055, loss_att=10.349, acc=0.916, loss=14.761, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:20:10,684 (trainer:676) INFO: 30epoch:train:411-451batch: iter_time=1.448e-04, forward_time=0.110, loss_ctc=24.799, loss_att=9.959, acc=0.919, loss=14.411, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.371
[ip-10-0-0-95] 2022-04-28 01:20:25,973 (trainer:676) INFO: 30epoch:train:452-492batch: iter_time=1.400e-04, forward_time=0.109, loss_ctc=24.356, loss_att=9.961, acc=0.914, loss=14.280, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:20:41,230 (trainer:676) INFO: 30epoch:train:493-533batch: iter_time=1.463e-04, forward_time=0.110, loss_ctc=24.079, loss_att=10.068, acc=0.918, loss=14.271, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:20:48,485 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:20:56,500 (trainer:676) INFO: 30epoch:train:534-574batch: iter_time=1.420e-04, forward_time=0.110, loss_ctc=23.328, loss_att=9.765, acc=0.914, loss=13.834, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:21:11,826 (trainer:676) INFO: 30epoch:train:575-615batch: iter_time=1.278e-04, forward_time=0.110, loss_ctc=23.793, loss_att=9.659, acc=0.920, loss=13.900, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:21:27,293 (trainer:676) INFO: 30epoch:train:616-656batch: iter_time=1.437e-04, forward_time=0.112, loss_ctc=23.931, loss_att=9.916, acc=0.917, loss=14.120, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.377
[ip-10-0-0-95] 2022-04-28 01:21:43,032 (trainer:676) INFO: 30epoch:train:657-697batch: iter_time=1.641e-04, forward_time=0.112, loss_ctc=23.988, loss_att=10.061, acc=0.915, loss=14.239, backward_time=0.116, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.384
[ip-10-0-0-95] 2022-04-28 01:21:46,952 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:21:58,580 (trainer:676) INFO: 30epoch:train:698-738batch: iter_time=1.311e-04, forward_time=0.112, loss_ctc=24.030, loss_att=9.990, acc=0.904, loss=14.202, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:22:14,183 (trainer:676) INFO: 30epoch:train:739-779batch: iter_time=1.562e-04, forward_time=0.111, loss_ctc=24.855, loss_att=10.438, acc=0.915, loss=14.763, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.380
[ip-10-0-0-95] 2022-04-28 01:22:29,704 (trainer:676) INFO: 30epoch:train:780-820batch: iter_time=1.462e-04, forward_time=0.112, loss_ctc=24.428, loss_att=10.107, acc=0.921, loss=14.403, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:23:08,595 (trainer:334) INFO: 30epoch results: [train] iter_time=3.793e-04, forward_time=0.111, loss_ctc=24.047, loss_att=9.950, acc=0.917, loss=14.179, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374, time=5 minutes and 8.69 seconds, total_count=24720, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=23.058, cer_ctc=0.095, loss_att=11.945, acc=0.918, cer=0.084, wer=0.879, loss=15.279, time=7.47 seconds, total_count=870, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.92 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 01:23:14,093 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 01:23:14,111 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/20epoch.pth
[ip-10-0-0-95] 2022-04-28 01:23:14,111 (trainer:268) INFO: 31/50epoch started. Estimated time to finish: 1 hour, 57 minutes and 38.17 seconds
[ip-10-0-0-95] 2022-04-28 01:23:29,443 (trainer:676) INFO: 31epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=22.731, loss_att=9.434, acc=0.926, loss=13.423, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:23:44,651 (trainer:676) INFO: 31epoch:train:42-82batch: iter_time=1.718e-04, forward_time=0.109, loss_ctc=24.091, loss_att=10.069, acc=0.922, loss=14.275, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.371
[ip-10-0-0-95] 2022-04-28 01:23:59,790 (trainer:676) INFO: 31epoch:train:83-123batch: iter_time=1.654e-04, forward_time=0.110, loss_ctc=23.191, loss_att=9.787, acc=0.922, loss=13.808, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.369
[ip-10-0-0-95] 2022-04-28 01:24:07,358 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:24:15,121 (trainer:676) INFO: 31epoch:train:124-164batch: iter_time=1.480e-04, forward_time=0.110, loss_ctc=23.041, loss_att=9.557, acc=0.912, loss=13.602, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:24:30,382 (trainer:676) INFO: 31epoch:train:165-205batch: iter_time=1.361e-04, forward_time=0.112, loss_ctc=22.827, loss_att=9.411, acc=0.920, loss=13.436, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:24:45,798 (trainer:676) INFO: 31epoch:train:206-246batch: iter_time=1.373e-04, forward_time=0.112, loss_ctc=22.202, loss_att=9.159, acc=0.920, loss=13.072, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:25:01,603 (trainer:676) INFO: 31epoch:train:247-287batch: iter_time=1.527e-04, forward_time=0.111, loss_ctc=23.474, loss_att=9.790, acc=0.921, loss=13.895, backward_time=0.117, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.385
[ip-10-0-0-95] 2022-04-28 01:25:17,104 (trainer:676) INFO: 31epoch:train:288-328batch: iter_time=1.480e-04, forward_time=0.111, loss_ctc=23.863, loss_att=9.656, acc=0.919, loss=13.918, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:25:20,054 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 01:25:32,549 (trainer:676) INFO: 31epoch:train:329-369batch: iter_time=1.570e-04, forward_time=0.113, loss_ctc=23.464, loss_att=9.914, acc=0.922, loss=13.979, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:25:48,050 (trainer:676) INFO: 31epoch:train:370-410batch: iter_time=1.517e-04, forward_time=0.111, loss_ctc=25.087, loss_att=10.223, acc=0.913, loss=14.683, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:26:03,708 (trainer:676) INFO: 31epoch:train:411-451batch: iter_time=1.535e-04, forward_time=0.111, loss_ctc=24.349, loss_att=9.804, acc=0.917, loss=14.167, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.382
[ip-10-0-0-95] 2022-04-28 01:26:19,023 (trainer:676) INFO: 31epoch:train:452-492batch: iter_time=1.671e-04, forward_time=0.110, loss_ctc=23.742, loss_att=9.698, acc=0.918, loss=13.911, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:26:34,765 (trainer:676) INFO: 31epoch:train:493-533batch: iter_time=1.422e-04, forward_time=0.111, loss_ctc=23.194, loss_att=9.749, acc=0.924, loss=13.782, backward_time=0.116, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.384
[ip-10-0-0-95] 2022-04-28 01:26:50,221 (trainer:676) INFO: 31epoch:train:534-574batch: iter_time=1.267e-04, forward_time=0.112, loss_ctc=23.731, loss_att=9.706, acc=0.921, loss=13.913, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.377
[ip-10-0-0-95] 2022-04-28 01:27:05,544 (trainer:676) INFO: 31epoch:train:575-615batch: iter_time=1.369e-04, forward_time=0.111, loss_ctc=24.253, loss_att=9.921, acc=0.922, loss=14.220, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:27:21,134 (trainer:676) INFO: 31epoch:train:616-656batch: iter_time=1.274e-04, forward_time=0.111, loss_ctc=23.600, loss_att=9.537, acc=0.922, loss=13.756, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.380
[ip-10-0-0-95] 2022-04-28 01:27:25,721 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:27:36,487 (trainer:676) INFO: 31epoch:train:657-697batch: iter_time=1.398e-04, forward_time=0.111, loss_ctc=24.082, loss_att=9.807, acc=0.917, loss=14.089, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:27:51,855 (trainer:676) INFO: 31epoch:train:698-738batch: iter_time=1.707e-04, forward_time=0.111, loss_ctc=23.637, loss_att=9.538, acc=0.923, loss=13.768, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:28:07,389 (trainer:676) INFO: 31epoch:train:739-779batch: iter_time=1.438e-04, forward_time=0.112, loss_ctc=22.574, loss_att=9.169, acc=0.918, loss=13.190, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:28:22,890 (trainer:676) INFO: 31epoch:train:780-820batch: iter_time=1.280e-04, forward_time=0.111, loss_ctc=24.196, loss_att=9.697, acc=0.916, loss=14.047, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:29:01,744 (trainer:334) INFO: 31epoch results: [train] iter_time=4.022e-04, forward_time=0.111, loss_ctc=23.571, loss_att=9.680, acc=0.920, loss=13.847, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.376, time=5 minutes and 10.34 seconds, total_count=25544, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=23.400, cer_ctc=0.093, loss_att=12.088, acc=0.919, cer=0.082, wer=0.850, loss=15.481, time=7.47 seconds, total_count=899, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.82 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 01:29:07,425 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 01:29:07,443 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/24epoch.pth
[ip-10-0-0-95] 2022-04-28 01:29:07,443 (trainer:268) INFO: 32/50epoch started. Estimated time to finish: 1 hour, 51 minutes and 45.52 seconds
[ip-10-0-0-95] 2022-04-28 01:29:22,318 (trainer:676) INFO: 32epoch:train:1-41batch: iter_time=0.005, forward_time=0.108, loss_ctc=22.293, loss_att=8.797, acc=0.926, loss=12.846, backward_time=0.102, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.362
[ip-10-0-0-95] 2022-04-28 01:29:26,547 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:29:37,600 (trainer:676) INFO: 32epoch:train:42-82batch: iter_time=1.706e-04, forward_time=0.110, loss_ctc=21.894, loss_att=8.896, acc=0.917, loss=12.796, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:29:53,025 (trainer:676) INFO: 32epoch:train:83-123batch: iter_time=1.373e-04, forward_time=0.113, loss_ctc=22.288, loss_att=9.063, acc=0.923, loss=13.030, backward_time=0.112, optim_step_time=0.040, optim0_lr0=0.002, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:30:08,302 (trainer:676) INFO: 32epoch:train:124-164batch: iter_time=1.353e-04, forward_time=0.110, loss_ctc=20.577, loss_att=8.479, acc=0.931, loss=12.108, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:30:23,626 (trainer:676) INFO: 32epoch:train:165-205batch: iter_time=1.261e-04, forward_time=0.110, loss_ctc=23.048, loss_att=9.576, acc=0.922, loss=13.618, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:30:39,176 (trainer:676) INFO: 32epoch:train:206-246batch: iter_time=1.341e-04, forward_time=0.111, loss_ctc=21.723, loss_att=8.871, acc=0.926, loss=12.727, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:30:54,631 (trainer:676) INFO: 32epoch:train:247-287batch: iter_time=1.205e-04, forward_time=0.110, loss_ctc=22.573, loss_att=9.457, acc=0.924, loss=13.392, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.377
[ip-10-0-0-95] 2022-04-28 01:31:10,225 (trainer:676) INFO: 32epoch:train:288-328batch: iter_time=1.166e-04, forward_time=0.111, loss_ctc=22.275, loss_att=8.807, acc=0.922, loss=12.847, backward_time=0.113, optim_step_time=0.040, optim0_lr0=0.002, train_time=0.380
[ip-10-0-0-95] 2022-04-28 01:31:25,647 (trainer:676) INFO: 32epoch:train:329-369batch: iter_time=1.609e-04, forward_time=0.112, loss_ctc=22.112, loss_att=9.164, acc=0.924, loss=13.048, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:31:41,164 (trainer:676) INFO: 32epoch:train:370-410batch: iter_time=1.541e-04, forward_time=0.111, loss_ctc=22.055, loss_att=8.879, acc=0.927, loss=12.832, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:31:56,230 (trainer:676) INFO: 32epoch:train:411-451batch: iter_time=1.528e-04, forward_time=0.110, loss_ctc=23.197, loss_att=9.457, acc=0.922, loss=13.579, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.367
[ip-10-0-0-95] 2022-04-28 01:32:11,503 (trainer:676) INFO: 32epoch:train:452-492batch: iter_time=1.555e-04, forward_time=0.110, loss_ctc=22.208, loss_att=8.759, acc=0.924, loss=12.793, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:32:26,929 (trainer:676) INFO: 32epoch:train:493-533batch: iter_time=1.601e-04, forward_time=0.111, loss_ctc=21.665, loss_att=8.770, acc=0.929, loss=12.639, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:32:42,166 (trainer:676) INFO: 32epoch:train:534-574batch: iter_time=1.579e-04, forward_time=0.110, loss_ctc=22.269, loss_att=8.827, acc=0.925, loss=12.860, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.371
[ip-10-0-0-95] 2022-04-28 01:32:57,404 (trainer:676) INFO: 32epoch:train:575-615batch: iter_time=1.571e-04, forward_time=0.110, loss_ctc=22.495, loss_att=8.875, acc=0.925, loss=12.961, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.371
[ip-10-0-0-95] 2022-04-28 01:33:12,751 (trainer:676) INFO: 32epoch:train:616-656batch: iter_time=1.605e-04, forward_time=0.110, loss_ctc=22.426, loss_att=9.251, acc=0.926, loss=13.203, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:33:28,377 (trainer:676) INFO: 32epoch:train:657-697batch: iter_time=1.532e-04, forward_time=0.111, loss_ctc=21.691, loss_att=8.755, acc=0.930, loss=12.636, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.381
[ip-10-0-0-95] 2022-04-28 01:33:31,195 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:33:43,764 (trainer:676) INFO: 32epoch:train:698-738batch: iter_time=1.594e-04, forward_time=0.110, loss_ctc=23.457, loss_att=9.613, acc=0.917, loss=13.766, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:33:58,948 (trainer:676) INFO: 32epoch:train:739-779batch: iter_time=1.579e-04, forward_time=0.111, loss_ctc=21.838, loss_att=9.026, acc=0.933, loss=12.870, backward_time=0.110, optim_step_time=0.040, optim0_lr0=0.002, train_time=0.370
[ip-10-0-0-95] 2022-04-28 01:34:14,426 (trainer:676) INFO: 32epoch:train:780-820batch: iter_time=1.495e-04, forward_time=0.111, loss_ctc=22.340, loss_att=8.881, acc=0.928, loss=12.919, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.377
[ip-10-0-0-95] 2022-04-28 01:34:53,262 (trainer:334) INFO: 32epoch results: [train] iter_time=3.904e-04, forward_time=0.111, loss_ctc=22.231, loss_att=9.012, acc=0.925, loss=12.978, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374, time=5 minutes and 8.55 seconds, total_count=26368, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=24.284, cer_ctc=0.093, loss_att=12.757, acc=0.916, cer=0.083, wer=0.845, loss=16.215, time=7.38 seconds, total_count=928, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.89 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 01:34:58,791 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 01:34:58,808 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/21epoch.pth
[ip-10-0-0-95] 2022-04-28 01:34:58,808 (trainer:268) INFO: 33/50epoch started. Estimated time to finish: 1 hour, 45 minutes and 51.72 seconds
[ip-10-0-0-95] 2022-04-28 01:35:13,998 (trainer:676) INFO: 33epoch:train:1-41batch: iter_time=0.006, forward_time=0.110, loss_ctc=20.947, loss_att=8.393, acc=0.927, loss=12.159, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.370
[ip-10-0-0-95] 2022-04-28 01:35:29,296 (trainer:676) INFO: 33epoch:train:42-82batch: iter_time=1.223e-04, forward_time=0.110, loss_ctc=20.844, loss_att=8.352, acc=0.927, loss=12.100, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:35:44,655 (trainer:676) INFO: 33epoch:train:83-123batch: iter_time=1.493e-04, forward_time=0.111, loss_ctc=21.248, loss_att=8.434, acc=0.929, loss=12.278, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:35:58,834 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:35:59,872 (trainer:676) INFO: 33epoch:train:124-164batch: iter_time=1.587e-04, forward_time=0.111, loss_ctc=21.620, loss_att=8.961, acc=0.921, loss=12.759, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.371
[ip-10-0-0-95] 2022-04-28 01:36:15,571 (trainer:676) INFO: 33epoch:train:165-205batch: iter_time=1.476e-04, forward_time=0.111, loss_ctc=21.064, loss_att=8.577, acc=0.932, loss=12.323, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.383
[ip-10-0-0-95] 2022-04-28 01:36:30,774 (trainer:676) INFO: 33epoch:train:206-246batch: iter_time=1.447e-04, forward_time=0.110, loss_ctc=21.359, loss_att=8.501, acc=0.930, loss=12.358, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.371
[ip-10-0-0-95] 2022-04-28 01:36:46,411 (trainer:676) INFO: 33epoch:train:247-287batch: iter_time=1.249e-04, forward_time=0.111, loss_ctc=20.745, loss_att=8.259, acc=0.932, loss=12.005, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.381
[ip-10-0-0-95] 2022-04-28 01:37:01,405 (trainer:676) INFO: 33epoch:train:288-328batch: iter_time=1.336e-04, forward_time=0.110, loss_ctc=21.781, loss_att=8.766, acc=0.923, loss=12.671, backward_time=0.106, optim_step_time=0.040, optim0_lr0=0.002, train_time=0.365
[ip-10-0-0-95] 2022-04-28 01:37:17,102 (trainer:676) INFO: 33epoch:train:329-369batch: iter_time=1.293e-04, forward_time=0.111, loss_ctc=21.130, loss_att=8.496, acc=0.934, loss=12.287, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.383
[ip-10-0-0-95] 2022-04-28 01:37:28,654 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:37:32,782 (trainer:676) INFO: 33epoch:train:370-410batch: iter_time=1.383e-04, forward_time=0.111, loss_ctc=21.612, loss_att=8.799, acc=0.924, loss=12.643, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.382
[ip-10-0-0-95] 2022-04-28 01:37:48,401 (trainer:676) INFO: 33epoch:train:411-451batch: iter_time=1.480e-04, forward_time=0.112, loss_ctc=20.685, loss_att=8.171, acc=0.925, loss=11.925, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.381
[ip-10-0-0-95] 2022-04-28 01:38:03,596 (trainer:676) INFO: 33epoch:train:452-492batch: iter_time=1.259e-04, forward_time=0.110, loss_ctc=21.127, loss_att=8.154, acc=0.929, loss=12.046, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.370
[ip-10-0-0-95] 2022-04-28 01:38:18,549 (trainer:676) INFO: 33epoch:train:493-533batch: iter_time=1.166e-04, forward_time=0.110, loss_ctc=21.919, loss_att=8.781, acc=0.929, loss=12.722, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.364
[ip-10-0-0-95] 2022-04-28 01:38:33,838 (trainer:676) INFO: 33epoch:train:534-574batch: iter_time=1.245e-04, forward_time=0.110, loss_ctc=21.985, loss_att=8.812, acc=0.927, loss=12.764, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:38:49,233 (trainer:676) INFO: 33epoch:train:575-615batch: iter_time=1.227e-04, forward_time=0.111, loss_ctc=20.382, loss_att=8.236, acc=0.929, loss=11.880, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:39:04,715 (trainer:676) INFO: 33epoch:train:616-656batch: iter_time=1.237e-04, forward_time=0.110, loss_ctc=21.544, loss_att=8.629, acc=0.929, loss=12.504, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.377
[ip-10-0-0-95] 2022-04-28 01:39:20,207 (trainer:676) INFO: 33epoch:train:657-697batch: iter_time=1.227e-04, forward_time=0.111, loss_ctc=21.299, loss_att=8.374, acc=0.930, loss=12.251, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:39:35,530 (trainer:676) INFO: 33epoch:train:698-738batch: iter_time=1.431e-04, forward_time=0.112, loss_ctc=20.774, loss_att=8.667, acc=0.934, loss=12.299, backward_time=0.112, optim_step_time=0.040, optim0_lr0=0.002, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:39:51,070 (trainer:676) INFO: 33epoch:train:739-779batch: iter_time=1.258e-04, forward_time=0.112, loss_ctc=21.610, loss_att=8.842, acc=0.931, loss=12.673, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:40:06,423 (trainer:676) INFO: 33epoch:train:780-820batch: iter_time=1.371e-04, forward_time=0.110, loss_ctc=21.678, loss_att=8.715, acc=0.929, loss=12.604, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:40:45,230 (trainer:334) INFO: 33epoch results: [train] iter_time=4.012e-04, forward_time=0.111, loss_ctc=21.278, loss_att=8.548, acc=0.928, loss=12.367, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.375, time=5 minutes and 9.23 seconds, total_count=27192, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=24.621, cer_ctc=0.094, loss_att=12.206, acc=0.919, cer=0.081, wer=0.861, loss=15.931, time=7.38 seconds, total_count=957, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.8 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 01:40:50,893 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 01:40:50,910 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/27epoch.pth
[ip-10-0-0-95] 2022-04-28 01:40:50,911 (trainer:268) INFO: 34/50epoch started. Estimated time to finish: 1 hour, 39 minutes and 58.45 seconds
[ip-10-0-0-95] 2022-04-28 01:41:06,239 (trainer:676) INFO: 34epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=20.664, loss_att=8.392, acc=0.934, loss=12.074, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:41:21,586 (trainer:676) INFO: 34epoch:train:42-82batch: iter_time=1.914e-04, forward_time=0.110, loss_ctc=18.997, loss_att=7.306, acc=0.936, loss=10.813, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:41:37,257 (trainer:676) INFO: 34epoch:train:83-123batch: iter_time=1.500e-04, forward_time=0.112, loss_ctc=20.673, loss_att=8.085, acc=0.935, loss=11.861, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.382
[ip-10-0-0-95] 2022-04-28 01:41:52,604 (trainer:676) INFO: 34epoch:train:124-164batch: iter_time=1.637e-04, forward_time=0.110, loss_ctc=20.927, loss_att=8.250, acc=0.933, loss=12.053, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:42:07,980 (trainer:676) INFO: 34epoch:train:165-205batch: iter_time=1.497e-04, forward_time=0.111, loss_ctc=20.601, loss_att=8.140, acc=0.935, loss=11.878, backward_time=0.111, optim_step_time=0.040, optim0_lr0=0.002, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:42:16,846 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 01:42:23,641 (trainer:676) INFO: 34epoch:train:206-246batch: iter_time=1.664e-04, forward_time=0.111, loss_ctc=20.532, loss_att=8.166, acc=0.936, loss=11.876, backward_time=0.116, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.382
[ip-10-0-0-95] 2022-04-28 01:42:38,814 (trainer:676) INFO: 34epoch:train:247-287batch: iter_time=1.709e-04, forward_time=0.111, loss_ctc=20.137, loss_att=7.928, acc=0.934, loss=11.590, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.370
[ip-10-0-0-95] 2022-04-28 01:42:53,988 (trainer:676) INFO: 34epoch:train:288-328batch: iter_time=1.554e-04, forward_time=0.110, loss_ctc=19.871, loss_att=7.734, acc=0.932, loss=11.375, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.370
[ip-10-0-0-95] 2022-04-28 01:43:09,539 (trainer:676) INFO: 34epoch:train:329-369batch: iter_time=1.605e-04, forward_time=0.111, loss_ctc=20.184, loss_att=8.007, acc=0.935, loss=11.660, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:43:24,859 (trainer:676) INFO: 34epoch:train:370-410batch: iter_time=1.549e-04, forward_time=0.110, loss_ctc=21.154, loss_att=8.279, acc=0.935, loss=12.142, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:43:40,301 (trainer:676) INFO: 34epoch:train:411-451batch: iter_time=1.550e-04, forward_time=0.111, loss_ctc=19.955, loss_att=7.809, acc=0.935, loss=11.453, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:43:55,742 (trainer:676) INFO: 34epoch:train:452-492batch: iter_time=1.636e-04, forward_time=0.112, loss_ctc=20.291, loss_att=8.084, acc=0.934, loss=11.746, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:44:11,363 (trainer:676) INFO: 34epoch:train:493-533batch: iter_time=1.639e-04, forward_time=0.112, loss_ctc=20.718, loss_att=8.227, acc=0.932, loss=11.974, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.381
[ip-10-0-0-95] 2022-04-28 01:44:26,800 (trainer:676) INFO: 34epoch:train:534-574batch: iter_time=1.626e-04, forward_time=0.110, loss_ctc=20.325, loss_att=7.861, acc=0.932, loss=11.600, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:44:42,128 (trainer:676) INFO: 34epoch:train:575-615batch: iter_time=1.528e-04, forward_time=0.111, loss_ctc=21.883, loss_att=8.545, acc=0.929, loss=12.546, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:44:54,963 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:44:57,575 (trainer:676) INFO: 34epoch:train:616-656batch: iter_time=1.473e-04, forward_time=0.111, loss_ctc=19.242, loss_att=7.527, acc=0.932, loss=11.041, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:45:03,499 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:45:13,175 (trainer:676) INFO: 34epoch:train:657-697batch: iter_time=1.473e-04, forward_time=0.111, loss_ctc=19.996, loss_att=8.124, acc=0.924, loss=11.685, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 01:45:28,449 (trainer:676) INFO: 34epoch:train:698-738batch: iter_time=1.582e-04, forward_time=0.110, loss_ctc=19.663, loss_att=7.839, acc=0.933, loss=11.386, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:45:43,461 (trainer:676) INFO: 34epoch:train:739-779batch: iter_time=1.540e-04, forward_time=0.109, loss_ctc=21.426, loss_att=8.633, acc=0.931, loss=12.471, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.366
[ip-10-0-0-95] 2022-04-28 01:45:58,564 (trainer:676) INFO: 34epoch:train:780-820batch: iter_time=1.546e-04, forward_time=0.111, loss_ctc=19.813, loss_att=7.878, acc=0.934, loss=11.459, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 01:46:37,404 (trainer:334) INFO: 34epoch results: [train] iter_time=4.022e-04, forward_time=0.111, loss_ctc=20.343, loss_att=8.031, acc=0.933, loss=11.725, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.002, train_time=0.375, time=5 minutes and 9.24 seconds, total_count=28016, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=22.839, cer_ctc=0.087, loss_att=11.556, acc=0.924, cer=0.074, wer=0.837, loss=14.941, time=7.4 seconds, total_count=986, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.85 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 01:46:42,939 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 01:46:42,957 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/28epoch.pth
[ip-10-0-0-95] 2022-04-28 01:46:42,957 (trainer:268) INFO: 35/50epoch started. Estimated time to finish: 1 hour, 34 minutes and 5.22 seconds
[ip-10-0-0-95] 2022-04-28 01:46:58,067 (trainer:676) INFO: 35epoch:train:1-41batch: iter_time=0.006, forward_time=0.109, loss_ctc=18.980, loss_att=7.323, acc=0.940, loss=10.820, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 01:47:13,328 (trainer:676) INFO: 35epoch:train:42-82batch: iter_time=1.767e-04, forward_time=0.109, loss_ctc=18.622, loss_att=7.073, acc=0.941, loss=10.538, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:47:28,378 (trainer:676) INFO: 35epoch:train:83-123batch: iter_time=1.566e-04, forward_time=0.110, loss_ctc=18.542, loss_att=6.952, acc=0.940, loss=10.429, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 01:47:43,492 (trainer:676) INFO: 35epoch:train:124-164batch: iter_time=1.410e-04, forward_time=0.112, loss_ctc=18.837, loss_att=7.481, acc=0.937, loss=10.888, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 01:47:58,500 (trainer:676) INFO: 35epoch:train:165-205batch: iter_time=1.262e-04, forward_time=0.108, loss_ctc=18.469, loss_att=7.057, acc=0.939, loss=10.481, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.366
[ip-10-0-0-95] 2022-04-28 01:48:13,585 (trainer:676) INFO: 35epoch:train:206-246batch: iter_time=1.504e-04, forward_time=0.109, loss_ctc=19.912, loss_att=7.760, acc=0.937, loss=11.406, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 01:48:28,575 (trainer:676) INFO: 35epoch:train:247-287batch: iter_time=1.569e-04, forward_time=0.108, loss_ctc=19.847, loss_att=7.694, acc=0.939, loss=11.340, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 01:48:44,056 (trainer:676) INFO: 35epoch:train:288-328batch: iter_time=1.379e-04, forward_time=0.111, loss_ctc=18.640, loss_att=7.180, acc=0.941, loss=10.618, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 01:48:59,372 (trainer:676) INFO: 35epoch:train:329-369batch: iter_time=1.373e-04, forward_time=0.110, loss_ctc=18.801, loss_att=7.578, acc=0.941, loss=10.945, backward_time=0.113, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:49:14,690 (trainer:676) INFO: 35epoch:train:370-410batch: iter_time=1.488e-04, forward_time=0.110, loss_ctc=19.443, loss_att=7.685, acc=0.934, loss=11.213, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:49:26,489 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:49:30,174 (trainer:676) INFO: 35epoch:train:411-451batch: iter_time=1.191e-04, forward_time=0.111, loss_ctc=20.012, loss_att=7.822, acc=0.931, loss=11.479, backward_time=0.112, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 01:49:45,366 (trainer:676) INFO: 35epoch:train:452-492batch: iter_time=1.191e-04, forward_time=0.110, loss_ctc=19.057, loss_att=7.403, acc=0.939, loss=10.899, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 01:50:00,507 (trainer:676) INFO: 35epoch:train:493-533batch: iter_time=1.172e-04, forward_time=0.110, loss_ctc=19.168, loss_att=7.351, acc=0.936, loss=10.896, backward_time=0.107, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 01:50:16,052 (trainer:676) INFO: 35epoch:train:534-574batch: iter_time=1.164e-04, forward_time=0.112, loss_ctc=18.873, loss_att=7.325, acc=0.941, loss=10.790, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:50:31,597 (trainer:676) INFO: 35epoch:train:575-615batch: iter_time=1.179e-04, forward_time=0.111, loss_ctc=19.303, loss_att=7.608, acc=0.937, loss=11.117, backward_time=0.113, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:50:47,152 (trainer:676) INFO: 35epoch:train:616-656batch: iter_time=1.183e-04, forward_time=0.111, loss_ctc=19.589, loss_att=7.805, acc=0.938, loss=11.340, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:51:02,835 (trainer:676) INFO: 35epoch:train:657-697batch: iter_time=1.131e-04, forward_time=0.111, loss_ctc=20.878, loss_att=8.043, acc=0.935, loss=11.894, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 01:51:09,715 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:51:18,472 (trainer:676) INFO: 35epoch:train:698-738batch: iter_time=1.149e-04, forward_time=0.111, loss_ctc=19.472, loss_att=8.088, acc=0.929, loss=11.503, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 01:51:34,027 (trainer:676) INFO: 35epoch:train:739-779batch: iter_time=1.190e-04, forward_time=0.112, loss_ctc=18.487, loss_att=7.094, acc=0.937, loss=10.512, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:51:49,366 (trainer:676) INFO: 35epoch:train:780-820batch: iter_time=1.132e-04, forward_time=0.111, loss_ctc=19.669, loss_att=8.009, acc=0.934, loss=11.507, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:52:28,206 (trainer:334) INFO: 35epoch results: [train] iter_time=4.038e-04, forward_time=0.110, loss_ctc=19.221, loss_att=7.511, acc=0.937, loss=11.024, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373, time=5 minutes and 8.01 seconds, total_count=28840, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=24.448, cer_ctc=0.091, loss_att=12.109, acc=0.922, cer=0.080, wer=0.866, loss=15.811, time=7.43 seconds, total_count=1015, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.81 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 01:52:33,898 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 01:52:33,915 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/23epoch.pth
[ip-10-0-0-95] 2022-04-28 01:52:33,916 (trainer:268) INFO: 36/50epoch started. Estimated time to finish: 1 hour, 28 minutes and 11.6 seconds
[ip-10-0-0-95] 2022-04-28 01:52:49,297 (trainer:676) INFO: 36epoch:train:1-41batch: iter_time=0.005, forward_time=0.113, loss_ctc=16.885, loss_att=6.639, acc=0.944, loss=9.713, backward_time=0.109, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:53:04,323 (trainer:676) INFO: 36epoch:train:42-82batch: iter_time=1.735e-04, forward_time=0.112, loss_ctc=18.726, loss_att=7.253, acc=0.938, loss=10.695, backward_time=0.104, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.366
[ip-10-0-0-95] 2022-04-28 01:53:19,805 (trainer:676) INFO: 36epoch:train:83-123batch: iter_time=1.581e-04, forward_time=0.114, loss_ctc=18.543, loss_att=7.208, acc=0.943, loss=10.608, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 01:53:35,362 (trainer:676) INFO: 36epoch:train:124-164batch: iter_time=1.594e-04, forward_time=0.111, loss_ctc=19.540, loss_att=7.498, acc=0.941, loss=11.111, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:53:37,328 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:53:50,479 (trainer:676) INFO: 36epoch:train:165-205batch: iter_time=1.594e-04, forward_time=0.110, loss_ctc=17.940, loss_att=6.806, acc=0.938, loss=10.146, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 01:54:06,152 (trainer:676) INFO: 36epoch:train:206-246batch: iter_time=1.602e-04, forward_time=0.112, loss_ctc=18.762, loss_att=7.180, acc=0.943, loss=10.655, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 01:54:21,475 (trainer:676) INFO: 36epoch:train:247-287batch: iter_time=1.566e-04, forward_time=0.111, loss_ctc=19.329, loss_att=7.393, acc=0.936, loss=10.974, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:54:37,037 (trainer:676) INFO: 36epoch:train:288-328batch: iter_time=1.380e-04, forward_time=0.111, loss_ctc=18.873, loss_att=7.382, acc=0.941, loss=10.830, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:54:52,300 (trainer:676) INFO: 36epoch:train:329-369batch: iter_time=1.308e-04, forward_time=0.112, loss_ctc=18.652, loss_att=7.343, acc=0.939, loss=10.736, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 01:55:07,843 (trainer:676) INFO: 36epoch:train:370-410batch: iter_time=1.152e-04, forward_time=0.112, loss_ctc=18.817, loss_att=7.305, acc=0.942, loss=10.759, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:55:23,214 (trainer:676) INFO: 36epoch:train:411-451batch: iter_time=1.233e-04, forward_time=0.111, loss_ctc=19.195, loss_att=7.638, acc=0.939, loss=11.105, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 01:55:38,847 (trainer:676) INFO: 36epoch:train:452-492batch: iter_time=1.283e-04, forward_time=0.111, loss_ctc=18.613, loss_att=7.459, acc=0.941, loss=10.806, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 01:55:54,290 (trainer:676) INFO: 36epoch:train:493-533batch: iter_time=1.263e-04, forward_time=0.110, loss_ctc=17.985, loss_att=7.018, acc=0.942, loss=10.308, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:56:09,631 (trainer:676) INFO: 36epoch:train:534-574batch: iter_time=1.300e-04, forward_time=0.111, loss_ctc=18.028, loss_att=7.091, acc=0.942, loss=10.372, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 01:56:25,204 (trainer:676) INFO: 36epoch:train:575-615batch: iter_time=1.387e-04, forward_time=0.112, loss_ctc=17.468, loss_att=6.784, acc=0.942, loss=9.989, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 01:56:40,722 (trainer:676) INFO: 36epoch:train:616-656batch: iter_time=1.439e-04, forward_time=0.111, loss_ctc=18.242, loss_att=7.055, acc=0.941, loss=10.411, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:56:56,166 (trainer:676) INFO: 36epoch:train:657-697batch: iter_time=1.379e-04, forward_time=0.111, loss_ctc=19.372, loss_att=7.471, acc=0.939, loss=11.041, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 01:57:11,686 (trainer:676) INFO: 36epoch:train:698-738batch: iter_time=1.277e-04, forward_time=0.111, loss_ctc=19.122, loss_att=7.205, acc=0.940, loss=10.780, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 01:57:19,452 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:57:27,312 (trainer:676) INFO: 36epoch:train:739-779batch: iter_time=1.246e-04, forward_time=0.112, loss_ctc=18.901, loss_att=7.235, acc=0.931, loss=10.735, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 01:57:42,857 (trainer:676) INFO: 36epoch:train:780-820batch: iter_time=1.331e-04, forward_time=0.112, loss_ctc=18.836, loss_att=7.202, acc=0.939, loss=10.692, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 01:58:21,876 (trainer:334) INFO: 36epoch results: [train] iter_time=3.835e-04, forward_time=0.112, loss_ctc=18.583, loss_att=7.201, acc=0.940, loss=10.616, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377, time=5 minutes and 10.56 seconds, total_count=29664, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=23.663, cer_ctc=0.087, loss_att=11.841, acc=0.922, cer=0.077, wer=0.845, loss=15.388, time=7.45 seconds, total_count=1044, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.95 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 01:58:27,439 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 01:58:27,457 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/22epoch.pth
[ip-10-0-0-95] 2022-04-28 01:58:27,457 (trainer:268) INFO: 37/50epoch started. Estimated time to finish: 1 hour, 22 minutes and 19.12 seconds
[ip-10-0-0-95] 2022-04-28 01:58:43,112 (trainer:676) INFO: 37epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=16.854, loss_att=6.490, acc=0.948, loss=9.599, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 01:58:58,298 (trainer:676) INFO: 37epoch:train:42-82batch: iter_time=1.291e-04, forward_time=0.109, loss_ctc=17.031, loss_att=6.462, acc=0.945, loss=9.633, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 01:59:13,479 (trainer:676) INFO: 37epoch:train:83-123batch: iter_time=1.280e-04, forward_time=0.109, loss_ctc=17.094, loss_att=6.768, acc=0.944, loss=9.866, backward_time=0.110, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 01:59:18,629 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 01:59:28,449 (trainer:676) INFO: 37epoch:train:124-164batch: iter_time=1.221e-04, forward_time=0.109, loss_ctc=18.305, loss_att=7.042, acc=0.936, loss=10.421, backward_time=0.106, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 01:59:43,769 (trainer:676) INFO: 37epoch:train:165-205batch: iter_time=1.408e-04, forward_time=0.111, loss_ctc=17.676, loss_att=6.973, acc=0.942, loss=10.184, backward_time=0.110, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 01:59:59,529 (trainer:676) INFO: 37epoch:train:206-246batch: iter_time=1.491e-04, forward_time=0.113, loss_ctc=16.699, loss_att=6.178, acc=0.944, loss=9.334, backward_time=0.116, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 02:00:14,814 (trainer:676) INFO: 37epoch:train:247-287batch: iter_time=1.324e-04, forward_time=0.112, loss_ctc=17.932, loss_att=6.813, acc=0.943, loss=10.149, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:00:30,234 (trainer:676) INFO: 37epoch:train:288-328batch: iter_time=1.604e-04, forward_time=0.112, loss_ctc=17.031, loss_att=6.474, acc=0.944, loss=9.641, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:00:45,504 (trainer:676) INFO: 37epoch:train:329-369batch: iter_time=1.339e-04, forward_time=0.110, loss_ctc=17.576, loss_att=6.727, acc=0.942, loss=9.982, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:01:01,109 (trainer:676) INFO: 37epoch:train:370-410batch: iter_time=1.545e-04, forward_time=0.112, loss_ctc=17.687, loss_att=7.072, acc=0.945, loss=10.257, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 02:01:16,502 (trainer:676) INFO: 37epoch:train:411-451batch: iter_time=1.638e-04, forward_time=0.111, loss_ctc=18.209, loss_att=7.127, acc=0.941, loss=10.451, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 02:01:32,008 (trainer:676) INFO: 37epoch:train:452-492batch: iter_time=1.609e-04, forward_time=0.111, loss_ctc=17.544, loss_att=6.463, acc=0.944, loss=9.787, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:01:47,377 (trainer:676) INFO: 37epoch:train:493-533batch: iter_time=1.517e-04, forward_time=0.113, loss_ctc=17.719, loss_att=6.686, acc=0.943, loss=9.996, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 02:02:02,732 (trainer:676) INFO: 37epoch:train:534-574batch: iter_time=1.614e-04, forward_time=0.111, loss_ctc=17.337, loss_att=6.845, acc=0.947, loss=9.993, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:02:18,150 (trainer:676) INFO: 37epoch:train:575-615batch: iter_time=1.708e-04, forward_time=0.111, loss_ctc=18.914, loss_att=7.120, acc=0.943, loss=10.658, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:02:33,912 (trainer:676) INFO: 37epoch:train:616-656batch: iter_time=1.355e-04, forward_time=0.114, loss_ctc=17.899, loss_att=6.763, acc=0.943, loss=10.104, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 02:02:42,854 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:02:49,573 (trainer:676) INFO: 37epoch:train:657-697batch: iter_time=1.390e-04, forward_time=0.111, loss_ctc=18.753, loss_att=7.254, acc=0.937, loss=10.704, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 02:03:05,233 (trainer:676) INFO: 37epoch:train:698-738batch: iter_time=1.296e-04, forward_time=0.112, loss_ctc=17.980, loss_att=7.018, acc=0.945, loss=10.306, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 02:03:20,843 (trainer:676) INFO: 37epoch:train:739-779batch: iter_time=1.375e-04, forward_time=0.112, loss_ctc=17.965, loss_att=6.798, acc=0.944, loss=10.148, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 02:03:30,089 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 02:03:36,417 (trainer:676) INFO: 37epoch:train:780-820batch: iter_time=1.413e-04, forward_time=0.111, loss_ctc=18.041, loss_att=6.733, acc=0.942, loss=10.126, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 02:04:15,489 (trainer:334) INFO: 37epoch results: [train] iter_time=4.071e-04, forward_time=0.111, loss_ctc=17.708, loss_att=6.784, acc=0.943, loss=10.061, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377, time=5 minutes and 10.62 seconds, total_count=30488, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=23.896, cer_ctc=0.083, loss_att=11.666, acc=0.925, cer=0.073, wer=0.846, loss=15.335, time=7.44 seconds, total_count=1073, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.97 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 02:04:21,237 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 02:04:21,255 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/25epoch.pth
[ip-10-0-0-95] 2022-04-28 02:04:21,255 (trainer:268) INFO: 38/50epoch started. Estimated time to finish: 1 hour, 16 minutes and 26.68 seconds
[ip-10-0-0-95] 2022-04-28 02:04:28,576 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:04:36,517 (trainer:676) INFO: 38epoch:train:1-41batch: iter_time=0.006, forward_time=0.111, loss_ctc=16.721, loss_att=6.173, acc=0.939, loss=9.338, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:04:51,744 (trainer:676) INFO: 38epoch:train:42-82batch: iter_time=1.432e-04, forward_time=0.110, loss_ctc=15.587, loss_att=5.909, acc=0.949, loss=8.812, backward_time=0.111, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 02:05:07,230 (trainer:676) INFO: 38epoch:train:83-123batch: iter_time=1.363e-04, forward_time=0.110, loss_ctc=16.920, loss_att=6.438, acc=0.947, loss=9.583, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 02:05:22,284 (trainer:676) INFO: 38epoch:train:124-164batch: iter_time=1.290e-04, forward_time=0.109, loss_ctc=16.679, loss_att=6.462, acc=0.948, loss=9.527, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:05:37,384 (trainer:676) INFO: 38epoch:train:165-205batch: iter_time=1.230e-04, forward_time=0.110, loss_ctc=17.246, loss_att=6.539, acc=0.944, loss=9.751, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:05:52,352 (trainer:676) INFO: 38epoch:train:206-246batch: iter_time=1.203e-04, forward_time=0.109, loss_ctc=16.542, loss_att=6.473, acc=0.946, loss=9.493, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 02:06:07,340 (trainer:676) INFO: 38epoch:train:247-287batch: iter_time=1.354e-04, forward_time=0.109, loss_ctc=16.632, loss_att=6.347, acc=0.948, loss=9.433, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 02:06:22,635 (trainer:676) INFO: 38epoch:train:288-328batch: iter_time=1.216e-04, forward_time=0.110, loss_ctc=17.527, loss_att=6.860, acc=0.943, loss=10.060, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:06:38,050 (trainer:676) INFO: 38epoch:train:329-369batch: iter_time=1.196e-04, forward_time=0.111, loss_ctc=16.248, loss_att=6.123, acc=0.948, loss=9.161, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:06:53,710 (trainer:676) INFO: 38epoch:train:370-410batch: iter_time=1.251e-04, forward_time=0.112, loss_ctc=16.071, loss_att=6.318, acc=0.949, loss=9.244, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.382
[ip-10-0-0-95] 2022-04-28 02:07:09,133 (trainer:676) INFO: 38epoch:train:411-451batch: iter_time=1.177e-04, forward_time=0.111, loss_ctc=17.304, loss_att=6.787, acc=0.947, loss=9.942, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:07:24,896 (trainer:676) INFO: 38epoch:train:452-492batch: iter_time=1.204e-04, forward_time=0.112, loss_ctc=17.895, loss_att=6.834, acc=0.945, loss=10.153, backward_time=0.116, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 02:07:40,308 (trainer:676) INFO: 38epoch:train:493-533batch: iter_time=1.180e-04, forward_time=0.111, loss_ctc=17.601, loss_att=6.859, acc=0.941, loss=10.081, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:07:55,770 (trainer:676) INFO: 38epoch:train:534-574batch: iter_time=1.203e-04, forward_time=0.111, loss_ctc=17.318, loss_att=6.561, acc=0.946, loss=9.788, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 02:08:00,866 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:08:11,062 (trainer:676) INFO: 38epoch:train:575-615batch: iter_time=1.194e-04, forward_time=0.110, loss_ctc=17.151, loss_att=6.407, acc=0.945, loss=9.630, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:08:26,303 (trainer:676) INFO: 38epoch:train:616-656batch: iter_time=1.217e-04, forward_time=0.110, loss_ctc=17.191, loss_att=6.545, acc=0.943, loss=9.739, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 02:08:41,510 (trainer:676) INFO: 38epoch:train:657-697batch: iter_time=1.158e-04, forward_time=0.110, loss_ctc=16.755, loss_att=6.402, acc=0.949, loss=9.508, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 02:08:56,957 (trainer:676) INFO: 38epoch:train:698-738batch: iter_time=1.264e-04, forward_time=0.110, loss_ctc=18.063, loss_att=6.789, acc=0.945, loss=10.171, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:09:12,449 (trainer:676) INFO: 38epoch:train:739-779batch: iter_time=1.176e-04, forward_time=0.110, loss_ctc=17.329, loss_att=6.437, acc=0.947, loss=9.704, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:09:27,619 (trainer:676) INFO: 38epoch:train:780-820batch: iter_time=1.236e-04, forward_time=0.109, loss_ctc=17.596, loss_att=6.651, acc=0.945, loss=9.934, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:10:06,656 (trainer:334) INFO: 38epoch results: [train] iter_time=3.934e-04, forward_time=0.110, loss_ctc=17.013, loss_att=6.493, acc=0.946, loss=9.649, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373, time=5 minutes and 7.92 seconds, total_count=31312, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.039, cer_ctc=0.085, loss_att=12.000, acc=0.923, cer=0.077, wer=0.829, loss=15.912, time=7.33 seconds, total_count=1102, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.15 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 02:10:12,167 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 02:10:12,185 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/26epoch.pth
[ip-10-0-0-95] 2022-04-28 02:10:12,185 (trainer:268) INFO: 39/50epoch started. Estimated time to finish: 1 hour, 10 minutes and 33.26 seconds
[ip-10-0-0-95] 2022-04-28 02:10:27,905 (trainer:676) INFO: 39epoch:train:1-41batch: iter_time=0.005, forward_time=0.111, loss_ctc=16.040, loss_att=6.155, acc=0.951, loss=9.121, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 02:10:43,160 (trainer:676) INFO: 39epoch:train:42-82batch: iter_time=1.486e-04, forward_time=0.110, loss_ctc=16.983, loss_att=6.454, acc=0.948, loss=9.612, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:10:58,303 (trainer:676) INFO: 39epoch:train:83-123batch: iter_time=1.382e-04, forward_time=0.109, loss_ctc=16.192, loss_att=6.040, acc=0.953, loss=9.086, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:11:13,597 (trainer:676) INFO: 39epoch:train:124-164batch: iter_time=1.490e-04, forward_time=0.110, loss_ctc=16.246, loss_att=6.305, acc=0.947, loss=9.287, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:11:28,860 (trainer:676) INFO: 39epoch:train:165-205batch: iter_time=1.536e-04, forward_time=0.110, loss_ctc=16.097, loss_att=6.214, acc=0.953, loss=9.179, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:11:43,838 (trainer:676) INFO: 39epoch:train:206-246batch: iter_time=1.395e-04, forward_time=0.109, loss_ctc=16.037, loss_att=5.951, acc=0.948, loss=8.977, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 02:11:58,573 (trainer:676) INFO: 39epoch:train:247-287batch: iter_time=1.405e-04, forward_time=0.109, loss_ctc=16.731, loss_att=6.155, acc=0.945, loss=9.328, backward_time=0.103, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.359
[ip-10-0-0-95] 2022-04-28 02:12:13,762 (trainer:676) INFO: 39epoch:train:288-328batch: iter_time=1.372e-04, forward_time=0.109, loss_ctc=15.925, loss_att=5.941, acc=0.950, loss=8.936, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:12:29,267 (trainer:676) INFO: 39epoch:train:329-369batch: iter_time=1.324e-04, forward_time=0.111, loss_ctc=16.219, loss_att=6.466, acc=0.947, loss=9.392, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:12:44,536 (trainer:676) INFO: 39epoch:train:370-410batch: iter_time=1.357e-04, forward_time=0.110, loss_ctc=15.905, loss_att=5.956, acc=0.950, loss=8.941, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:13:00,074 (trainer:676) INFO: 39epoch:train:411-451batch: iter_time=1.309e-04, forward_time=0.111, loss_ctc=15.183, loss_att=5.737, acc=0.954, loss=8.571, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 02:13:15,603 (trainer:676) INFO: 39epoch:train:452-492batch: iter_time=1.357e-04, forward_time=0.111, loss_ctc=16.493, loss_att=6.286, acc=0.949, loss=9.348, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:13:31,023 (trainer:676) INFO: 39epoch:train:493-533batch: iter_time=1.267e-04, forward_time=0.110, loss_ctc=16.132, loss_att=5.944, acc=0.949, loss=9.001, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:13:46,575 (trainer:676) INFO: 39epoch:train:534-574batch: iter_time=1.356e-04, forward_time=0.111, loss_ctc=16.707, loss_att=6.327, acc=0.946, loss=9.441, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 02:14:01,634 (trainer:676) INFO: 39epoch:train:575-615batch: iter_time=1.413e-04, forward_time=0.110, loss_ctc=16.811, loss_att=6.524, acc=0.948, loss=9.610, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:14:16,801 (trainer:676) INFO: 39epoch:train:616-656batch: iter_time=1.196e-04, forward_time=0.110, loss_ctc=17.259, loss_att=6.447, acc=0.948, loss=9.691, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:14:17,307 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:14:23,883 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:14:32,354 (trainer:676) INFO: 39epoch:train:657-697batch: iter_time=1.314e-04, forward_time=0.112, loss_ctc=16.661, loss_att=6.527, acc=0.937, loss=9.567, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 02:14:47,487 (trainer:676) INFO: 39epoch:train:698-738batch: iter_time=1.556e-04, forward_time=0.110, loss_ctc=15.863, loss_att=6.021, acc=0.950, loss=8.974, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:15:02,631 (trainer:676) INFO: 39epoch:train:739-779batch: iter_time=1.534e-04, forward_time=0.109, loss_ctc=16.867, loss_att=6.332, acc=0.946, loss=9.492, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:15:17,847 (trainer:676) INFO: 39epoch:train:780-820batch: iter_time=1.714e-04, forward_time=0.110, loss_ctc=17.166, loss_att=6.399, acc=0.947, loss=9.629, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 02:15:56,775 (trainer:334) INFO: 39epoch results: [train] iter_time=4.028e-04, forward_time=0.110, loss_ctc=16.362, loss_att=6.201, acc=0.948, loss=9.249, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373, time=5 minutes and 7.29 seconds, total_count=32136, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=24.770, cer_ctc=0.081, loss_att=12.188, acc=0.923, cer=0.075, wer=0.836, loss=15.962, time=7.27 seconds, total_count=1131, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.02 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 02:16:02,312 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 02:16:02,330 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/29epoch.pth
[ip-10-0-0-95] 2022-04-28 02:16:02,331 (trainer:268) INFO: 40/50epoch started. Estimated time to finish: 1 hour, 4 minutes and 39.75 seconds
[ip-10-0-0-95] 2022-04-28 02:16:17,438 (trainer:676) INFO: 40epoch:train:1-41batch: iter_time=0.005, forward_time=0.108, loss_ctc=15.301, loss_att=5.881, acc=0.953, loss=8.707, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:16:32,051 (trainer:676) INFO: 40epoch:train:42-82batch: iter_time=1.315e-04, forward_time=0.107, loss_ctc=15.627, loss_att=6.007, acc=0.950, loss=8.893, backward_time=0.101, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.356
[ip-10-0-0-95] 2022-04-28 02:16:47,109 (trainer:676) INFO: 40epoch:train:83-123batch: iter_time=1.379e-04, forward_time=0.108, loss_ctc=15.674, loss_att=5.884, acc=0.952, loss=8.821, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:17:02,158 (trainer:676) INFO: 40epoch:train:124-164batch: iter_time=1.276e-04, forward_time=0.108, loss_ctc=15.909, loss_att=6.120, acc=0.951, loss=9.057, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:17:17,126 (trainer:676) INFO: 40epoch:train:165-205batch: iter_time=1.281e-04, forward_time=0.109, loss_ctc=15.324, loss_att=5.604, acc=0.949, loss=8.520, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 02:17:32,204 (trainer:676) INFO: 40epoch:train:206-246batch: iter_time=1.251e-04, forward_time=0.109, loss_ctc=15.033, loss_att=5.538, acc=0.955, loss=8.387, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:17:47,246 (trainer:676) INFO: 40epoch:train:247-287batch: iter_time=1.534e-04, forward_time=0.108, loss_ctc=15.350, loss_att=5.811, acc=0.950, loss=8.673, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:18:02,347 (trainer:676) INFO: 40epoch:train:288-328batch: iter_time=1.531e-04, forward_time=0.110, loss_ctc=14.637, loss_att=5.580, acc=0.956, loss=8.297, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:18:17,526 (trainer:676) INFO: 40epoch:train:329-369batch: iter_time=1.541e-04, forward_time=0.108, loss_ctc=16.336, loss_att=6.371, acc=0.952, loss=9.361, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:18:32,670 (trainer:676) INFO: 40epoch:train:370-410batch: iter_time=1.581e-04, forward_time=0.109, loss_ctc=17.040, loss_att=6.349, acc=0.948, loss=9.556, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:18:47,842 (trainer:676) INFO: 40epoch:train:411-451batch: iter_time=1.598e-04, forward_time=0.109, loss_ctc=14.954, loss_att=5.594, acc=0.954, loss=8.402, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:19:02,249 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 02:19:03,023 (trainer:676) INFO: 40epoch:train:452-492batch: iter_time=1.224e-04, forward_time=0.110, loss_ctc=15.555, loss_att=5.985, acc=0.952, loss=8.856, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:19:17,809 (trainer:676) INFO: 40epoch:train:493-533batch: iter_time=1.189e-04, forward_time=0.109, loss_ctc=14.899, loss_att=5.789, acc=0.953, loss=8.522, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.360
[ip-10-0-0-95] 2022-04-28 02:19:32,960 (trainer:676) INFO: 40epoch:train:534-574batch: iter_time=1.216e-04, forward_time=0.109, loss_ctc=16.455, loss_att=6.070, acc=0.949, loss=9.185, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:19:48,299 (trainer:676) INFO: 40epoch:train:575-615batch: iter_time=1.216e-04, forward_time=0.109, loss_ctc=16.350, loss_att=6.123, acc=0.952, loss=9.191, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:19:49,882 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:19:56,717 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:20:03,524 (trainer:676) INFO: 40epoch:train:616-656batch: iter_time=1.163e-04, forward_time=0.109, loss_ctc=16.184, loss_att=5.965, acc=0.941, loss=9.031, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 02:20:18,469 (trainer:676) INFO: 40epoch:train:657-697batch: iter_time=1.160e-04, forward_time=0.108, loss_ctc=16.827, loss_att=6.457, acc=0.944, loss=9.568, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.364
[ip-10-0-0-95] 2022-04-28 02:20:33,285 (trainer:676) INFO: 40epoch:train:698-738batch: iter_time=1.375e-04, forward_time=0.108, loss_ctc=15.393, loss_att=5.675, acc=0.951, loss=8.591, backward_time=0.104, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.361
[ip-10-0-0-95] 2022-04-28 02:20:48,056 (trainer:676) INFO: 40epoch:train:739-779batch: iter_time=1.514e-04, forward_time=0.109, loss_ctc=15.537, loss_att=5.688, acc=0.949, loss=8.643, backward_time=0.104, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.360
[ip-10-0-0-95] 2022-04-28 02:21:03,340 (trainer:676) INFO: 40epoch:train:780-820batch: iter_time=1.171e-04, forward_time=0.109, loss_ctc=15.977, loss_att=6.074, acc=0.951, loss=9.045, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:21:42,205 (trainer:334) INFO: 40epoch results: [train] iter_time=3.770e-04, forward_time=0.109, loss_ctc=15.718, loss_att=5.924, acc=0.950, loss=8.862, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367, time=5 minutes and 2.6 seconds, total_count=32960, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.788, cer_ctc=0.084, loss_att=12.035, acc=0.925, cer=0.075, wer=0.816, loss=16.160, time=7.22 seconds, total_count=1160, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.05 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 02:21:47,748 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 02:21:47,765 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/32epoch.pth
[ip-10-0-0-95] 2022-04-28 02:21:47,766 (trainer:268) INFO: 41/50epoch started. Estimated time to finish: 58 minutes and 45.23 seconds
[ip-10-0-0-95] 2022-04-28 02:22:02,791 (trainer:676) INFO: 41epoch:train:1-41batch: iter_time=0.005, forward_time=0.108, loss_ctc=15.568, loss_att=5.806, acc=0.954, loss=8.734, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.366
[ip-10-0-0-95] 2022-04-28 02:22:17,646 (trainer:676) INFO: 41epoch:train:42-82batch: iter_time=1.407e-04, forward_time=0.106, loss_ctc=14.771, loss_att=5.420, acc=0.953, loss=8.225, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.362
[ip-10-0-0-95] 2022-04-28 02:22:32,203 (trainer:676) INFO: 41epoch:train:83-123batch: iter_time=1.415e-04, forward_time=0.106, loss_ctc=15.004, loss_att=5.718, acc=0.951, loss=8.504, backward_time=0.101, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.355
[ip-10-0-0-95] 2022-04-28 02:22:47,167 (trainer:676) INFO: 41epoch:train:124-164batch: iter_time=1.336e-04, forward_time=0.108, loss_ctc=15.263, loss_att=5.734, acc=0.955, loss=8.592, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 02:23:02,183 (trainer:676) INFO: 41epoch:train:165-205batch: iter_time=1.365e-04, forward_time=0.109, loss_ctc=15.817, loss_att=6.122, acc=0.951, loss=9.031, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.366
[ip-10-0-0-95] 2022-04-28 02:23:08,513 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:23:17,047 (trainer:676) INFO: 41epoch:train:206-246batch: iter_time=1.285e-04, forward_time=0.107, loss_ctc=16.039, loss_att=5.762, acc=0.949, loss=8.845, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.362
[ip-10-0-0-95] 2022-04-28 02:23:32,196 (trainer:676) INFO: 41epoch:train:247-287batch: iter_time=1.310e-04, forward_time=0.109, loss_ctc=15.004, loss_att=5.718, acc=0.955, loss=8.504, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:23:47,073 (trainer:676) INFO: 41epoch:train:288-328batch: iter_time=1.197e-04, forward_time=0.108, loss_ctc=15.252, loss_att=5.604, acc=0.950, loss=8.499, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.363
[ip-10-0-0-95] 2022-04-28 02:24:01,922 (trainer:676) INFO: 41epoch:train:329-369batch: iter_time=1.163e-04, forward_time=0.109, loss_ctc=14.478, loss_att=5.513, acc=0.956, loss=8.202, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.362
[ip-10-0-0-95] 2022-04-28 02:24:17,028 (trainer:676) INFO: 41epoch:train:370-410batch: iter_time=1.169e-04, forward_time=0.109, loss_ctc=14.677, loss_att=5.615, acc=0.954, loss=8.334, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:24:32,366 (trainer:676) INFO: 41epoch:train:411-451batch: iter_time=1.196e-04, forward_time=0.109, loss_ctc=14.849, loss_att=5.508, acc=0.952, loss=8.310, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:24:47,683 (trainer:676) INFO: 41epoch:train:452-492batch: iter_time=1.181e-04, forward_time=0.109, loss_ctc=15.457, loss_att=5.764, acc=0.952, loss=8.672, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:24:59,415 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:25:03,154 (trainer:676) INFO: 41epoch:train:493-533batch: iter_time=1.167e-04, forward_time=0.110, loss_ctc=14.952, loss_att=5.567, acc=0.945, loss=8.382, backward_time=0.113, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 02:25:18,622 (trainer:676) INFO: 41epoch:train:534-574batch: iter_time=1.161e-04, forward_time=0.110, loss_ctc=15.321, loss_att=5.770, acc=0.951, loss=8.635, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 02:25:33,908 (trainer:676) INFO: 41epoch:train:575-615batch: iter_time=1.163e-04, forward_time=0.110, loss_ctc=16.163, loss_att=6.162, acc=0.952, loss=9.162, backward_time=0.111, optim_step_time=0.040, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:25:48,972 (trainer:676) INFO: 41epoch:train:616-656batch: iter_time=1.190e-04, forward_time=0.109, loss_ctc=15.339, loss_att=5.843, acc=0.954, loss=8.692, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:26:04,142 (trainer:676) INFO: 41epoch:train:657-697batch: iter_time=1.233e-04, forward_time=0.111, loss_ctc=14.942, loss_att=5.678, acc=0.951, loss=8.457, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:26:19,746 (trainer:676) INFO: 41epoch:train:698-738batch: iter_time=1.219e-04, forward_time=0.110, loss_ctc=14.614, loss_att=5.532, acc=0.956, loss=8.257, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 02:26:34,820 (trainer:676) INFO: 41epoch:train:739-779batch: iter_time=1.140e-04, forward_time=0.111, loss_ctc=14.995, loss_att=5.455, acc=0.955, loss=8.317, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:26:50,088 (trainer:676) INFO: 41epoch:train:780-820batch: iter_time=1.157e-04, forward_time=0.111, loss_ctc=15.344, loss_att=5.783, acc=0.954, loss=8.651, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:27:28,993 (trainer:334) INFO: 41epoch results: [train] iter_time=3.760e-04, forward_time=0.109, loss_ctc=15.185, loss_att=5.699, acc=0.952, loss=8.545, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368, time=5 minutes and 3.92 seconds, total_count=33784, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=26.381, cer_ctc=0.088, loss_att=12.279, acc=0.923, cer=0.078, wer=0.846, loss=16.510, time=7.36 seconds, total_count=1189, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.94 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 02:27:34,670 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 02:27:34,688 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/30epoch.pth
[ip-10-0-0-95] 2022-04-28 02:27:34,688 (trainer:268) INFO: 42/50epoch started. Estimated time to finish: 52 minutes and 51.47 seconds
[ip-10-0-0-95] 2022-04-28 02:27:38,643 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:27:50,069 (trainer:676) INFO: 42epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=14.635, loss_att=5.283, acc=0.954, loss=8.088, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 02:28:05,094 (trainer:676) INFO: 42epoch:train:42-82batch: iter_time=1.253e-04, forward_time=0.108, loss_ctc=14.885, loss_att=5.556, acc=0.951, loss=8.355, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.366
[ip-10-0-0-95] 2022-04-28 02:28:20,155 (trainer:676) INFO: 42epoch:train:83-123batch: iter_time=1.245e-04, forward_time=0.108, loss_ctc=14.261, loss_att=5.408, acc=0.954, loss=8.064, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:28:35,297 (trainer:676) INFO: 42epoch:train:124-164batch: iter_time=1.377e-04, forward_time=0.108, loss_ctc=15.562, loss_att=5.815, acc=0.952, loss=8.739, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:28:50,562 (trainer:676) INFO: 42epoch:train:165-205batch: iter_time=1.317e-04, forward_time=0.109, loss_ctc=14.982, loss_att=5.575, acc=0.955, loss=8.397, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:29:05,682 (trainer:676) INFO: 42epoch:train:206-246batch: iter_time=1.572e-04, forward_time=0.110, loss_ctc=15.286, loss_att=5.830, acc=0.956, loss=8.667, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:29:20,847 (trainer:676) INFO: 42epoch:train:247-287batch: iter_time=1.206e-04, forward_time=0.109, loss_ctc=15.200, loss_att=5.861, acc=0.953, loss=8.663, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:29:35,771 (trainer:676) INFO: 42epoch:train:288-328batch: iter_time=1.264e-04, forward_time=0.109, loss_ctc=14.544, loss_att=5.473, acc=0.956, loss=8.194, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.364
[ip-10-0-0-95] 2022-04-28 02:29:50,900 (trainer:676) INFO: 42epoch:train:329-369batch: iter_time=1.237e-04, forward_time=0.109, loss_ctc=15.432, loss_att=5.807, acc=0.951, loss=8.695, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:30:05,801 (trainer:676) INFO: 42epoch:train:370-410batch: iter_time=1.267e-04, forward_time=0.109, loss_ctc=14.328, loss_att=5.183, acc=0.953, loss=7.926, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.363
[ip-10-0-0-95] 2022-04-28 02:30:09,275 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:30:20,893 (trainer:676) INFO: 42epoch:train:411-451batch: iter_time=1.268e-04, forward_time=0.110, loss_ctc=14.206, loss_att=5.290, acc=0.950, loss=7.965, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:30:36,295 (trainer:676) INFO: 42epoch:train:452-492batch: iter_time=1.557e-04, forward_time=0.110, loss_ctc=13.641, loss_att=5.231, acc=0.958, loss=7.754, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 02:30:51,266 (trainer:676) INFO: 42epoch:train:493-533batch: iter_time=1.355e-04, forward_time=0.109, loss_ctc=15.818, loss_att=5.932, acc=0.952, loss=8.898, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 02:31:06,348 (trainer:676) INFO: 42epoch:train:534-574batch: iter_time=1.578e-04, forward_time=0.109, loss_ctc=14.663, loss_att=5.380, acc=0.954, loss=8.165, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:31:21,448 (trainer:676) INFO: 42epoch:train:575-615batch: iter_time=1.583e-04, forward_time=0.110, loss_ctc=14.967, loss_att=5.584, acc=0.956, loss=8.399, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:31:36,896 (trainer:676) INFO: 42epoch:train:616-656batch: iter_time=1.333e-04, forward_time=0.109, loss_ctc=15.243, loss_att=5.573, acc=0.953, loss=8.474, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:31:51,959 (trainer:676) INFO: 42epoch:train:657-697batch: iter_time=1.216e-04, forward_time=0.109, loss_ctc=15.403, loss_att=5.615, acc=0.953, loss=8.552, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:32:07,660 (trainer:676) INFO: 42epoch:train:698-738batch: iter_time=1.250e-04, forward_time=0.112, loss_ctc=15.612, loss_att=5.723, acc=0.955, loss=8.690, backward_time=0.116, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 02:32:22,916 (trainer:676) INFO: 42epoch:train:739-779batch: iter_time=1.372e-04, forward_time=0.112, loss_ctc=14.357, loss_att=5.317, acc=0.952, loss=8.029, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:32:38,486 (trainer:676) INFO: 42epoch:train:780-820batch: iter_time=1.369e-04, forward_time=0.112, loss_ctc=15.354, loss_att=5.824, acc=0.953, loss=8.683, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 02:33:17,528 (trainer:334) INFO: 42epoch results: [train] iter_time=3.748e-04, forward_time=0.110, loss_ctc=14.910, loss_att=5.558, acc=0.953, loss=8.364, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370, time=5 minutes and 5.38 seconds, total_count=34608, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=24.115, cer_ctc=0.082, loss_att=11.777, acc=0.925, cer=0.075, wer=0.820, loss=15.478, time=7.43 seconds, total_count=1218, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.02 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 02:33:23,067 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 02:33:23,085 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/31epoch.pth
[ip-10-0-0-95] 2022-04-28 02:33:23,085 (trainer:268) INFO: 43/50epoch started. Estimated time to finish: 46 minutes and 58.33 seconds
[ip-10-0-0-95] 2022-04-28 02:33:38,373 (trainer:676) INFO: 43epoch:train:1-41batch: iter_time=0.006, forward_time=0.111, loss_ctc=14.007, loss_att=5.153, acc=0.958, loss=7.809, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:33:53,966 (trainer:676) INFO: 43epoch:train:42-82batch: iter_time=1.330e-04, forward_time=0.110, loss_ctc=14.426, loss_att=5.384, acc=0.956, loss=8.096, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 02:34:06,094 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 02:34:09,469 (trainer:676) INFO: 43epoch:train:83-123batch: iter_time=1.351e-04, forward_time=0.111, loss_ctc=13.814, loss_att=5.280, acc=0.957, loss=7.840, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:34:24,994 (trainer:676) INFO: 43epoch:train:124-164batch: iter_time=1.320e-04, forward_time=0.111, loss_ctc=13.615, loss_att=5.135, acc=0.959, loss=7.679, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:34:40,767 (trainer:676) INFO: 43epoch:train:165-205batch: iter_time=1.324e-04, forward_time=0.111, loss_ctc=15.204, loss_att=5.653, acc=0.954, loss=8.518, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 02:34:56,057 (trainer:676) INFO: 43epoch:train:206-246batch: iter_time=1.425e-04, forward_time=0.111, loss_ctc=14.199, loss_att=5.158, acc=0.956, loss=7.870, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:35:11,420 (trainer:676) INFO: 43epoch:train:247-287batch: iter_time=1.328e-04, forward_time=0.112, loss_ctc=14.022, loss_att=5.055, acc=0.957, loss=7.745, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:35:26,757 (trainer:676) INFO: 43epoch:train:288-328batch: iter_time=1.221e-04, forward_time=0.110, loss_ctc=14.719, loss_att=5.572, acc=0.954, loss=8.316, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:35:42,258 (trainer:676) INFO: 43epoch:train:329-369batch: iter_time=1.295e-04, forward_time=0.111, loss_ctc=14.511, loss_att=5.458, acc=0.955, loss=8.174, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:35:58,180 (trainer:676) INFO: 43epoch:train:370-410batch: iter_time=1.180e-04, forward_time=0.112, loss_ctc=14.974, loss_att=5.690, acc=0.955, loss=8.475, backward_time=0.117, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.388
[ip-10-0-0-95] 2022-04-28 02:36:13,465 (trainer:676) INFO: 43epoch:train:411-451batch: iter_time=1.262e-04, forward_time=0.112, loss_ctc=14.413, loss_att=5.241, acc=0.955, loss=7.993, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:36:29,072 (trainer:676) INFO: 43epoch:train:452-492batch: iter_time=1.364e-04, forward_time=0.111, loss_ctc=14.779, loss_att=5.397, acc=0.956, loss=8.211, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 02:36:44,542 (trainer:676) INFO: 43epoch:train:493-533batch: iter_time=1.393e-04, forward_time=0.112, loss_ctc=14.145, loss_att=5.283, acc=0.955, loss=7.942, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 02:37:00,325 (trainer:676) INFO: 43epoch:train:534-574batch: iter_time=1.360e-04, forward_time=0.112, loss_ctc=13.977, loss_att=5.272, acc=0.957, loss=7.884, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.385
[ip-10-0-0-95] 2022-04-28 02:37:16,218 (trainer:676) INFO: 43epoch:train:575-615batch: iter_time=1.312e-04, forward_time=0.113, loss_ctc=14.151, loss_att=5.320, acc=0.957, loss=7.969, backward_time=0.119, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.387
[ip-10-0-0-95] 2022-04-28 02:37:32,020 (trainer:676) INFO: 43epoch:train:616-656batch: iter_time=1.435e-04, forward_time=0.116, loss_ctc=13.765, loss_att=5.025, acc=0.956, loss=7.647, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.385
[ip-10-0-0-95] 2022-04-28 02:37:47,741 (trainer:676) INFO: 43epoch:train:657-697batch: iter_time=1.530e-04, forward_time=0.116, loss_ctc=13.532, loss_att=5.014, acc=0.956, loss=7.569, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 02:37:59,382 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:38:02,981 (trainer:676) INFO: 43epoch:train:698-738batch: iter_time=1.620e-04, forward_time=0.115, loss_ctc=14.823, loss_att=5.287, acc=0.953, loss=8.148, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 02:38:18,509 (trainer:676) INFO: 43epoch:train:739-779batch: iter_time=1.618e-04, forward_time=0.117, loss_ctc=14.336, loss_att=5.288, acc=0.956, loss=8.002, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:38:31,601 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:38:34,221 (trainer:676) INFO: 43epoch:train:780-820batch: iter_time=1.602e-04, forward_time=0.117, loss_ctc=14.687, loss_att=5.430, acc=0.951, loss=8.207, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 02:39:13,367 (trainer:334) INFO: 43epoch results: [train] iter_time=4.092e-04, forward_time=0.113, loss_ctc=14.282, loss_att=5.293, acc=0.956, loss=7.989, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379, time=5 minutes and 12.75 seconds, total_count=35432, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.406, cer_ctc=0.083, loss_att=12.062, acc=0.925, cer=0.075, wer=0.833, loss=16.066, time=7.5 seconds, total_count=1247, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.03 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 02:39:19,129 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 02:39:19,147 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/33epoch.pth
[ip-10-0-0-95] 2022-04-28 02:39:19,147 (trainer:268) INFO: 44/50epoch started. Estimated time to finish: 41 minutes and 6.65 seconds
[ip-10-0-0-95] 2022-04-28 02:39:34,484 (trainer:676) INFO: 44epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=13.408, loss_att=4.926, acc=0.957, loss=7.470, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:39:46,906 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:39:49,871 (trainer:676) INFO: 44epoch:train:42-82batch: iter_time=1.462e-04, forward_time=0.111, loss_ctc=13.114, loss_att=4.982, acc=0.956, loss=7.422, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 02:40:05,621 (trainer:676) INFO: 44epoch:train:83-123batch: iter_time=1.509e-04, forward_time=0.114, loss_ctc=14.131, loss_att=5.129, acc=0.956, loss=7.830, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 02:40:21,117 (trainer:676) INFO: 44epoch:train:124-164batch: iter_time=1.225e-04, forward_time=0.114, loss_ctc=14.414, loss_att=5.283, acc=0.958, loss=8.022, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:40:36,709 (trainer:676) INFO: 44epoch:train:165-205batch: iter_time=1.551e-04, forward_time=0.115, loss_ctc=13.002, loss_att=4.823, acc=0.961, loss=7.276, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 02:40:52,350 (trainer:676) INFO: 44epoch:train:206-246batch: iter_time=1.616e-04, forward_time=0.112, loss_ctc=13.673, loss_att=5.273, acc=0.959, loss=7.793, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.381
[ip-10-0-0-95] 2022-04-28 02:41:07,696 (trainer:676) INFO: 44epoch:train:247-287batch: iter_time=1.638e-04, forward_time=0.111, loss_ctc=14.132, loss_att=5.087, acc=0.957, loss=7.801, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:41:23,201 (trainer:676) INFO: 44epoch:train:288-328batch: iter_time=1.425e-04, forward_time=0.111, loss_ctc=12.710, loss_att=4.647, acc=0.962, loss=7.066, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:41:33,542 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:41:38,813 (trainer:676) INFO: 44epoch:train:329-369batch: iter_time=1.666e-04, forward_time=0.112, loss_ctc=14.131, loss_att=5.222, acc=0.952, loss=7.895, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 02:41:54,310 (trainer:676) INFO: 44epoch:train:370-410batch: iter_time=1.568e-04, forward_time=0.111, loss_ctc=14.122, loss_att=5.165, acc=0.958, loss=7.852, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:42:09,836 (trainer:676) INFO: 44epoch:train:411-451batch: iter_time=1.609e-04, forward_time=0.111, loss_ctc=12.970, loss_att=4.574, acc=0.958, loss=7.093, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:42:25,336 (trainer:676) INFO: 44epoch:train:452-492batch: iter_time=1.705e-04, forward_time=0.112, loss_ctc=13.961, loss_att=5.062, acc=0.959, loss=7.732, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 02:42:40,812 (trainer:676) INFO: 44epoch:train:493-533batch: iter_time=1.755e-04, forward_time=0.112, loss_ctc=14.850, loss_att=5.409, acc=0.957, loss=8.242, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 02:42:56,089 (trainer:676) INFO: 44epoch:train:534-574batch: iter_time=1.490e-04, forward_time=0.111, loss_ctc=14.557, loss_att=5.386, acc=0.956, loss=8.138, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:43:11,426 (trainer:676) INFO: 44epoch:train:575-615batch: iter_time=1.623e-04, forward_time=0.111, loss_ctc=14.623, loss_att=5.430, acc=0.957, loss=8.188, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:43:26,842 (trainer:676) INFO: 44epoch:train:616-656batch: iter_time=1.729e-04, forward_time=0.111, loss_ctc=14.524, loss_att=5.237, acc=0.955, loss=8.023, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:43:41,955 (trainer:676) INFO: 44epoch:train:657-697batch: iter_time=1.614e-04, forward_time=0.111, loss_ctc=14.257, loss_att=5.243, acc=0.957, loss=7.947, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:43:57,374 (trainer:676) INFO: 44epoch:train:698-738batch: iter_time=1.559e-04, forward_time=0.111, loss_ctc=13.939, loss_att=5.119, acc=0.959, loss=7.765, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:44:12,822 (trainer:676) INFO: 44epoch:train:739-779batch: iter_time=1.641e-04, forward_time=0.111, loss_ctc=14.726, loss_att=5.446, acc=0.956, loss=8.230, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:44:28,083 (trainer:676) INFO: 44epoch:train:780-820batch: iter_time=1.608e-04, forward_time=0.110, loss_ctc=13.911, loss_att=5.000, acc=0.956, loss=7.673, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:45:07,314 (trainer:334) INFO: 44epoch results: [train] iter_time=4.139e-04, forward_time=0.112, loss_ctc=13.952, loss_att=5.117, acc=0.957, loss=7.767, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376, time=5 minutes and 10.44 seconds, total_count=36256, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.932, cer_ctc=0.082, loss_att=12.352, acc=0.926, cer=0.072, wer=0.820, loss=16.426, time=7.38 seconds, total_count=1276, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.35 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 02:45:12,906 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 02:45:12,924 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/35epoch.pth
[ip-10-0-0-95] 2022-04-28 02:45:12,924 (trainer:268) INFO: 45/50epoch started. Estimated time to finish: 35 minutes and 14.46 seconds
[ip-10-0-0-95] 2022-04-28 02:45:28,308 (trainer:676) INFO: 45epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=13.478, loss_att=4.948, acc=0.962, loss=7.507, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 02:45:43,371 (trainer:676) INFO: 45epoch:train:42-82batch: iter_time=1.762e-04, forward_time=0.108, loss_ctc=13.473, loss_att=4.896, acc=0.960, loss=7.469, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:45:58,505 (trainer:676) INFO: 45epoch:train:83-123batch: iter_time=1.602e-04, forward_time=0.109, loss_ctc=13.316, loss_att=4.930, acc=0.959, loss=7.446, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:46:00,844 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:46:13,671 (trainer:676) INFO: 45epoch:train:124-164batch: iter_time=1.601e-04, forward_time=0.108, loss_ctc=13.124, loss_att=4.820, acc=0.956, loss=7.311, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:46:28,542 (trainer:676) INFO: 45epoch:train:165-205batch: iter_time=1.471e-04, forward_time=0.108, loss_ctc=14.096, loss_att=5.054, acc=0.958, loss=7.766, backward_time=0.104, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.362
[ip-10-0-0-95] 2022-04-28 02:46:43,632 (trainer:676) INFO: 45epoch:train:206-246batch: iter_time=1.420e-04, forward_time=0.110, loss_ctc=13.464, loss_att=4.833, acc=0.958, loss=7.422, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:46:58,587 (trainer:676) INFO: 45epoch:train:247-287batch: iter_time=1.339e-04, forward_time=0.110, loss_ctc=14.081, loss_att=5.150, acc=0.959, loss=7.829, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.364
[ip-10-0-0-95] 2022-04-28 02:47:13,892 (trainer:676) INFO: 45epoch:train:288-328batch: iter_time=1.717e-04, forward_time=0.111, loss_ctc=13.379, loss_att=4.867, acc=0.959, loss=7.420, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:47:29,153 (trainer:676) INFO: 45epoch:train:329-369batch: iter_time=1.693e-04, forward_time=0.110, loss_ctc=13.055, loss_att=4.802, acc=0.959, loss=7.278, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:47:44,284 (trainer:676) INFO: 45epoch:train:370-410batch: iter_time=1.764e-04, forward_time=0.110, loss_ctc=14.408, loss_att=5.196, acc=0.958, loss=7.959, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:47:59,573 (trainer:676) INFO: 45epoch:train:411-451batch: iter_time=1.504e-04, forward_time=0.110, loss_ctc=12.960, loss_att=4.669, acc=0.961, loss=7.157, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:48:14,906 (trainer:676) INFO: 45epoch:train:452-492batch: iter_time=1.642e-04, forward_time=0.111, loss_ctc=13.507, loss_att=4.943, acc=0.958, loss=7.512, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:48:17,557 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 02:48:30,383 (trainer:676) INFO: 45epoch:train:493-533batch: iter_time=1.667e-04, forward_time=0.111, loss_ctc=13.767, loss_att=5.296, acc=0.959, loss=7.837, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 02:48:45,834 (trainer:676) INFO: 45epoch:train:534-574batch: iter_time=1.565e-04, forward_time=0.111, loss_ctc=13.439, loss_att=4.841, acc=0.959, loss=7.420, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 02:49:01,408 (trainer:676) INFO: 45epoch:train:575-615batch: iter_time=1.314e-04, forward_time=0.111, loss_ctc=14.654, loss_att=5.496, acc=0.956, loss=8.244, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 02:49:16,978 (trainer:676) INFO: 45epoch:train:616-656batch: iter_time=1.588e-04, forward_time=0.112, loss_ctc=12.627, loss_att=4.509, acc=0.960, loss=6.944, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 02:49:31,476 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:49:32,535 (trainer:676) INFO: 45epoch:train:657-697batch: iter_time=1.540e-04, forward_time=0.111, loss_ctc=14.718, loss_att=5.464, acc=0.950, loss=8.240, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 02:49:48,244 (trainer:676) INFO: 45epoch:train:698-738batch: iter_time=1.560e-04, forward_time=0.113, loss_ctc=12.832, loss_att=4.537, acc=0.961, loss=7.026, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 02:50:03,634 (trainer:676) INFO: 45epoch:train:739-779batch: iter_time=1.553e-04, forward_time=0.112, loss_ctc=12.844, loss_att=4.593, acc=0.961, loss=7.068, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 02:50:19,120 (trainer:676) INFO: 45epoch:train:780-820batch: iter_time=1.658e-04, forward_time=0.112, loss_ctc=14.227, loss_att=5.380, acc=0.957, loss=8.034, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 02:50:58,195 (trainer:334) INFO: 45epoch results: [train] iter_time=4.227e-04, forward_time=0.110, loss_ctc=13.575, loss_att=4.962, acc=0.958, loss=7.546, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373, time=5 minutes and 7.78 seconds, total_count=37080, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.368, cer_ctc=0.083, loss_att=12.244, acc=0.925, cer=0.078, wer=0.832, loss=16.182, time=7.41 seconds, total_count=1305, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.09 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 02:51:03,913 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 02:51:03,931 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/36epoch.pth
[ip-10-0-0-95] 2022-04-28 02:51:03,931 (trainer:268) INFO: 46/50epoch started. Estimated time to finish: 29 minutes and 21.9 seconds
[ip-10-0-0-95] 2022-04-28 02:51:19,350 (trainer:676) INFO: 46epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=13.149, loss_att=4.904, acc=0.961, loss=7.377, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 02:51:34,251 (trainer:676) INFO: 46epoch:train:42-82batch: iter_time=1.811e-04, forward_time=0.108, loss_ctc=12.233, loss_att=4.460, acc=0.964, loss=6.792, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.363
[ip-10-0-0-95] 2022-04-28 02:51:49,148 (trainer:676) INFO: 46epoch:train:83-123batch: iter_time=1.715e-04, forward_time=0.109, loss_ctc=12.024, loss_att=4.221, acc=0.962, loss=6.562, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.363
[ip-10-0-0-95] 2022-04-28 02:52:04,206 (trainer:676) INFO: 46epoch:train:124-164batch: iter_time=1.463e-04, forward_time=0.110, loss_ctc=13.244, loss_att=4.843, acc=0.959, loss=7.363, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 02:52:19,295 (trainer:676) INFO: 46epoch:train:165-205batch: iter_time=1.597e-04, forward_time=0.108, loss_ctc=13.201, loss_att=4.784, acc=0.960, loss=7.309, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:52:34,211 (trainer:676) INFO: 46epoch:train:206-246batch: iter_time=1.547e-04, forward_time=0.108, loss_ctc=13.180, loss_att=4.743, acc=0.960, loss=7.274, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.363
[ip-10-0-0-95] 2022-04-28 02:52:49,046 (trainer:676) INFO: 46epoch:train:247-287batch: iter_time=1.420e-04, forward_time=0.109, loss_ctc=13.109, loss_att=4.787, acc=0.963, loss=7.284, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.362
[ip-10-0-0-95] 2022-04-28 02:53:04,241 (trainer:676) INFO: 46epoch:train:288-328batch: iter_time=1.267e-04, forward_time=0.109, loss_ctc=13.383, loss_att=5.108, acc=0.960, loss=7.590, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:53:19,339 (trainer:676) INFO: 46epoch:train:329-369batch: iter_time=1.228e-04, forward_time=0.109, loss_ctc=14.109, loss_att=5.230, acc=0.958, loss=7.894, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 02:53:34,239 (trainer:676) INFO: 46epoch:train:370-410batch: iter_time=1.323e-04, forward_time=0.109, loss_ctc=13.269, loss_att=4.855, acc=0.959, loss=7.379, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.363
[ip-10-0-0-95] 2022-04-28 02:53:49,541 (trainer:676) INFO: 46epoch:train:411-451batch: iter_time=1.398e-04, forward_time=0.110, loss_ctc=12.768, loss_att=4.635, acc=0.959, loss=7.075, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:53:56,907 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:54:05,028 (trainer:676) INFO: 46epoch:train:452-492batch: iter_time=1.367e-04, forward_time=0.110, loss_ctc=12.380, loss_att=4.716, acc=0.958, loss=7.015, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 02:54:19,973 (trainer:676) INFO: 46epoch:train:493-533batch: iter_time=1.408e-04, forward_time=0.109, loss_ctc=13.467, loss_att=5.018, acc=0.961, loss=7.552, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.364
[ip-10-0-0-95] 2022-04-28 02:54:32,782 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:54:35,277 (trainer:676) INFO: 46epoch:train:534-574batch: iter_time=1.489e-04, forward_time=0.111, loss_ctc=13.271, loss_att=4.752, acc=0.955, loss=7.308, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 02:54:50,309 (trainer:676) INFO: 46epoch:train:575-615batch: iter_time=1.251e-04, forward_time=0.109, loss_ctc=14.738, loss_att=5.388, acc=0.955, loss=8.193, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.366
[ip-10-0-0-95] 2022-04-28 02:55:05,444 (trainer:676) INFO: 46epoch:train:616-656batch: iter_time=1.303e-04, forward_time=0.111, loss_ctc=12.646, loss_att=4.662, acc=0.962, loss=7.057, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:55:21,201 (trainer:676) INFO: 46epoch:train:657-697batch: iter_time=1.285e-04, forward_time=0.113, loss_ctc=13.096, loss_att=4.664, acc=0.961, loss=7.194, backward_time=0.116, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 02:55:36,595 (trainer:676) INFO: 46epoch:train:698-738batch: iter_time=1.208e-04, forward_time=0.110, loss_ctc=14.190, loss_att=5.086, acc=0.958, loss=7.817, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 02:55:52,309 (trainer:676) INFO: 46epoch:train:739-779batch: iter_time=1.256e-04, forward_time=0.112, loss_ctc=13.377, loss_att=4.924, acc=0.961, loss=7.460, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.383
[ip-10-0-0-95] 2022-04-28 02:56:08,069 (trainer:676) INFO: 46epoch:train:780-820batch: iter_time=1.198e-04, forward_time=0.112, loss_ctc=13.371, loss_att=4.841, acc=0.959, loss=7.400, backward_time=0.115, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.384
[ip-10-0-0-95] 2022-04-28 02:56:47,370 (trainer:334) INFO: 46epoch results: [train] iter_time=3.855e-04, forward_time=0.110, loss_ctc=13.211, loss_att=4.831, acc=0.960, loss=7.345, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371, time=5 minutes and 5.73 seconds, total_count=37904, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=24.884, cer_ctc=0.079, loss_att=12.137, acc=0.926, cer=0.074, wer=0.812, loss=15.961, time=7.45 seconds, total_count=1334, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.25 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 02:56:52,966 (trainer:382) INFO: The best model has been updated: valid.acc
[ip-10-0-0-95] 2022-04-28 02:56:52,984 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/41epoch.pth
[ip-10-0-0-95] 2022-04-28 02:56:52,984 (trainer:268) INFO: 47/50epoch started. Estimated time to finish: 23 minutes and 29.23 seconds
[ip-10-0-0-95] 2022-04-28 02:57:08,581 (trainer:676) INFO: 47epoch:train:1-41batch: iter_time=0.006, forward_time=0.112, loss_ctc=12.312, loss_att=4.522, acc=0.961, loss=6.859, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 02:57:16,052 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 02:57:23,317 (trainer:676) INFO: 47epoch:train:42-82batch: iter_time=1.509e-04, forward_time=0.109, loss_ctc=12.209, loss_att=4.675, acc=0.964, loss=6.936, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.359
[ip-10-0-0-95] 2022-04-28 02:57:38,478 (trainer:676) INFO: 47epoch:train:83-123batch: iter_time=1.440e-04, forward_time=0.109, loss_ctc=12.570, loss_att=4.408, acc=0.961, loss=6.857, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:57:53,836 (trainer:676) INFO: 47epoch:train:124-164batch: iter_time=1.643e-04, forward_time=0.109, loss_ctc=13.375, loss_att=4.780, acc=0.960, loss=7.359, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:57:59,134 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 02:58:09,031 (trainer:676) INFO: 47epoch:train:165-205batch: iter_time=1.470e-04, forward_time=0.109, loss_ctc=13.303, loss_att=4.756, acc=0.960, loss=7.320, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:58:24,427 (trainer:676) INFO: 47epoch:train:206-246batch: iter_time=1.377e-04, forward_time=0.110, loss_ctc=12.496, loss_att=4.707, acc=0.963, loss=7.044, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 02:58:39,761 (trainer:676) INFO: 47epoch:train:247-287batch: iter_time=1.638e-04, forward_time=0.113, loss_ctc=13.857, loss_att=4.961, acc=0.959, loss=7.630, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:58:55,027 (trainer:676) INFO: 47epoch:train:288-328batch: iter_time=1.425e-04, forward_time=0.113, loss_ctc=12.988, loss_att=4.576, acc=0.961, loss=7.100, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 02:59:10,386 (trainer:676) INFO: 47epoch:train:329-369batch: iter_time=1.405e-04, forward_time=0.113, loss_ctc=12.937, loss_att=4.612, acc=0.962, loss=7.109, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 02:59:25,588 (trainer:676) INFO: 47epoch:train:370-410batch: iter_time=1.518e-04, forward_time=0.113, loss_ctc=12.464, loss_att=4.451, acc=0.963, loss=6.855, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 02:59:40,720 (trainer:676) INFO: 47epoch:train:411-451batch: iter_time=1.679e-04, forward_time=0.114, loss_ctc=13.339, loss_att=4.784, acc=0.962, loss=7.350, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 02:59:56,006 (trainer:676) INFO: 47epoch:train:452-492batch: iter_time=1.633e-04, forward_time=0.114, loss_ctc=12.976, loss_att=4.747, acc=0.960, loss=7.216, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 03:00:04,102 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 03:00:11,310 (trainer:676) INFO: 47epoch:train:493-533batch: iter_time=1.621e-04, forward_time=0.114, loss_ctc=14.064, loss_att=5.096, acc=0.954, loss=7.786, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 03:00:26,508 (trainer:676) INFO: 47epoch:train:534-574batch: iter_time=1.631e-04, forward_time=0.112, loss_ctc=12.501, loss_att=4.398, acc=0.963, loss=6.829, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 03:00:42,038 (trainer:676) INFO: 47epoch:train:575-615batch: iter_time=1.499e-04, forward_time=0.114, loss_ctc=12.617, loss_att=4.544, acc=0.964, loss=6.966, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 03:00:57,349 (trainer:676) INFO: 47epoch:train:616-656batch: iter_time=1.609e-04, forward_time=0.109, loss_ctc=12.704, loss_att=4.712, acc=0.963, loss=7.109, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 03:01:12,297 (trainer:676) INFO: 47epoch:train:657-697batch: iter_time=1.680e-04, forward_time=0.109, loss_ctc=13.162, loss_att=4.726, acc=0.960, loss=7.257, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.364
[ip-10-0-0-95] 2022-04-28 03:01:27,363 (trainer:676) INFO: 47epoch:train:698-738batch: iter_time=1.565e-04, forward_time=0.109, loss_ctc=12.352, loss_att=4.551, acc=0.963, loss=6.891, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 03:01:42,549 (trainer:676) INFO: 47epoch:train:739-779batch: iter_time=1.574e-04, forward_time=0.110, loss_ctc=12.804, loss_att=4.654, acc=0.961, loss=7.099, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.370
[ip-10-0-0-95] 2022-04-28 03:01:57,844 (trainer:676) INFO: 47epoch:train:780-820batch: iter_time=1.317e-04, forward_time=0.111, loss_ctc=12.377, loss_att=4.417, acc=0.962, loss=6.805, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 03:02:36,975 (trainer:334) INFO: 47epoch results: [train] iter_time=4.396e-04, forward_time=0.111, loss_ctc=12.892, loss_att=4.660, acc=0.961, loss=7.130, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371, time=5 minutes and 6.43 seconds, total_count=38728, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=25.663, cer_ctc=0.083, loss_att=12.197, acc=0.926, cer=0.073, wer=0.825, loss=16.237, time=7.37 seconds, total_count=1363, gpu_max_cached_mem_GB=7.422, [att_plot] time=30.19 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 03:02:42,687 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 03:02:42,705 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/39epoch.pth
[ip-10-0-0-95] 2022-04-28 03:02:42,705 (trainer:268) INFO: 48/50epoch started. Estimated time to finish: 17 minutes and 36.76 seconds
[ip-10-0-0-95] 2022-04-28 03:02:58,222 (trainer:676) INFO: 48epoch:train:1-41batch: iter_time=0.005, forward_time=0.112, loss_ctc=12.460, loss_att=4.603, acc=0.964, loss=6.960, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 03:03:13,169 (trainer:676) INFO: 48epoch:train:42-82batch: iter_time=1.382e-04, forward_time=0.109, loss_ctc=12.865, loss_att=4.504, acc=0.962, loss=7.012, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.364
[ip-10-0-0-95] 2022-04-28 03:03:28,239 (trainer:676) INFO: 48epoch:train:83-123batch: iter_time=1.222e-04, forward_time=0.110, loss_ctc=11.846, loss_att=4.480, acc=0.963, loss=6.690, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 03:03:43,763 (trainer:676) INFO: 48epoch:train:124-164batch: iter_time=1.211e-04, forward_time=0.110, loss_ctc=12.973, loss_att=4.634, acc=0.960, loss=7.136, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 03:03:59,238 (trainer:676) INFO: 48epoch:train:165-205batch: iter_time=1.172e-04, forward_time=0.110, loss_ctc=13.053, loss_att=4.707, acc=0.961, loss=7.211, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 03:04:14,618 (trainer:676) INFO: 48epoch:train:206-246batch: iter_time=1.312e-04, forward_time=0.111, loss_ctc=12.753, loss_att=4.730, acc=0.962, loss=7.137, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 03:04:29,921 (trainer:676) INFO: 48epoch:train:247-287batch: iter_time=1.323e-04, forward_time=0.111, loss_ctc=13.002, loss_att=4.585, acc=0.963, loss=7.110, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 03:04:45,332 (trainer:676) INFO: 48epoch:train:288-328batch: iter_time=1.491e-04, forward_time=0.111, loss_ctc=12.204, loss_att=4.396, acc=0.964, loss=6.738, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 03:05:00,694 (trainer:676) INFO: 48epoch:train:329-369batch: iter_time=1.399e-04, forward_time=0.111, loss_ctc=12.299, loss_att=4.448, acc=0.961, loss=6.803, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 03:05:16,233 (trainer:676) INFO: 48epoch:train:370-410batch: iter_time=1.350e-04, forward_time=0.110, loss_ctc=13.144, loss_att=4.747, acc=0.958, loss=7.266, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 03:05:31,545 (trainer:676) INFO: 48epoch:train:411-451batch: iter_time=1.268e-04, forward_time=0.111, loss_ctc=12.501, loss_att=4.617, acc=0.961, loss=6.982, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 03:05:46,909 (trainer:676) INFO: 48epoch:train:452-492batch: iter_time=1.217e-04, forward_time=0.111, loss_ctc=12.846, loss_att=4.687, acc=0.961, loss=7.135, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 03:06:02,367 (trainer:676) INFO: 48epoch:train:493-533batch: iter_time=1.291e-04, forward_time=0.112, loss_ctc=13.033, loss_att=4.772, acc=0.963, loss=7.250, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 03:06:17,823 (trainer:676) INFO: 48epoch:train:534-574batch: iter_time=1.312e-04, forward_time=0.111, loss_ctc=12.683, loss_att=4.666, acc=0.964, loss=7.071, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 03:06:26,397 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 03:06:33,057 (trainer:676) INFO: 48epoch:train:575-615batch: iter_time=1.338e-04, forward_time=0.111, loss_ctc=12.729, loss_att=4.622, acc=0.959, loss=7.054, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 03:06:48,441 (trainer:676) INFO: 48epoch:train:616-656batch: iter_time=1.248e-04, forward_time=0.111, loss_ctc=12.375, loss_att=4.658, acc=0.963, loss=6.973, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 03:07:03,669 (trainer:676) INFO: 48epoch:train:657-697batch: iter_time=1.361e-04, forward_time=0.110, loss_ctc=12.597, loss_att=4.481, acc=0.963, loss=6.916, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 03:07:06,741 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 03:07:18,917 (trainer:676) INFO: 48epoch:train:698-738batch: iter_time=1.662e-04, forward_time=0.109, loss_ctc=14.030, loss_att=5.151, acc=0.954, loss=7.815, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 03:07:34,070 (trainer:676) INFO: 48epoch:train:739-779batch: iter_time=1.497e-04, forward_time=0.110, loss_ctc=13.361, loss_att=4.817, acc=0.960, loss=7.380, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 03:07:48,878 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:07:49,066 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:07:49,073 (trainer:676) INFO: 48epoch:train:780-820batch: iter_time=1.347e-04, forward_time=0.109, loss_ctc=12.471, loss_att=4.689, acc=0.940, loss=7.024, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.366
/home/ubuntu/accentedASR/espnet2/train/reporter.py:87: UserWarning: No valid stats found
  warnings.warn("No valid stats found")
/home/ubuntu/accentedASR/tools/anaconda/envs/espnet/lib/python3.9/site-packages/matplotlib/image.py:397: UserWarning: Warning: converting a masked element to nan.
  dv = (np.float64(self.norm.vmax) -
/home/ubuntu/accentedASR/tools/anaconda/envs/espnet/lib/python3.9/site-packages/matplotlib/image.py:398: UserWarning: Warning: converting a masked element to nan.
  np.float64(self.norm.vmin))
/home/ubuntu/accentedASR/tools/anaconda/envs/espnet/lib/python3.9/site-packages/matplotlib/image.py:405: UserWarning: Warning: converting a masked element to nan.
  a_min = np.float64(newmin)
/home/ubuntu/accentedASR/tools/anaconda/envs/espnet/lib/python3.9/site-packages/matplotlib/image.py:410: UserWarning: Warning: converting a masked element to nan.
  a_max = np.float64(newmax)
<__array_function__ internals>:5: UserWarning: Warning: converting a masked element to nan.
/home/ubuntu/accentedASR/tools/anaconda/envs/espnet/lib/python3.9/site-packages/numpy/ma/core.py:711: UserWarning: Warning: converting a masked element to nan.
  data = np.array(a, copy=False, subok=subok)
[ip-10-0-0-95] 2022-04-28 03:08:26,945 (trainer:334) INFO: 48epoch results: [train] iter_time=3.774e-04, forward_time=0.111, loss_ctc=12.760, loss_att=4.649, acc=0.960, loss=7.082, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373, time=5 minutes and 7.98 seconds, total_count=39552, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=nan, cer_ctc=1.000, loss_att=nan, acc=0.000e+00, cer=1.000, wer=1.000, loss=nan, time=6.96 seconds, total_count=1392, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.3 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 03:08:32,504 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 03:08:32,505 (trainer:268) INFO: 49/50epoch started. Estimated time to finish: 11 minutes and 44.4 seconds
[ip-10-0-0-95] 2022-04-28 03:08:47,603 (trainer:676) INFO: 49epoch:train:1-41batch: iter_time=0.006, forward_time=0.108, loss_ctc=11.805, loss_att=4.329, acc=0.967, loss=6.572, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 03:09:02,368 (trainer:676) INFO: 49epoch:train:42-82batch: iter_time=1.370e-04, forward_time=0.107, loss_ctc=12.221, loss_att=4.380, acc=0.964, loss=6.732, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.360
[ip-10-0-0-95] 2022-04-28 03:09:17,126 (trainer:676) INFO: 49epoch:train:83-123batch: iter_time=1.376e-04, forward_time=0.107, loss_ctc=12.475, loss_att=4.605, acc=0.962, loss=6.966, backward_time=0.104, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.360
[ip-10-0-0-95] 2022-04-28 03:09:32,249 (trainer:676) INFO: 49epoch:train:124-164batch: iter_time=1.230e-04, forward_time=0.109, loss_ctc=11.467, loss_att=4.164, acc=0.965, loss=6.355, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 03:09:47,383 (trainer:676) INFO: 49epoch:train:165-205batch: iter_time=1.188e-04, forward_time=0.108, loss_ctc=11.670, loss_att=4.296, acc=0.966, loss=6.509, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 03:10:02,284 (trainer:676) INFO: 49epoch:train:206-246batch: iter_time=1.275e-04, forward_time=0.107, loss_ctc=12.089, loss_att=4.246, acc=0.964, loss=6.599, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.363
[ip-10-0-0-95] 2022-04-28 03:10:09,158 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 03:10:17,555 (trainer:676) INFO: 49epoch:train:247-287batch: iter_time=1.347e-04, forward_time=0.111, loss_ctc=12.573, loss_att=4.528, acc=0.961, loss=6.942, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 03:10:32,866 (trainer:676) INFO: 49epoch:train:288-328batch: iter_time=1.407e-04, forward_time=0.110, loss_ctc=11.310, loss_att=4.083, acc=0.966, loss=6.251, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 03:10:48,117 (trainer:676) INFO: 49epoch:train:329-369batch: iter_time=1.330e-04, forward_time=0.111, loss_ctc=12.838, loss_att=4.498, acc=0.960, loss=7.000, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 03:11:03,702 (trainer:676) INFO: 49epoch:train:370-410batch: iter_time=1.446e-04, forward_time=0.112, loss_ctc=11.586, loss_att=4.211, acc=0.965, loss=6.423, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.380
[ip-10-0-0-95] 2022-04-28 03:11:19,143 (trainer:676) INFO: 49epoch:train:411-451batch: iter_time=1.548e-04, forward_time=0.111, loss_ctc=11.586, loss_att=4.276, acc=0.964, loss=6.469, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 03:11:34,600 (trainer:676) INFO: 49epoch:train:452-492batch: iter_time=1.331e-04, forward_time=0.111, loss_ctc=12.873, loss_att=4.446, acc=0.962, loss=6.974, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.377
[ip-10-0-0-95] 2022-04-28 03:11:50,099 (trainer:676) INFO: 49epoch:train:493-533batch: iter_time=1.261e-04, forward_time=0.111, loss_ctc=11.805, loss_att=4.275, acc=0.963, loss=6.534, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 03:12:05,319 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 03:12:05,635 (trainer:676) INFO: 49epoch:train:534-574batch: iter_time=1.301e-04, forward_time=0.111, loss_ctc=12.613, loss_att=4.577, acc=0.958, loss=6.988, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.379
[ip-10-0-0-95] 2022-04-28 03:12:21,074 (trainer:676) INFO: 49epoch:train:575-615batch: iter_time=1.369e-04, forward_time=0.112, loss_ctc=11.452, loss_att=4.298, acc=0.967, loss=6.444, backward_time=0.113, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 03:12:36,143 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:12:36,378 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:12:36,386 (trainer:676) INFO: 49epoch:train:616-656batch: iter_time=1.292e-04, forward_time=0.110, loss_ctc=12.486, loss_att=4.770, acc=0.942, loss=7.085, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 03:12:44,488 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:12:44,665 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:12:49,650 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:12:49,879 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:12:51,667 (trainer:676) INFO: 49epoch:train:657-697batch: iter_time=1.303e-04, forward_time=0.110, loss_ctc=12.755, loss_att=4.602, acc=0.919, loss=7.048, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.372
[ip-10-0-0-95] 2022-04-28 03:13:04,230 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:13:04,408 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:13:07,109 (trainer:676) INFO: 49epoch:train:698-738batch: iter_time=1.407e-04, forward_time=0.112, loss_ctc=11.843, loss_att=4.351, acc=0.945, loss=6.599, backward_time=0.114, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 03:13:09,712 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:13:09,947 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:13:22,093 (trainer:676) INFO: 49epoch:train:739-779batch: iter_time=1.340e-04, forward_time=0.112, loss_ctc=12.362, loss_att=4.450, acc=0.941, loss=6.823, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 03:13:37,478 (trainer:676) INFO: 49epoch:train:780-820batch: iter_time=1.382e-04, forward_time=0.111, loss_ctc=11.981, loss_att=4.434, acc=0.966, loss=6.698, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 03:14:15,114 (trainer:334) INFO: 49epoch results: [train] iter_time=4.015e-04, forward_time=0.110, loss_ctc=12.112, loss_att=4.395, acc=0.958, loss=6.710, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371, time=5 minutes and 6.42 seconds, total_count=40376, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=nan, cer_ctc=1.000, loss_att=nan, acc=0.000e+00, cer=1.000, wer=1.000, loss=nan, time=7.08 seconds, total_count=1421, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.11 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 03:14:20,844 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 03:14:20,862 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/48epoch.pth
[ip-10-0-0-95] 2022-04-28 03:14:20,863 (trainer:268) INFO: 50/50epoch started. Estimated time to finish: 5 minutes and 52.12 seconds
[ip-10-0-0-95] 2022-04-28 03:14:36,212 (trainer:676) INFO: 50epoch:train:1-41batch: iter_time=0.005, forward_time=0.110, loss_ctc=11.440, loss_att=4.044, acc=0.967, loss=6.262, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 03:14:45,796 (ctc:74) WARNING: 1/76 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 03:14:51,075 (trainer:676) INFO: 50epoch:train:42-82batch: iter_time=1.429e-04, forward_time=0.109, loss_ctc=12.217, loss_att=4.178, acc=0.961, loss=6.590, backward_time=0.105, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.362
[ip-10-0-0-95] 2022-04-28 03:15:06,076 (trainer:676) INFO: 50epoch:train:83-123batch: iter_time=1.260e-04, forward_time=0.109, loss_ctc=11.969, loss_att=4.385, acc=0.965, loss=6.660, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.366
[ip-10-0-0-95] 2022-04-28 03:15:20,735 (trainer:676) INFO: 50epoch:train:124-164batch: iter_time=1.285e-04, forward_time=0.108, loss_ctc=13.147, loss_att=4.697, acc=0.961, loss=7.232, backward_time=0.102, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.357
[ip-10-0-0-95] 2022-04-28 03:15:25,943 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:15:26,169 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:15:35,640 (trainer:676) INFO: 50epoch:train:165-205batch: iter_time=1.221e-04, forward_time=0.108, loss_ctc=12.481, loss_att=4.519, acc=0.939, loss=6.908, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.363
[ip-10-0-0-95] 2022-04-28 03:15:50,621 (trainer:676) INFO: 50epoch:train:206-246batch: iter_time=1.291e-04, forward_time=0.109, loss_ctc=11.289, loss_att=4.058, acc=0.968, loss=6.228, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.365
[ip-10-0-0-95] 2022-04-28 03:16:05,477 (trainer:676) INFO: 50epoch:train:247-287batch: iter_time=1.349e-04, forward_time=0.109, loss_ctc=11.902, loss_att=4.323, acc=0.965, loss=6.597, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.362
[ip-10-0-0-95] 2022-04-28 03:16:20,545 (trainer:676) INFO: 50epoch:train:288-328batch: iter_time=1.423e-04, forward_time=0.113, loss_ctc=12.177, loss_att=4.347, acc=0.965, loss=6.696, backward_time=0.107, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 03:16:35,989 (trainer:676) INFO: 50epoch:train:329-369batch: iter_time=1.540e-04, forward_time=0.114, loss_ctc=12.232, loss_att=4.296, acc=0.962, loss=6.677, backward_time=0.110, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.376
[ip-10-0-0-95] 2022-04-28 03:16:44,730 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:16:44,921 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:16:51,347 (trainer:676) INFO: 50epoch:train:370-410batch: iter_time=1.456e-04, forward_time=0.113, loss_ctc=11.950, loss_att=4.179, acc=0.937, loss=6.510, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.374
[ip-10-0-0-95] 2022-04-28 03:17:03,478 (ctc:74) WARNING: 8/83 samples got nan grad. These were ignored for CTC loss.
[ip-10-0-0-95] 2022-04-28 03:17:06,475 (trainer:676) INFO: 50epoch:train:411-451batch: iter_time=1.328e-04, forward_time=0.112, loss_ctc=12.545, loss_att=4.481, acc=0.960, loss=6.900, backward_time=0.106, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369
[ip-10-0-0-95] 2022-04-28 03:17:20,796 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:17:20,986 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:17:21,760 (trainer:676) INFO: 50epoch:train:452-492batch: iter_time=1.387e-04, forward_time=0.113, loss_ctc=11.506, loss_att=4.106, acc=0.941, loss=6.326, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 03:17:26,393 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:17:26,569 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:17:31,651 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:17:31,843 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:17:37,128 (trainer:676) INFO: 50epoch:train:493-533batch: iter_time=1.329e-04, forward_time=0.114, loss_ctc=12.234, loss_att=4.235, acc=0.889, loss=6.634, backward_time=0.111, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 03:17:47,318 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:17:47,557 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:17:52,446 (trainer:676) INFO: 50epoch:train:534-574batch: iter_time=1.434e-04, forward_time=0.114, loss_ctc=12.082, loss_att=4.235, acc=0.944, loss=6.589, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.373
[ip-10-0-0-95] 2022-04-28 03:18:02,170 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:18:02,403 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:18:07,565 (trainer:676) INFO: 50epoch:train:575-615batch: iter_time=1.440e-04, forward_time=0.113, loss_ctc=11.472, loss_att=4.065, acc=0.943, loss=6.287, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.368
[ip-10-0-0-95] 2022-04-28 03:18:23,065 (trainer:676) INFO: 50epoch:train:616-656batch: iter_time=1.426e-04, forward_time=0.112, loss_ctc=12.356, loss_att=4.482, acc=0.964, loss=6.844, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.378
[ip-10-0-0-95] 2022-04-28 03:18:38,464 (trainer:676) INFO: 50epoch:train:657-697batch: iter_time=1.351e-04, forward_time=0.113, loss_ctc=10.857, loss_att=3.769, acc=0.967, loss=5.895, backward_time=0.112, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.375
[ip-10-0-0-95] 2022-04-28 03:18:53,687 (trainer:676) INFO: 50epoch:train:698-738batch: iter_time=1.331e-04, forward_time=0.111, loss_ctc=12.082, loss_att=4.273, acc=0.964, loss=6.616, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.371
[ip-10-0-0-95] 2022-04-28 03:19:08,722 (trainer:676) INFO: 50epoch:train:739-779batch: iter_time=1.369e-04, forward_time=0.112, loss_ctc=11.927, loss_att=4.229, acc=0.967, loss=6.539, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.366
[ip-10-0-0-95] 2022-04-28 03:19:22,391 (ctc:69) WARNING: All samples in this mini-batch got nan grad. Returning nan value instead of CTC loss
[ip-10-0-0-95] 2022-04-28 03:19:22,584 (trainer:618) WARNING: The grad norm is nan. Skipping updating the model.
[ip-10-0-0-95] 2022-04-28 03:19:23,768 (trainer:676) INFO: 50epoch:train:780-820batch: iter_time=1.438e-04, forward_time=0.111, loss_ctc=11.481, loss_att=4.290, acc=0.910, loss=6.447, backward_time=0.109, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.367
[ip-10-0-0-95] 2022-04-28 03:20:01,713 (trainer:334) INFO: 50epoch results: [train] iter_time=3.856e-04, forward_time=0.111, loss_ctc=11.983, loss_att=4.264, acc=0.952, loss=6.580, backward_time=0.108, optim_step_time=0.041, optim0_lr0=0.001, train_time=0.369, time=5 minutes and 4.47 seconds, total_count=41200, gpu_max_cached_mem_GB=7.422, [valid] loss_ctc=nan, cer_ctc=1.000, loss_att=nan, acc=0.000e+00, cer=1.000, wer=1.000, loss=nan, time=7.12 seconds, total_count=1450, gpu_max_cached_mem_GB=7.422, [att_plot] time=29.25 seconds, total_count=0, gpu_max_cached_mem_GB=7.422
[ip-10-0-0-95] 2022-04-28 03:20:07,253 (trainer:380) INFO: There are no improvements in this epoch
[ip-10-0-0-95] 2022-04-28 03:20:07,271 (trainer:436) INFO: The model files were removed: exp/asr_train_asr_conformer5_raw_bpe100_sp/49epoch.pth
[ip-10-0-0-95] 2022-04-28 03:20:07,271 (trainer:454) INFO: The training was finished at 50 epochs 
[ip-10-0-0-95] 2022-04-28 03:20:07,297 (average_nbest_models:72) INFO: Averaging 10best models: criterion="valid.acc": exp/asr_train_asr_conformer5_raw_bpe100_sp/valid.acc.ave_10best.pth
# Accounting: time=17617 threads=1
# Ended (code 0) at Thu Apr 28 03:20:10 UTC 2022, elapsed time 17617 seconds
